{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bitnlpcondaed063720e340471382fe7eca751f187d",
   "display_name": "Python 3.7.9 64-bit ('nlp': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that  we have gotten the data, preprocessed it and explored it initially, we move on to the LDA topic analysis. \n",
    "\n",
    "# Import required packages to do LDA \n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import itertools # peoblwm\n",
    "import collections # problwm\n",
    "import spacy\n",
    "from pprint import pprint # problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/anaconda3/envs/nlp/lib/python3.7/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n",
      "/Users/Astridll/.local/lib/python3.7/site-packages/nltk/decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n",
      "/Users/Astridll/.local/lib/python3.7/site-packages/nltk/lm/counter.py:15: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Sequence, defaultdict\n",
      "/Users/Astridll/.local/lib/python3.7/site-packages/nltk/lm/vocabulary.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Counter, Iterable\n"
     ]
    }
   ],
   "source": [
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "# NLTK\n",
    "from nltk import bigrams\n",
    "from nltk.stem import PorterStemmer\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"whitegrid\")\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "#%matplotlib inlin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text             location  \\\n",
       "0  The Future of Vaccines\\nhttps://t.co/GI23fycj1...                Earth   \n",
       "1  @AndrewButters91 *66 million (Moderna requires...          Toronto, ON   \n",
       "2  The immunization task force can likely deliver...  Mississauga, Canada   \n",
       "3  As I was writing this, Health Canada said it a...         New York, NY   \n",
       "4  BREAKING: Health Canada authorizes Moderna's C...   North Bay, Ontario   \n",
       "\n",
       "   statuses_count  followers_count  friends_count  listed_count  \\\n",
       "0           11485              774           3240             2   \n",
       "1              10              110            732             1   \n",
       "2             280              225            675             3   \n",
       "3            9050             2826           4013           185   \n",
       "4           36196             4825            493            35   \n",
       "\n",
       "   favourites_count                                          tidy_text  \\\n",
       "0              1107  the future never before have billions people b...   \n",
       "1                 9                         million requires two doses   \n",
       "2                26  the immunization task force can likely deliver...   \n",
       "3              9650  was writing this health canada said authorized...   \n",
       "4                 4                  breaking health canada authorizes   \n",
       "\n",
       "                                      less_prep_text  \\\n",
       "0  The Future of Vaccines\\n\\nNever before have bi...   \n",
       "1           *66 million (Moderna requires two doses)   \n",
       "2  The immunization task force can likely deliver...   \n",
       "3  As I was writing this, Health Canada said it a...   \n",
       "4  BREAKING: Health Canada authorizes Moderna's C...   \n",
       "\n",
       "                                    tidy_text_tokens  \\\n",
       "0  [the, future, never, before, have, billions, p...   \n",
       "1                    [million, requires, two, doses]   \n",
       "2  [the, immunization, task, force, can, likely, ...   \n",
       "3  [was, writing, this, health, canada, said, aut...   \n",
       "4             [breaking, health, canada, authorizes]   \n",
       "\n",
       "                                 tokens_no_stopwords  \\\n",
       "0  [future, never, billions, people, pressured, s...   \n",
       "1                         [million, requires, doses]   \n",
       "2  [immunization, task, force, likely, deliver, s...   \n",
       "3              [writing, health, canada, authorized]   \n",
       "4             [breaking, health, canada, authorizes]   \n",
       "\n",
       "                                 no_stopwords_string  \n",
       "0      future never billions people pressured submit  \n",
       "1                             million requires doses  \n",
       "2  immunization task force likely deliver shots s...  \n",
       "3                   writing health canada authorized  \n",
       "4                  breaking health canada authorizes  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>location</th>\n      <th>statuses_count</th>\n      <th>followers_count</th>\n      <th>friends_count</th>\n      <th>listed_count</th>\n      <th>favourites_count</th>\n      <th>tidy_text</th>\n      <th>less_prep_text</th>\n      <th>tidy_text_tokens</th>\n      <th>tokens_no_stopwords</th>\n      <th>no_stopwords_string</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The Future of Vaccines\\nhttps://t.co/GI23fycj1...</td>\n      <td>Earth</td>\n      <td>11485</td>\n      <td>774</td>\n      <td>3240</td>\n      <td>2</td>\n      <td>1107</td>\n      <td>the future never before have billions people b...</td>\n      <td>The Future of Vaccines\\n\\nNever before have bi...</td>\n      <td>[the, future, never, before, have, billions, p...</td>\n      <td>[future, never, billions, people, pressured, s...</td>\n      <td>future never billions people pressured submit</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@AndrewButters91 *66 million (Moderna requires...</td>\n      <td>Toronto, ON</td>\n      <td>10</td>\n      <td>110</td>\n      <td>732</td>\n      <td>1</td>\n      <td>9</td>\n      <td>million requires two doses</td>\n      <td>*66 million (Moderna requires two doses)</td>\n      <td>[million, requires, two, doses]</td>\n      <td>[million, requires, doses]</td>\n      <td>million requires doses</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The immunization task force can likely deliver...</td>\n      <td>Mississauga, Canada</td>\n      <td>280</td>\n      <td>225</td>\n      <td>675</td>\n      <td>3</td>\n      <td>26</td>\n      <td>the immunization task force can likely deliver...</td>\n      <td>The immunization task force can likely deliver...</td>\n      <td>[the, immunization, task, force, can, likely, ...</td>\n      <td>[immunization, task, force, likely, deliver, s...</td>\n      <td>immunization task force likely deliver shots s...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>As I was writing this, Health Canada said it a...</td>\n      <td>New York, NY</td>\n      <td>9050</td>\n      <td>2826</td>\n      <td>4013</td>\n      <td>185</td>\n      <td>9650</td>\n      <td>was writing this health canada said authorized...</td>\n      <td>As I was writing this, Health Canada said it a...</td>\n      <td>[was, writing, this, health, canada, said, aut...</td>\n      <td>[writing, health, canada, authorized]</td>\n      <td>writing health canada authorized</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>BREAKING: Health Canada authorizes Moderna's C...</td>\n      <td>North Bay, Ontario</td>\n      <td>36196</td>\n      <td>4825</td>\n      <td>493</td>\n      <td>35</td>\n      <td>4</td>\n      <td>breaking health canada authorizes</td>\n      <td>BREAKING: Health Canada authorizes Moderna's C...</td>\n      <td>[breaking, health, canada, authorizes]</td>\n      <td>[breaking, health, canada, authorizes]</td>\n      <td>breaking health canada authorizes</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# LOAD PREPROCESSED DATA\n",
    "\n",
    "# Data minus query\n",
    "#tweets_df = pd.read_pickle('prepped_tweets_minus_query.pkl')\n",
    "#tweets_df.head()\n",
    "\n",
    "# data minus query + vaccine + vaccines\n",
    "#tweets_df = pd.read_pickle('prepped_tweets_minus_query_vaccine_vaccines.pkl')\n",
    "#tweets_df.head()\n",
    "\n",
    "# data minus query + vaccine + vaccines + covid\n",
    "#tweets_df = pd.read_pickle('prepped_tweets_minus_query_vaccine_vaccines_covid.pkl')\n",
    "#tweets_df.head()\n",
    "\n",
    "# use this for synopsis hand in version\n",
    "# data minus query + vaccine + vaccines + covid + pfizer\n",
    "#tweets_df = pd.read_pickle('prepped_tweets_minus_query_vaccine_vaccines_covid_pfizer.pkl')\n",
    "#tweets_df.head()\n",
    "\n",
    "# LINE BELOW ADDED AFTER SYNOPSIS HAND IN:\n",
    "tweets_df = pd.read_pickle('updated_prepped_tweets_minus_query_vaccine_vaccines_covid_pfizer.pkl')\n",
    "tweets_df.head()\n",
    "#####\n",
    "\n",
    "# Actually the preprocessing continues a little more before we can do LDA\n",
    "# What we more do is the following: \n",
    "#  - Load preprocessed data minus query\n",
    "#  - Add bigrams by making a list of the data with the added bigrams. Then apply this list when doing lemmetization. \n",
    "#  - Lemmetization (lemmas generated from bigram-data-list and addded as column in df)\n",
    "#  - stemming (but generated from the no_stopwords_string which I guess is without the bigrams)\n",
    "\n",
    "# - saving and reloading df in 4 versions (df1 where only the query/moderna is removed, df2 where vaccine and vaccines are removed additionally, df3 where covid is additionally removed, and df4 where pfizer is additionally removed)\n",
    "\n",
    "#  - Because LDA models are performing horribly I removed top 10 most frequent words /lemmas from data, after this I excluded all tweets where lemmas < 3 (below 3). Also, I then removed any possible duplicates \n",
    "# Hopefully help, reduced from Before: (13435, 15) to After (). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(13435, 12)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "tweets_df.shape\n",
    "# (13889, 13) - query (extra column is an irrelevant after preprocessing tweet_length column)\n",
    "# (13718, 12) - query + vaccine + vaccines \n",
    "# (13583, 12) - query + vaccine + vaccines + covid\n",
    "# (13435, 11) - query + vaccine + vaccines + covid + pfizer (one less column bc I skipped tweet_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "doses           1262\n",
       "first           1250\n",
       "health           903\n",
       "coronavirus      783\n",
       "trump            761\n",
       "fda              642\n",
       "week             602\n",
       "new              588\n",
       "today            536\n",
       "receive          519\n",
       "stock            490\n",
       "fauci            484\n",
       "received         473\n",
       "people           442\n",
       "approved         438\n",
       "second           424\n",
       "news             418\n",
       "distribution     392\n",
       "know             388\n",
       "emergency        382\n",
       "canada           368\n",
       "side             362\n",
       "mrna             356\n",
       "says             334\n",
       "monday           333\n",
       "workers          329\n",
       "shipments        328\n",
       "shipment         300\n",
       "getting          288\n",
       "biontech         275\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "# get most frequent words \n",
    "frequentw_list = pd.Series(' '.join(tweets_df.no_stopwords_string).split()).value_counts()[:30]\n",
    "#tweets_df.head()\n",
    "frequentw_list\n",
    "#no_stopwords_string\n",
    "\n",
    "# minus query \n",
    "# top 10 most frequntly used words \n",
    "#vaccine         7565\n",
    "#covid           4049\n",
    "#pfizer          2984\n",
    "#vaccines        1624\n",
    "#doses           1269\n",
    "#first           1253\n",
    "#health           903\n",
    "#coronavirus      788\n",
    "#trump            763\n",
    "#fda              647\n",
    "\n",
    "# minus query + vaccine + vaccines\n",
    "# top 10 most frequntly used words \n",
    "#covid           4122\n",
    "#pfizer          2960\n",
    "#doses           1268\n",
    "#first           1250\n",
    "#health           903\n",
    "#coronavirus      787\n",
    "#trump            763\n",
    "#fda              645\n",
    "#week             602\n",
    "#new              588\n",
    "\n",
    "# minus query + vaccine + vaccines + covid\n",
    "# top 10 most frequntly used words \n",
    "#pfizer          2952\n",
    "#doses           1265\n",
    "#first           1250\n",
    "#health           903\n",
    "#coronavirus      786\n",
    "#trump            763\n",
    "#fda              642\n",
    "#week             602\n",
    "#new              588\n",
    "#today            536\n",
    "\n",
    "# minus query + vaccine + vaccines + covid + pfizer\n",
    "# top 10 most frequntly used words \n",
    "#doses           1262\n",
    "#first           1250\n",
    "#health           903\n",
    "#coronavirus      783\n",
    "#trump            761\n",
    "#fda              642\n",
    "#week             602\n",
    "#new              588\n",
    "#today            536\n",
    "#receive          519\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to add bigrams and maybe trigrams (set of n_grams (words) that often occur together)\n",
    "# make list of tokenized no_stopwords_string called data_token_words\n",
    "# NB note - it is just a list and will therefore not be added as column in df \n",
    "\n",
    "# Tokenize words from column and put into list \n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "data_token_words = list(sent_to_words(tweets_df['no_stopwords_string']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### About gensim.model.phrases\n",
    "\n",
    "# `min_count` ignore all words and bigrams that have a total collected count below this \n",
    "\n",
    "# `threshold` stands for a threshold for forming  phrases (the higher value the fewer phrases). \"A phrase of words `a` and `b` is accepted if `(cnt(a, b) - min_count) * N / (cnt(a) * cnt(b)) > threshold`, where `N` is the total vocabulary size.\"\n",
    "\n",
    "# `max_vocab_size` loft for the size of the vocabulary. Applied to trim less common words. Default of 40M needs around 3.6GB of RAM; consider turning up and down for the 'max_vocab_size' dependent on the amount of available memory.\n",
    "\n",
    "#`delimiter` is the character used to join collocation tokens (and should be a byte string as b'_').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bigrams (and later maybe trigrams)\n",
    "\n",
    "# Build bigram (and trigram) model\n",
    "bigram = gensim.models.Phrases(data_token_words, min_count=6, threshold=100)\n",
    "# trigram = gensim.models.Phrases(bigram[data_token_words], threshold=100)\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "#trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_model[doc] for doc in texts]\n",
    "#def make_trigrams(texts):\n",
    "#    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_token_words)\n",
    "# Form trigrams \n",
    "#data_words_trigrams = make_trigrams(data_token_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['choice', 'pfizer', 'biontech', 'vaccines', 'pick'], ['wait', 'brand', 'evolves'], ['unknown', 'whether', 'current', 'pfizer', 'vaccines', 'confer', 'even', 'short', 'term', 'immunity', 'new', 'strain'], ['nether', 'pfizer', 'clearly', 'explained', 'public', 'vaccines', 'effectively', 'immunize'], ['first', 'ever_consumer', 'facing', 'campaign', 'launched', 'comes'], ['glance', 'ingredient', 'lists', 'pfizer', 'sars_cov', 'vax', 'mindblowingly', 'benign'], ['yes', 'talking', 'glen', 'wrote', 'canada', 'vaccine'], ['vaccine', 'landed', 'hands', 'onslow', 'county', 'officials', 'tuesday', 'concerning', 'levels', 'cases', 'deaths'], ['pharmacies', 'soon', 'receiving', 'thousand', 'doses', 'covid', 'vaccine'], ['vaccine', 'landed', 'hands', 'onslow', 'county', 'officials', 'tuesday', 'concerning', 'levels', 'cases', 'deaths'], ['pfizer', 'trials', 'inadvertent', 'pregnant', 'people', 'trials', 'ppl', 'respectively', 'peop'], ['interesting', 'looks', 'evolution', 'vaccines', 'machine', 'vaccine'], ['idiot', 'purchase', 'enough', 'vaccine', 'herd_immunity', 'let', 'countries', 'fulfill'], ['pfizer', 'covid', 'vaccines', 'effective', 'new', 'strains', 'covid'], ['fair', 'ahead', 'alphabet', 'haha'], ['vaccine', 'stay', 'essential', 'worker', 'hospital', 'care', 'health', 'overall', 'love'], ['told', 'red', 'eye', 'flight', 'madagascar', 'vaccine', 'freakin', 'pumped', 'count'], ['pfizer', 'astrazeneca', 'anti', 'vax', 'efficient', 'weed', 'last', 'ones', 'post', 'quarantine', 'lives'], ['article', 'suggest', 'pfizer', 'vaccines', 'immunize', 'new', 'strains', 'covid'], ['people', 'died', 'covid', 'vaccine', 'trial', 'news_lifesite']]\n"
     ]
    }
   ],
   "source": [
    "# See bigram example\n",
    "print(data_words_bigrams[1140:1160])\n",
    "# can e.g. see that bigrams such as:\n",
    "# - allergic_reactions\n",
    "# - allergic_reaction \n",
    "# - biotech_firm\n",
    "# - approves_game\n",
    "# - leaves_warehouses\n",
    "# - expanding_immunization\n",
    "# - side_effects\n",
    "# - herd_immunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization \n",
    "# Let's try to do lemmetization. Lemmatization transforms a word into its bsae form, e.g. \"playing\" --> \"play\", and “best” -> “good”. and \"houses\" --> \"house\"\n",
    "# Use the spaCy package (open-source NLP library with different pre-built models)\n",
    "\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner']) # Initialize spacy 'en' model, keeping only the tagger component\n",
    "\n",
    "def lemmatization(tweets, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    tweets_out = []\n",
    "    for sent in tweets:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        tweets_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return tweets_out\n",
    "\n",
    "# Lemmatizating bigram data (keeping only noun, adj, vb, adv)\n",
    "tweets_df['lemmas'] = pd.Series(lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(13435, 14)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# NOT DOING THIS (could consider doing it...)  - did add lemmas as string though but did not remove duplicates\n",
    "\n",
    "# MAYBE - REMOVE DUPLICATED\n",
    "# Not sure if we should remove duplicates here. Having few tokens when lemmatizing could give duplicate rows. \n",
    "# add column to data which is lemmas as string \n",
    "# so that I can DROP DUPLICATES \n",
    "\n",
    "tweets_df['lemmas_string'] = [' '.join(map(str, l)) for l in tweets_df['lemmas']]\n",
    "#tweets_df.head()\n",
    "#tweets_df.drop_duplicates(subset=['lemmas_string'], keep='first',inplace=True)\n",
    "# did not actually do, but could: tweets_df = tweets_df.drop(['lemmatized_string'], axis=1)\n",
    "#tweets_df.reset_index(drop=True, inplace=True)\n",
    "tweets_df.shape # 2280,15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dose            928\n",
       "receive         778\n",
       "first           658\n",
       "week            434\n",
       "today           410\n",
       "trump           365\n",
       "shipment        361\n",
       "stock           345\n",
       "people          330\n",
       "approve         302\n",
       "new             294\n",
       "begin           283\n",
       "expect          272\n",
       "say             271\n",
       "health          265\n",
       "know            256\n",
       "arrive          253\n",
       "get             249\n",
       "distribution    225\n",
       "second          216\n",
       "state           204\n",
       "start           200\n",
       "hospital        199\n",
       "work            194\n",
       "ship            188\n",
       "go              188\n",
       "think           179\n",
       "good            175\n",
       "worker          173\n",
       "need            163\n",
       "emergency       162\n",
       "wait            157\n",
       "vaccinate       157\n",
       "trial           156\n",
       "thank           155\n",
       "update          147\n",
       "care            147\n",
       "vaccination     147\n",
       "live            138\n",
       "company         138\n",
       "next            135\n",
       "take            135\n",
       "morning         128\n",
       "virus           128\n",
       "shot            127\n",
       "money           126\n",
       "administer      123\n",
       "official        118\n",
       "year            116\n",
       "come            115\n",
       "time            114\n",
       "last            111\n",
       "already         111\n",
       "news            111\n",
       "read            108\n",
       "authorize       106\n",
       "own             106\n",
       "case            106\n",
       "distribute      102\n",
       "look            102\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 140
    }
   ],
   "source": [
    "# get most frequent words in lemmas\n",
    "frequentw_list3 = pd.Series(' '.join(tweets_df.lemmas_string).split()).value_counts()[:30]\n",
    "#tweets_df.head()\n",
    "frequentw_list3\n",
    "#lemmas_string\n",
    "#vaccine, covid, pfizer not removed \n",
    "# - pfizer not here as lemmas are restricted to verbs, adjectives, nouns, and adverbiums \n",
    "\n",
    "# Top 10 frequnt lemmas (query removed) \n",
    "#vaccine         7445\n",
    "#covid           3013\n",
    "#dose            1368\n",
    "#receive         1240\n",
    "#first            941\n",
    "#approve          663\n",
    "#week             608\n",
    "#begin            575\n",
    "#shipment         563\n",
    "#today            535\n",
    "\n",
    "# Top 10 frequnt lemmas (query + vaccine + vaccines removed) \n",
    "#covid           2345\n",
    "#dose            1361\n",
    "#receive         1241\n",
    "#first            983\n",
    "#approve          653\n",
    "#week             620\n",
    "#shipment         598\n",
    "#begin            584\n",
    "#today            533\n",
    "#stock            511\n",
    "\n",
    "# Top 10 frequnt lemmas (query + vaccine + vaccines + covid removed) \n",
    "#dose            1373\n",
    "#receive         1220\n",
    "#first            971\n",
    "#approve          637\n",
    "#week             612\n",
    "#shipment         589\n",
    "#begin            572\n",
    "#today            531\n",
    "#stock            511\n",
    "#people           443\n",
    "\n",
    "# Top 10 frequnt lemmas (query + vaccine + vaccines + covid + pfizer removed) \n",
    "#dose            1378\n",
    "#receive         1216\n",
    "#first            980\n",
    "#approve          635\n",
    "#week             625\n",
    "#shipment         595\n",
    "#begin            570\n",
    "#today            532\n",
    "#stock            528\n",
    "#new              444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEMMING \n",
    "# Transforms all words into their root form by cutting of the suffix (can result in non-meaningful words).\n",
    "# E.g. \"animals\" would be transformed into \"anim\" (but to \"animal\" when lemmatizing)\n",
    "# Stemming and lemmetization both contributes to \"dimensionality reduction\" \n",
    "\n",
    "# STEMMING - NB note that stemming is not on bigram data - maybe change this later\n",
    "stemmer = PorterStemmer()\n",
    "tweets_df['stemmed'] = tweets_df['tokens_no_stopwords'].apply(lambda x : [stemmer.stem(y) for y in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(13435, 15)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "#tweets_df.head()\n",
    "tweets_df.shape \n",
    "# (13889, 16) q\n",
    "# (13718, 15) q + vacc\n",
    "# (13583, 15) q + vacc + covid\n",
    "# (13435, 14) q + vacc + covid + pfizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dose          1497\n",
       "receiv        1257\n",
       "first         1250\n",
       "health         903\n",
       "approv         858\n",
       "trump          788\n",
       "coronaviru     784\n",
       "week           674\n",
       "fda            643\n",
       "shipment       628\n",
       "distribut      620\n",
       "vaccin         594\n",
       "new            588\n",
       "stock          583\n",
       "arriv          562\n",
       "author         555\n",
       "today          538\n",
       "fauci          484\n",
       "state          484\n",
       "expect         461\n",
       "begin          452\n",
       "shot           450\n",
       "peopl          448\n",
       "second         424\n",
       "news           418\n",
       "ship           415\n",
       "know           410\n",
       "hospit         395\n",
       "emerg          393\n",
       "say            382\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "# get most frequent words stemmed \n",
    "# adding a column of stemmed words in string format \n",
    "tweets_df['stemmed_string'] = [' '.join(map(str, l)) for l in tweets_df['stemmed']]\n",
    "\n",
    "frequentw_list4 = pd.Series(' '.join(tweets_df.stemmed_string).split()).value_counts()[:30]\n",
    "#tweets_df.head()\n",
    "frequentw_list4\n",
    "\n",
    "# Overview of top 10 most frequent stemmed words (only q removed)\n",
    "#vaccin        9787\n",
    "#covid         4050\n",
    "#pfizer        2990\n",
    "#dose          1507\n",
    "#receiv        1268\n",
    "#first         1253\n",
    "#health         903\n",
    "#approv         884\n",
    "#coronaviru     789\n",
    "#trump          789\n",
    "\n",
    "# Overview of top 10 most frequent stemmed words (only q + vaccine + vaccines removed)\n",
    "#covid         4137\n",
    "#pfizer        2966\n",
    "#dose          1505\n",
    "#receiv        1264\n",
    "#first         1250\n",
    "#health         903\n",
    "#approv         873\n",
    "#trump          790\n",
    "#coronaviru     788\n",
    "#week           675\n",
    "\n",
    "# Overview of top 10 most frequent stemmed words (only q + vaccine + vaccines + covid removed)\n",
    "#pfizer        2958\n",
    "#dose          1502\n",
    "#receiv        1259\n",
    "#first         1250\n",
    "#health         903\n",
    "#approv         860\n",
    "#trump          790\n",
    "#coronaviru     787\n",
    "#week           675\n",
    "#fda            643\n",
    "\n",
    "# Overview of top 10 most frequent stemmed words (only q + vaccine + vaccines + covid + pfizer removed)\n",
    "#dose          1497\n",
    "#receiv        1257\n",
    "#first         1250\n",
    "#health         903\n",
    "#approv         858\n",
    "#trump          788\n",
    "#coronaviru     784\n",
    "#week           674\n",
    "#fda            643\n",
    "#shipment       628"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING UPDATED DATAFRAME WITH LEMMAS AND STEMMED WORDS \n",
    "\n",
    "# minus query \n",
    "# # save to csv file:\n",
    "#tweets_df.to_csv('updated_stemmedlemmas_data_minus_q.csv', header=True, mode='a')\n",
    "# save to pickle \n",
    "#tweets_df.to_pickle('updated_stemmedlemmas_data_minus_q.pkl')\n",
    "\n",
    "# minus query + vaccine + vaccines\n",
    "# save to csv file:\n",
    "# tweets_df.to_csv('updated_stemmedlemmas_data_minus_q_vaccine.csv', header=True, mode='a')\n",
    "# save to pickle \n",
    "# tweets_df.to_pickle('updated_stemmedlemmas_data_minus_q_vaccine.pkl')\n",
    "\n",
    "# minus query + vaccine + vaccines + covid\n",
    "# save to csv file:\n",
    "# tweets_df.to_csv('updated_stemmedlemmas_data_minus_q_vaccine_covid.csv', header=True, mode='a')\n",
    "# save to pickle \n",
    "# tweets_df.to_pickle('updated_stemmedlemmas_data_minus_q_vaccine_covid.pkl')\n",
    "\n",
    "# minus query + vaccine + vaccines + covid + pfizer\n",
    "# save to csv file:\n",
    "#tweets_df.to_csv('updated_stemmedlemmas_data_minus_q_vaccine_covid_pfizer.csv', header=True, mode='a')\n",
    "# save to pickle \n",
    "#tweets_df.to_pickle('updated_stemmedlemmas_data_minus_q_vaccine_covid_pfizer.pkl')\n",
    "\n",
    "\n",
    "# LINES BELOW ADDED AFTER SYNOPSIS HAND IN: \n",
    "# updated df with less preprocessed column (data minus query + vaccine + vaccines + covid + pfizer)\n",
    "# save to csv file:\n",
    "tweets_df.to_csv('new_updated_stemmedlemmas_data_minus_q_vaccine_covid_pfizer.csv', header=True, mode='a')\n",
    "# save to pickle \n",
    "tweets_df.to_pickle('new_updated_stemmedlemmas_data_minus_q_vaccine_covid_pfizer.pkl')\n",
    "\n",
    "\n",
    "\n",
    "###### THIS IS THE END OF THE PREPROCESSING AND WE NOW MOVE ON TO BUILD MODELS ######### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now open each of the 4 versions of the preprocessed df as df 1-4\n",
    "\n",
    "# df1 = data minus (query) \n",
    "#df1 = pd.read_pickle('updated_stemmedlemmas_data_minus_q.pkl')\n",
    "\n",
    "# df2 = data minus (query + vaccine + vaccines)\n",
    "#df2 = pd.read_pickle('updated_stemmedlemmas_data_minus_q_vaccine.pkl')\n",
    "\n",
    "# df3 = data minus (query + vaccine + vaccines + covid)\n",
    "#df3 = pd.read_pickle('updated_stemmedlemmas_data_minus_q_vaccine_covid.pkl')\n",
    "\n",
    "# df4 = data minus (query + vaccine + vaccines + covid + pfizer)\n",
    "#df4 = pd.read_pickle('updated_stemmedlemmas_data_minus_q_vaccine_covid_pfizer.pkl')\n",
    "\n",
    "# LINE BELOW ADDED AFTER SYNOPSIS HAND IN\n",
    "df5 = pd.read_pickle('new_updated_stemmedlemmas_data_minus_q_vaccine_covid_pfizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(13435, 15)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# check that it is the correct data\n",
    "# (NB column numbers vary bc I included a block that add a column w tweet character length in 1-3, and also a post preprocessing tweet character length column in df1)\n",
    "# \n",
    "# #df1.shape #(13889, 17)\n",
    "#df2.shape #(13718, 16)\n",
    "#df3.shape #(13583, 16)\n",
    "df4.shape #(13435, 15)\n",
    "#df4.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### OBS - DO NOT RUN FOR UPDATED DF WITH ADDED LESS PREPROCESSED COLUMN\n",
    "# RUN BLOCK BELOW INSTEAD\n",
    "\n",
    "#  REMOVING 10 MOST FREQUENT LEMMAS IN ORDER TO GET MORE COHERENT TOPICS \n",
    "# ADDING COLUMN LEMMAS_REDUCED TO CHECK OUT TOPIC MODELING MINUS MOST FREQUENT LEMMAS\n",
    "\n",
    "# Excluding 10 most common words (lemmas) to see if it improves model performance - it does, a lot! But discuss cost\n",
    "# Top 10 frequent lemmas (when query is removed) \n",
    "#vaccine         7445 - already gone in df4\n",
    "#covid           3013 - already gone in df4 \n",
    "#dose            1368-\n",
    "#receive         1240-\n",
    "#first            941-\n",
    "#approve          663-\n",
    "#week             608-\n",
    "#begin            575-\n",
    "#shipment         563-\n",
    "#today            535-\n",
    "\n",
    "\n",
    "def remove_1_word(tweet):\n",
    "    tweet_no_1 = re.sub(r\"dose\", \"\", tweet)\n",
    "    return tweet_no_1\n",
    "df4['lemmas_string'] = np.vectorize(remove_1_word)(df4['lemmas_string'])\n",
    "def remove_2_word(tweet):\n",
    "    tweet_no_2 = re.sub(r\"receive\", \"\", tweet)\n",
    "    return tweet_no_2\n",
    "df4['lemmas_string'] = np.vectorize(remove_2_word)(df4['lemmas_string'])\n",
    "def remove_3_word(tweet):\n",
    "    tweet_no_3 = re.sub(r\"first\", \"\", tweet)\n",
    "    return tweet_no_3\n",
    "df4['lemmas_string'] = np.vectorize(remove_3_word)(df4['lemmas_string'])\n",
    "def remove_4_word(tweet):\n",
    "    tweet_no_4 = re.sub(r\"approve\", \"\", tweet)\n",
    "    return tweet_no_4\n",
    "df4['lemmas_string'] = np.vectorize(remove_4_word)(df4['lemmas_string'])\n",
    "def remove_5_word(tweet):\n",
    "    tweet_no_5 = re.sub(r\"week\", \"\", tweet)\n",
    "    return tweet_no_5\n",
    "df4['lemmas_string'] = np.vectorize(remove_5_word)(df4['lemmas_string'])\n",
    "def remove_6_word(tweet):\n",
    "    tweet_no_6 = re.sub(r\"begin\", \"\", tweet)\n",
    "    return tweet_no_6\n",
    "df4['lemmas_string'] = np.vectorize(remove_6_word)(df4['lemmas_string'])\n",
    "def remove_7_word(tweet):\n",
    "    tweet_no_7 = re.sub(r\"shipment\", \"\", tweet)\n",
    "    return tweet_no_7\n",
    "df4['lemmas_string'] = np.vectorize(remove_7_word)(df4['lemmas_string'])\n",
    "def remove_8_word(tweet):\n",
    "    tweet_no_8 = re.sub(r\"today\", \"\", tweet)\n",
    "    return tweet_no_8\n",
    "df4['lemmas_string'] = np.vectorize(remove_8_word)(df4['lemmas_string'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS BLOCK IS ADDED AFTER SYNOPSIS HAND IN \n",
    "def remove_1_word(tweet):\n",
    "    tweet_no_1 = re.sub(r\"dose\", \"\", tweet)\n",
    "    return tweet_no_1\n",
    "df5['lemmas_string'] = np.vectorize(remove_1_word)(df5['lemmas_string'])\n",
    "df5['less_prep_text'] = np.vectorize(remove_1_word)(df5['less_prep_text'])\n",
    "\n",
    "def remove_2_word(tweet):\n",
    "    tweet_no_2 = re.sub(r\"receive\", \"\", tweet)\n",
    "    return tweet_no_2\n",
    "df5['lemmas_string'] = np.vectorize(remove_2_word)(df5['lemmas_string'])\n",
    "df5['less_prep_text'] = np.vectorize(remove_2_word)(df5['less_prep_text'])\n",
    "\n",
    "def remove_3_word(tweet):\n",
    "    tweet_no_3 = re.sub(r\"first\", \"\", tweet)\n",
    "    return tweet_no_3\n",
    "df5['lemmas_string'] = np.vectorize(remove_3_word)(df5['lemmas_string'])\n",
    "df5['less_prep_text'] = np.vectorize(remove_3_word)(df5['less_prep_text'])\n",
    "\n",
    "def remove_4_word(tweet):\n",
    "    tweet_no_4 = re.sub(r\"approve\", \"\", tweet)\n",
    "    return tweet_no_4\n",
    "df5['lemmas_string'] = np.vectorize(remove_4_word)(df5['lemmas_string'])\n",
    "df5['less_prep_text'] = np.vectorize(remove_4_word)(df5['less_prep_text'])\n",
    "\n",
    "def remove_5_word(tweet):\n",
    "    tweet_no_5 = re.sub(r\"week\", \"\", tweet)\n",
    "    return tweet_no_5\n",
    "df5['lemmas_string'] = np.vectorize(remove_5_word)(df5['lemmas_string'])\n",
    "df5['less_prep_text'] = np.vectorize(remove_5_word)(df5['less_prep_text'])\n",
    "\n",
    "def remove_6_word(tweet):\n",
    "    tweet_no_6 = re.sub(r\"begin\", \"\", tweet)\n",
    "    return tweet_no_6\n",
    "df5['lemmas_string'] = np.vectorize(remove_6_word)(df5['lemmas_string'])\n",
    "df5['less_prep_text'] = np.vectorize(remove_6_word)(df5['less_prep_text'])\n",
    "\n",
    "def remove_7_word(tweet):\n",
    "    tweet_no_7 = re.sub(r\"shipment\", \"\", tweet)\n",
    "    return tweet_no_7\n",
    "df5['lemmas_string'] = np.vectorize(remove_7_word)(df5['lemmas_string'])\n",
    "df5['less_prep_text'] = np.vectorize(remove_7_word)(df5['less_prep_text'])\n",
    "\n",
    "def remove_8_word(tweet):\n",
    "    tweet_no_8 = re.sub(r\"today\", \"\", tweet)\n",
    "    return tweet_no_8\n",
    "df5['lemmas_string'] = np.vectorize(remove_8_word)(df5['lemmas_string'])\n",
    "df5['less_prep_text'] = np.vectorize(remove_8_word)(df5['less_prep_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column that is the tokenized lemmas_string (just bc I could not make it work with the lemmas column, had to be a string to do it this way, so now I need to return it to the right format which I can do with the tokenize function bc it splits the string by whitespace)\n",
    "def tokenize(tweet):\n",
    "    for word in tweet:\n",
    "        yield(gensim.utils.simple_preprocess(str(word), deacc=True))  \n",
    "df5['lemmas_reduced'] = list(tokenize(df5['lemmas_string']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0               [never, billion, people, pressure, submit]\n1                                                [require]\n2        [immunization, likely, deliver, shot, small, r...\n3                                       [write, authorize]\n4                                       [break, authorize]\n                               ...                        \n13430    [already, exhibit, death, unknown, volunteer, ...\n13431                               [authorize, emergency]\n13432                            [authorize, supply, next]\n13433                                                   []\n13434    [throttle, push, warp_speed, ahead, baby, bet,...\nName: lemmas_reduced, Length: 13435, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df4['lemmas_reduced'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(8406, 16)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Because I am concerned about the state of my LDA models, I will try to reduce noise by removing short tweets, i.e. tweets that are only one or two lemmas. \n",
    "\n",
    "# RUN FOR HAND IN VERSION OF DF \n",
    "# Remove tweets that only have two or less (AKA below 3) lemmas:\n",
    "#df4['length'] = df4['lemmas_reduced'].apply(len)\n",
    "#df4 = df4.drop(df4[df4['length']<3].index) # evt 4\n",
    "#df4 = df4.drop(['length'], axis=1)\n",
    "#df4.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Also remove duplicates: \n",
    "#df4.drop_duplicates(subset=['lemmas_string'], keep='first',inplace=True)\n",
    "#df4.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#df4.shape \n",
    "# FOR LEMMASREDUCED + BELOW 3 + DROP DUPLICATES = (8406, 16)\n",
    "#(for df4 below 3 + drop duplicates: 9193)\n",
    "\n",
    "###############################################################\n",
    "# CODE BELOW IS ADDED AFTER SYNOPSIS HAND IN \n",
    "# Remove tweets that only have two or less (AKA below 3) lemmas:\n",
    "df5['length'] = df5['lemmas_reduced'].apply(len)\n",
    "df5 = df5.drop(df5[df5['length']<3].index) # evt 4\n",
    "df5 = df5.drop(['length'], axis=1)\n",
    "df5.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Also remove duplicates: \n",
    "df5.drop_duplicates(subset=['lemmas_string'], keep='first',inplace=True)\n",
    "df5.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df5.shape \n",
    "# FOR LEMMASREDUCED + BELOW 3 + DROP DUPLICATES = (8406, 16)\n",
    "\n",
    "# the reason why the df still have 16 columns as the previous version is bc I dont have the stemmed_string column in this updated version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING AND LOADING FINAL VERSION OF THE DATA\n",
    "\n",
    "# Save df4 with the latest alterations so we can begin bulding models \n",
    "# Alterations include: \n",
    "#   - remove all instances of words from the top 10 list of most frequent lemmas (disregarding the previously removed query)\n",
    "#   - Remove all rows that has less than 3 lemmas to work with\n",
    "#   - Remove possible duplicates \n",
    "\n",
    "# save to csv file:\n",
    "#df4.to_csv('final_data_version_minustop10.csv', header=True, mode='a')\n",
    "# save to pickle \n",
    "#df4.to_pickle('final_data_version_minustop10.pkl')\n",
    "\n",
    "# Read data\n",
    "#df4 = pd.read_pickle('final_data_version_minustop10.pkl')\n",
    "# remember to use column lemmas_reduced instead of lemmas! \n",
    "\n",
    "\n",
    "# CODE BELOW ADDED AFTER SYNOPSIS HAND IN\n",
    "# save to csv file:\n",
    "df5.to_csv('oralexamupdated_final_data_version_minustop10.csv', header=True, mode='a')\n",
    "# save to pickle \n",
    "df5.to_pickle('oralexamupdated_final_data_version_minustop10.pkl')\n",
    "\n",
    "# Read data\n",
    "df5 = pd.read_pickle('oralexamupdated_final_data_version_minustop10.pkl')\n",
    "# remember to use column lemmas_reduced instead of lemmas! \n",
    "\n",
    "\n",
    "\n",
    "###### NOW THIS IS TRULY THE END OF THE PREPROCESSING AND WE NOW MOVE ON TO BUILD MODELS ######### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text             location  \\\n",
       "0  The Future of Vaccines\\nhttps://t.co/GI23fycj1...                Earth   \n",
       "1  The immunization task force can likely deliver...  Mississauga, Canada   \n",
       "2  Question if city staff will remain out working...              Toronto   \n",
       "3  @iamIqraKhalid @GovCanHealth @moderna_tx Enoug...      Ontario, Canada   \n",
       "4  .@USForcesKorea will start administering the @...       Washington, DC   \n",
       "\n",
       "   statuses_count  followers_count  friends_count  listed_count  \\\n",
       "0           11485              774           3240             2   \n",
       "1             280              225            675             3   \n",
       "2          251890            19164           9952           607   \n",
       "3           41897             1317            883             5   \n",
       "4           33658            16782           1181           529   \n",
       "\n",
       "   favourites_count                                          tidy_text  \\\n",
       "0              1107  the future never before have billions people b...   \n",
       "1                26  the immunization task force can likely deliver...   \n",
       "2             30736  question city staff will remain out working wi...   \n",
       "3            156959  enough for people big whoppy population canada...   \n",
       "4               558  will start administering the its members this ...   \n",
       "\n",
       "                                      less_prep_text  \\\n",
       "0  The Future of Vaccines\\n\\nNever before have bi...   \n",
       "1  The immunization task force can likely deliver...   \n",
       "2  Question if city staff will remain out working...   \n",
       "3     Enough for 84,000 people!  Big Whoppy Do.  ...   \n",
       "4  . will start administering the  COVID-19  to i...   \n",
       "\n",
       "                                    tidy_text_tokens  \\\n",
       "0  [the, future, never, before, have, billions, p...   \n",
       "1  [the, immunization, task, force, can, likely, ...   \n",
       "2  [question, city, staff, will, remain, out, wor...   \n",
       "3  [enough, for, people, big, whoppy, population,...   \n",
       "4  [will, start, administering, the, its, members...   \n",
       "\n",
       "                                 tokens_no_stopwords  \\\n",
       "0  [future, never, billions, people, pressured, s...   \n",
       "1  [immunization, task, force, likely, deliver, s...   \n",
       "2  [question, city, staff, remain, working, homel...   \n",
       "3  [enough, people, big, whoppy, population, cana...   \n",
       "4              [start, administering, members, week]   \n",
       "\n",
       "                                 no_stopwords_string  \\\n",
       "0      future never billions people pressured submit   \n",
       "1  immunization task force likely deliver shots s...   \n",
       "2  question city staff remain working homeless pe...   \n",
       "3  enough people big whoppy population canada millio   \n",
       "4                   start administering members week   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0         [never, billion, people, pressure, submit]   \n",
       "1  [immunization, likely, deliver, shot, small, r...   \n",
       "2  [question, city, staff, remain, work, homeless...   \n",
       "3                              [enough, people, big]   \n",
       "4                  [start, administer, member, week]   \n",
       "\n",
       "                                       lemmas_string  \\\n",
       "0               never billion people pressure submit   \n",
       "1  immunization likely deliver shot small rural h...   \n",
       "2  question city staff remain work homeless peopl...   \n",
       "3                                  enough people big   \n",
       "4                           start administer member    \n",
       "\n",
       "                                             stemmed  \\\n",
       "0    [futur, never, billion, peopl, pressur, submit]   \n",
       "1  [immun, task, forc, like, deliv, shot, small, ...   \n",
       "2  [question, citi, staff, remain, work, homeless...   \n",
       "3  [enough, peopl, big, whoppi, popul, canada, mi...   \n",
       "4                    [start, administ, member, week]   \n",
       "\n",
       "                                      lemmas_reduced  \n",
       "0         [never, billion, people, pressure, submit]  \n",
       "1  [immunization, likely, deliver, shot, small, r...  \n",
       "2  [question, city, staff, remain, work, homeless...  \n",
       "3                              [enough, people, big]  \n",
       "4                        [start, administer, member]  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>location</th>\n      <th>statuses_count</th>\n      <th>followers_count</th>\n      <th>friends_count</th>\n      <th>listed_count</th>\n      <th>favourites_count</th>\n      <th>tidy_text</th>\n      <th>less_prep_text</th>\n      <th>tidy_text_tokens</th>\n      <th>tokens_no_stopwords</th>\n      <th>no_stopwords_string</th>\n      <th>lemmas</th>\n      <th>lemmas_string</th>\n      <th>stemmed</th>\n      <th>lemmas_reduced</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The Future of Vaccines\\nhttps://t.co/GI23fycj1...</td>\n      <td>Earth</td>\n      <td>11485</td>\n      <td>774</td>\n      <td>3240</td>\n      <td>2</td>\n      <td>1107</td>\n      <td>the future never before have billions people b...</td>\n      <td>The Future of Vaccines\\n\\nNever before have bi...</td>\n      <td>[the, future, never, before, have, billions, p...</td>\n      <td>[future, never, billions, people, pressured, s...</td>\n      <td>future never billions people pressured submit</td>\n      <td>[never, billion, people, pressure, submit]</td>\n      <td>never billion people pressure submit</td>\n      <td>[futur, never, billion, peopl, pressur, submit]</td>\n      <td>[never, billion, people, pressure, submit]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The immunization task force can likely deliver...</td>\n      <td>Mississauga, Canada</td>\n      <td>280</td>\n      <td>225</td>\n      <td>675</td>\n      <td>3</td>\n      <td>26</td>\n      <td>the immunization task force can likely deliver...</td>\n      <td>The immunization task force can likely deliver...</td>\n      <td>[the, immunization, task, force, can, likely, ...</td>\n      <td>[immunization, task, force, likely, deliver, s...</td>\n      <td>immunization task force likely deliver shots s...</td>\n      <td>[immunization, likely, deliver, shot, small, r...</td>\n      <td>immunization likely deliver shot small rural h...</td>\n      <td>[immun, task, forc, like, deliv, shot, small, ...</td>\n      <td>[immunization, likely, deliver, shot, small, r...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Question if city staff will remain out working...</td>\n      <td>Toronto</td>\n      <td>251890</td>\n      <td>19164</td>\n      <td>9952</td>\n      <td>607</td>\n      <td>30736</td>\n      <td>question city staff will remain out working wi...</td>\n      <td>Question if city staff will remain out working...</td>\n      <td>[question, city, staff, will, remain, out, wor...</td>\n      <td>[question, city, staff, remain, working, homel...</td>\n      <td>question city staff remain working homeless pe...</td>\n      <td>[question, city, staff, remain, work, homeless...</td>\n      <td>question city staff remain work homeless peopl...</td>\n      <td>[question, citi, staff, remain, work, homeless...</td>\n      <td>[question, city, staff, remain, work, homeless...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@iamIqraKhalid @GovCanHealth @moderna_tx Enoug...</td>\n      <td>Ontario, Canada</td>\n      <td>41897</td>\n      <td>1317</td>\n      <td>883</td>\n      <td>5</td>\n      <td>156959</td>\n      <td>enough for people big whoppy population canada...</td>\n      <td>Enough for 84,000 people!  Big Whoppy Do.  ...</td>\n      <td>[enough, for, people, big, whoppy, population,...</td>\n      <td>[enough, people, big, whoppy, population, cana...</td>\n      <td>enough people big whoppy population canada millio</td>\n      <td>[enough, people, big]</td>\n      <td>enough people big</td>\n      <td>[enough, peopl, big, whoppi, popul, canada, mi...</td>\n      <td>[enough, people, big]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>.@USForcesKorea will start administering the @...</td>\n      <td>Washington, DC</td>\n      <td>33658</td>\n      <td>16782</td>\n      <td>1181</td>\n      <td>529</td>\n      <td>558</td>\n      <td>will start administering the its members this ...</td>\n      <td>. will start administering the  COVID-19  to i...</td>\n      <td>[will, start, administering, the, its, members...</td>\n      <td>[start, administering, members, week]</td>\n      <td>start administering members week</td>\n      <td>[start, administer, member, week]</td>\n      <td>start administer member</td>\n      <td>[start, administ, member, week]</td>\n      <td>[start, administer, member]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "######## BUID MODELS ########\n",
    "# FIRST WITH GENSIM\n",
    "# THEN WITH MALLET\n",
    "#############################\n",
    "df5.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dictionary(5737 unique tokens: ['billion', 'never', 'people', 'pressure', 'submit']...)\n"
     ]
    }
   ],
   "source": [
    "# Before we can build the LDA model we have to generate a dicitonary and a corpus. These are two main inputs in the model, and can be created with the gensim package. \n",
    "\n",
    "# Create Dictionary:\n",
    "\n",
    "# lemmas dictionary\n",
    "id2word_lemmas = corpora.Dictionary(df4['lemmas_reduced'])\n",
    "print(id2word_lemmas)\n",
    "#Dictionary(5737 unique tokens: ['billion', 'never', 'people', 'pressure', 'submit']...)\n",
    " \n",
    "# tokens dictionary \n",
    "#id2word_tokens = corpora.Dictionary(tweets_df['tokens_no_stopwords'])\n",
    "#print(id2word_tokens)\n",
    "\n",
    "# Create Corpus:\n",
    "\n",
    "# lemmas corpus\n",
    "tweets_lemmas = df4['lemmas_reduced']\n",
    "\n",
    "# token corpus \n",
    "#tweets_tokens = tweets_df['tokens_no_stopwords']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8406"
      ]
     },
     "metadata": {},
     "execution_count": 269
    }
   ],
   "source": [
    "# What the corpus looks like\n",
    "# Corpus length: 8406\n",
    "len(tweets_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A unique ID is assigned to each word and the corpus is represented in version of a tuple as tuple (word_id,word_frequency)\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus_lemmas = [id2word_lemmas.doc2bow(tweet) for tweet in tweets_lemmas]\n",
    "#corpus_tokens = [id2word_tokens.doc2bow(tweet) for tweet in tweets_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### BUILDING THE LDA MODEL ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA models are built with inspiration from an article from machinelearningplus that goes over an example of how to do LDA topic modeling with gensim. More information and more about how to interpret visualisations can be found here https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/ (also used in class)\n",
    "\n",
    "# DONT RUN \n",
    "# Build LDA model - with lemmas\n",
    "# Now we initialize with number of topics k = ?, which will be changed and adjusted \n",
    "# try with 10 topicszzzz\n",
    "lda_model_lemmas = gensim.models.ldamodel.LdaModel(corpus=corpus_lemmas,\n",
    "                                           id2word=id2word_lemmas,\n",
    "                                           num_topics=8, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=16,\n",
    "                                           alpha= 'auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model - with tokens \n",
    "# DONT RUN \n",
    "# Now we initialize with number of topics k = ?, which will be changed and adjusted \n",
    "# try with 10 topicszzzz\n",
    "lda_model_tokens = gensim.models.ldamodel.LdaModel(corpus=corpus_tokens,\n",
    "                                           id2word=id2word_tokens,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=16,\n",
    "                                           alpha= 'auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Perplexity:  -7.93589828684483\n",
      "\n",
      "Coherence Score:  0.6162289980301385\n"
     ]
    }
   ],
   "source": [
    "# Get evaluation metrics - COHERENCE SCORE \n",
    "# lemmas model\n",
    "\n",
    "# DONT RUN\n",
    "\n",
    "# also for fun see perplexity, dont really use this \n",
    "print('\\nPerplexity: ', lda_model_lemmas.log_perplexity(corpus_lemmas))  \n",
    "# offers a measure of how good the model actually is. The lower the better.\n",
    "\n",
    "# Coherence Score\n",
    "# change out the c_v coherence with u_mass for faster computation\n",
    "coherence_model_lda = CoherenceModel(model=lda_model_lemmas, texts=tweets_lemmas, dictionary=id2word_lemmas, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "# With 8 topics I have a coherence score of 0.6162 # from old dataset\n",
    "# 8: perplexity -7.9359 # from old dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Perplexity:  -9.060419745134636\n",
      "\n",
      "Coherence Score:  0.5910795641425967\n"
     ]
    }
   ],
   "source": [
    "# Get evaluation metrics - COHERENCE SCORE \n",
    "# lemmas model\n",
    "\n",
    "# DONT RUN \n",
    "\n",
    "# also for fun see perplexity, dont really use this \n",
    "# Perplexity\n",
    "print('\\nPerplexity: ', lda_model_tokens.log_perplexity(corpus_tokens))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda_tokens = CoherenceModel(model=lda_model_tokens, texts=tweets_tokens, dictionary=id2word_tokens, coherence='c_v')\n",
    "coherence_lda_tokens = coherence_model_lda_tokens.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_tokens)\n",
    "\n",
    "# With 10 topics I have a coherence score of 0.6327\n",
    "# 10 : perplexity -8.3956"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/Users/Astridll/Desktop/Kandidat CogSci/1_Semester/Natural Language Processing/NLP_exam_code'"
      ]
     },
     "metadata": {},
     "execution_count": 165
    }
   ],
   "source": [
    "############### Visualisation of topic modeling #######\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "5      0.077017  0.275813       1        1  14.010238\n",
       "7     -0.341766  0.009203       2        1  13.615231\n",
       "2      0.065843 -0.215016       3        1  13.354447\n",
       "0      0.042383  0.002677       4        1  12.891637\n",
       "6      0.042937 -0.003293       5        1  12.384558\n",
       "4      0.054033 -0.079024       6        1  12.182258\n",
       "3      0.035738  0.010766       7        1  11.596590\n",
       "1      0.023816 -0.001126       8        1   9.965042, topic_info=               Term        Freq       Total Category  logprob  loglift\n",
       "60        emergency  131.000000  131.000000  Default  30.0000  30.0000\n",
       "96         approval   99.000000   99.000000  Default  29.0000  29.0000\n",
       "9             trial   89.000000   89.000000  Default  28.0000  28.0000\n",
       "50        effective   79.000000   79.000000  Default  27.0000  27.0000\n",
       "14            first   71.000000   71.000000  Default  26.0000  26.0000\n",
       "...             ...         ...         ...      ...      ...      ...\n",
       "224         promise    7.313097    8.032976   Topic8  -5.0188   2.2122\n",
       "525          reason    7.043385    7.763785   Topic8  -5.0563   2.2087\n",
       "2151            mad    6.587224    7.308352   Topic8  -5.1233   2.2022\n",
       "1583            man    6.325815    7.048477   Topic8  -5.1638   2.1979\n",
       "222   effectiveness    6.109761    6.829062   Topic8  -5.1985   2.1948\n",
       "\n",
       "[270 rows x 6 columns], token_table=      Topic      Freq            Term\n",
       "term                                 \n",
       "382       3  0.970381          accord\n",
       "605       1  0.977752  administration\n",
       "569       4  0.952269             ago\n",
       "448       7  0.937132           allow\n",
       "266       1  0.961083         already\n",
       "...     ...       ...             ...\n",
       "82        1  0.987731            week\n",
       "308       2  0.908786            well\n",
       "304       1  0.992557            work\n",
       "56        2  0.975204            year\n",
       "309       8  0.936744             yet\n",
       "\n",
       "[240 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[6, 8, 3, 1, 7, 5, 4, 2])"
      ],
      "text/html": "\n<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n\n\n<div id=\"ldavis_el279561125332771369678741069\"></div>\n<script type=\"text/javascript\">\n\nvar ldavis_el279561125332771369678741069_data = {\"mdsDat\": {\"x\": [0.07701718401273933, -0.3417659099922741, 0.06584274978137816, 0.042382855822781694, 0.042936519239532074, 0.05403343141002347, 0.035737641115283145, 0.023815528610536058], \"y\": [0.27581321360707883, 0.009202773655615067, -0.21501555023155958, 0.0026766370635686295, -0.0032927159687074735, -0.07902404393123859, 0.010765931330493915, -0.0011262455252507918], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [14.010237643972747, 13.6152306998038, 13.354447016322046, 12.891636553759309, 12.384557660650506, 12.182258402363729, 11.596589804192334, 9.965042218935526]}, \"tinfo\": {\"Term\": [\"emergency\", \"approval\", \"trial\", \"effective\", \"first\", \"say\", \"trump\", \"stock\", \"today\", \"authorization\", \"ask\", \"dose\", \"company\", \"work\", \"datum\", \"news\", \"go\", \"know\", \"seek\", \"apply\", \"week\", \"need\", \"good\", \"result\", \"buy\", \"case\", \"people\", \"severe\", \"think\", \"new\", \"work\", \"week\", \"approve\", \"efficacy\", \"tell\", \"great\", \"second\", \"speed\", \"high\", \"already\", \"coronavirus\", \"bad\", \"find\", \"administration\", \"hour\", \"claim\", \"time\", \"help\", \"bill_gate\", \"infection\", \"public\", \"record\", \"watch\", \"ready\", \"different\", \"meet\", \"next\", \"patient\", \"low\", \"make\", \"emergency\", \"approval\", \"seek\", \"apply\", \"case\", \"receive\", \"candidate\", \"look\", \"file\", \"money\", \"year\", \"begin\", \"mean\", \"set\", \"review\", \"design\", \"become\", \"race\", \"top\", \"eua\", \"success\", \"government\", \"question\", \"authorize\", \"well\", \"side_effect\", \"regulatory\", \"distribute\", \"benefit\", \"founder\", \"trial\", \"say\", \"company\", \"severe\", \"share\", \"right\", \"develop\", \"announce\", \"phase\", \"let\", \"move\", \"volunteer\", \"last\", \"pandemic\", \"disease\", \"deal\", \"meeting\", \"accord\", \"death\", \"manufacture\", \"credit\", \"note\", \"side\", \"seem\", \"easy\", \"participant\", \"true\", \"prove\", \"vacci\", \"possible\", \"trump\", \"dose\", \"know\", \"people\", \"day\", \"available\", \"country\", \"expect\", \"require\", \"health\", \"base\", \"come\", \"front\", \"line\", \"likely\", \"ingenuity\", \"storage\", \"ago\", \"late\", \"crazy\", \"die\", \"real\", \"always\", \"course\", \"school\", \"temperature\", \"trust\", \"far\", \"decide\", \"term\", \"ask\", \"go\", \"keep\", \"report\", \"take\", \"read\", \"thing\", \"test\", \"virus\", \"rate\", \"market\", \"shot\", \"early\", \"never\", \"feel\", \"create\", \"person\", \"enough\", \"fast\", \"cold\", \"hospital\", \"listen\", \"morning\", \"product\", \"choice\", \"super\", \"end\", \"testing\", \"percent\", \"long\", \"effective\", \"authorization\", \"news\", \"need\", \"result\", \"new\", \"release\", \"show\", \"request\", \"submit\", \"prevent\", \"distribution\", \"scientist\", \"plan\", \"send\", \"clearance\", \"drug\", \"medical\", \"discuss\", \"point\", \"analysis\", \"step\", \"final\", \"state\", \"prepare\", \"large\", \"interest\", \"probably\", \"potential\", \"cent\", \"stock\", \"today\", \"datum\", \"good\", \"buy\", \"even\", \"update\", \"sure\", \"do\", \"clinical\", \"allow\", \"part\", \"purchase\", \"old\", \"soon\", \"interview\", \"big\", \"back\", \"study\", \"believe\", \"full\", \"open\", \"put\", \"call\", \"price\", \"european\", \"sell\", \"confirm\", \"gain\", \"genetic\", \"first\", \"think\", \"start\", \"month\", \"really\", \"get\", \"thank\", \"wait\", \"hope\", \"yet\", \"fact\", \"modern\", \"team\", \"close\", \"vaccinate\", \"try\", \"live\", \"research\", \"deliver\", \"invest\", \"own\", \"fund\", \"sound\", \"child\", \"treatment\", \"promise\", \"reason\", \"mad\", \"man\", \"effectiveness\"], \"Freq\": [131.0, 99.0, 89.0, 79.0, 71.0, 76.0, 74.0, 69.0, 66.0, 68.0, 66.0, 67.0, 65.0, 64.0, 53.0, 53.0, 54.0, 54.0, 54.0, 53.0, 53.0, 48.0, 46.0, 46.0, 45.0, 49.0, 47.0, 47.0, 40.0, 44.0, 63.76640643560414, 52.94490156625579, 46.54192106034063, 45.713994208614864, 31.47289472731893, 30.523399280109732, 29.077272835342416, 28.447859367665075, 21.582092218270784, 20.09660715875647, 19.828347355308157, 19.42241638845931, 19.057250114904903, 18.71877326844401, 18.40422003030671, 17.88698295887627, 17.13055759060755, 17.08649418780636, 16.890965281498683, 16.342096511005206, 14.428985460133607, 14.225296664418305, 14.181366484301321, 14.167916785487117, 13.66487372221525, 13.327349707560964, 13.17747853204699, 12.614909546170093, 12.43704712624628, 12.418297481516918, 131.08758235031837, 98.69373762547251, 53.49573801059838, 52.4519840559904, 48.34243329616488, 37.88883426494287, 35.64407230501689, 30.23535816932746, 28.861174538211888, 25.244573418099794, 24.91422254733773, 23.324282857421313, 22.469856891953214, 22.070957687790976, 19.99809779652502, 18.552441575952006, 17.353343202717603, 17.14989491267367, 15.314986155538309, 15.15833159139248, 14.379585879294831, 13.155483568800786, 11.956879138721387, 10.705401682374587, 10.282172611526065, 9.141429189232346, 8.9652610559729, 8.563250432107765, 7.518092046534842, 7.299268192640233, 89.19462997215061, 76.12906278761083, 64.82538227306526, 46.39147010676213, 32.00002267727695, 30.087764309303584, 28.99816233408038, 28.187986535528303, 23.054891506446936, 21.516094422525022, 19.074811796832126, 17.3461183727211, 17.13503318033836, 15.872733294887137, 15.154281996030406, 14.876022165532405, 14.02768407231226, 13.709137678790647, 13.628041422329678, 11.949473277836379, 11.070937861062971, 10.646828166980052, 9.986661739933256, 9.733891059264822, 9.236885419507354, 8.734968703200808, 8.624082309128672, 8.564286708458994, 8.26476072945293, 8.037292169138963, 73.96459449373685, 66.7521909338979, 53.49900178779884, 46.305327230653674, 31.56123698451838, 29.077781353988605, 27.606259495214356, 19.873455885213414, 19.117336359343764, 17.24530218641872, 15.519090674692116, 15.126603743525434, 15.104862057629411, 13.66973357183292, 13.52060908446237, 12.686059861128369, 12.626861502300857, 11.88233301189174, 11.480172449567057, 11.251850770028712, 10.58077192247609, 10.05818864302314, 9.719772113450974, 9.436216131930165, 9.426626445505352, 9.330217251893547, 9.014819567372355, 8.505593493917116, 8.441209882245063, 8.329226941942524, 65.87303233171292, 53.53822366976766, 36.89210200802146, 29.253999834109383, 27.321026420254125, 26.088560373998188, 23.76231095276106, 23.56559614635025, 17.95998963075024, 17.261453532960996, 17.247805689639556, 16.64033329675844, 14.871459134274575, 14.648514918981792, 14.30916594135877, 14.063829893087004, 13.776590280852508, 13.54261975507662, 12.768680026961357, 12.202542857915908, 12.112430226688874, 12.03964812920119, 11.621111707253075, 10.621637592117311, 10.116941318924962, 9.893174034484206, 9.721098180631332, 9.668314231353255, 9.60598242394963, 9.558689869142476, 79.18005016570305, 67.28382344003496, 53.19288732653202, 47.8841632489777, 45.97741323882368, 43.77940995108672, 40.138633177327584, 36.772836606864374, 29.10962381350187, 28.62979980306728, 21.687664005248, 18.338865771435586, 18.346579238866905, 13.578270057522959, 13.527661602337359, 11.974103031113664, 11.509873420636126, 11.419375535443315, 11.095899707250688, 10.010469031709784, 9.8138366803095, 9.758591456626375, 9.369250851320043, 9.201923002737713, 9.100783529233476, 8.049958948841505, 7.943386932857279, 7.909565909034404, 7.890530919825629, 7.764458663862814, 68.56668841918297, 65.69839929542513, 52.4525961379975, 45.41038028719871, 44.76311101470725, 28.766347526058436, 26.406714100690095, 15.27420880029412, 14.869814730986779, 14.539234193785363, 14.220510231606143, 13.694546714441836, 13.663758214381675, 12.632622137426187, 12.383797071887619, 12.246996983229295, 12.132097757774117, 11.681239866111806, 11.22632590390005, 11.110634402253034, 10.870605879788453, 10.815268803805367, 9.824841459741743, 8.939169568952458, 8.54656315502884, 8.109195533471137, 7.677846094605149, 7.639717353396998, 7.587107097892631, 7.597687161350063, 70.70013600860455, 39.822401867581945, 34.08395435845577, 30.8508704994659, 27.115275250279822, 26.676012173123787, 23.71143289371749, 20.10916296256486, 18.140544799628884, 17.428646968721562, 15.069180433599037, 14.154534309738933, 14.07746381781329, 12.87323892642806, 12.615591246071892, 11.305410392802521, 11.3029528876743, 11.145907392313548, 11.033766468864037, 9.293737481597505, 8.786738518122315, 8.011457448285553, 7.708626074522024, 7.544991869896715, 7.475502976985675, 7.313097009940168, 7.043384792149203, 6.587223534822937, 6.3258149240494586, 6.109760981474325], \"Total\": [131.0, 99.0, 89.0, 79.0, 71.0, 76.0, 74.0, 69.0, 66.0, 68.0, 66.0, 67.0, 65.0, 64.0, 53.0, 53.0, 54.0, 54.0, 54.0, 53.0, 53.0, 48.0, 46.0, 46.0, 45.0, 49.0, 47.0, 47.0, 40.0, 44.0, 64.47994167048846, 53.65830943962262, 47.25523946203104, 46.42756656916902, 32.186052606794256, 31.23677794905634, 29.790771189698248, 29.161398977600804, 22.295358279130753, 20.809859599410384, 20.54169081448198, 20.136130032537903, 19.770557345339274, 19.432326183730115, 19.11772008463639, 18.601312425890278, 17.84394720098641, 17.79965458922999, 17.604205055746963, 17.055986296813792, 15.142258155121421, 14.93851638238849, 14.89449847316578, 14.881254449207487, 14.378113575692554, 14.040735945651122, 13.890451584825573, 13.328221476518888, 13.150875125988895, 13.132125814270177, 131.80868888864333, 99.41476154569746, 54.21674155362137, 53.17300331452163, 49.064130783467135, 38.61002542926886, 36.36545520068727, 30.956760538636264, 29.582230401962303, 25.96628661469662, 25.63566964369079, 24.045445432255637, 23.191716326187066, 22.792156534769163, 20.719385717281007, 19.273752700099653, 18.074703046464474, 17.87150858414214, 16.037090796349958, 15.880258793841952, 15.101329455511962, 13.876550201042322, 12.6781695750619, 11.426670032405795, 11.00369030935202, 9.862969867867271, 9.686590354487294, 9.284316924689115, 8.240791654360367, 8.020459176912976, 89.91290272594532, 76.84753305717706, 65.54383605398284, 47.1096559575431, 32.71832631017702, 30.80621997474065, 29.716191390963935, 28.906241170581442, 23.77338618876343, 22.234404872213148, 19.79343650068017, 18.06425717492322, 17.85322240119061, 16.59131868442177, 15.872585379056595, 15.594858656833223, 14.745857507056828, 14.427324772523278, 14.347108526859536, 12.668558384366959, 11.78938405502401, 11.365317110090421, 10.705301459991492, 10.45228262151951, 9.955268810287388, 9.453216450205494, 9.342479244115035, 9.283068248150348, 8.992307865876654, 8.75654936789613, 74.68382263727742, 67.4709529896741, 54.217857179286405, 47.024234063399625, 32.280339512301865, 29.79658335882327, 28.3252015798641, 20.59220533711367, 19.835868731407803, 17.964148739820825, 16.238042458797675, 15.845499489134362, 15.824203564057912, 14.388357230355917, 14.239666443907455, 13.405521598501723, 13.345842967927425, 12.601485643222277, 12.199499523999245, 11.97090330351526, 11.299540274071777, 10.777294586343467, 10.439344903175362, 10.155645361168709, 10.147566068602606, 10.050760323823871, 9.733813963326213, 9.224356376085261, 9.160117574906183, 9.047954117835594, 66.59209176242491, 54.25713023949159, 37.61140942023484, 29.973480018456343, 28.039822736124663, 26.80752459114762, 24.481623634041412, 24.284550123747902, 18.678642611258148, 17.98063114475632, 17.96654997390449, 17.359163314059952, 15.590314541025808, 15.367205984293514, 15.028255173008537, 14.782645742901138, 14.495993740011446, 14.261832683965263, 13.48801226508697, 12.921114506442123, 12.831468081644694, 12.758986610591842, 12.34093575905211, 11.34014625314047, 10.836106621185696, 10.612214311963534, 10.439882420516797, 10.387677369636897, 10.327912873748845, 10.277347554980862, 79.90158242385981, 68.00553052423902, 53.914848242436435, 48.605990944507134, 46.69923458202101, 44.501502012609905, 40.860345526311434, 37.49442812150613, 29.831351443623866, 29.351595563035456, 22.40908328251519, 19.060503203598632, 19.069097117378092, 14.299840919053679, 14.250054961865672, 12.695572956925455, 12.231629764693876, 12.14148428429768, 11.817943767900864, 10.73235431544711, 10.535671150025044, 10.480594737571876, 10.090867775123877, 9.924164536936647, 9.82258794074673, 8.779863626334116, 8.665646286934308, 8.631066869733932, 8.61336357455003, 8.486595679887609, 69.28484507790603, 66.41675013986716, 53.1711616695531, 46.128715058431645, 45.481148496179564, 29.484759432183225, 27.125660143608155, 15.992554393408708, 15.5878598758302, 15.258478905909097, 14.939199594356909, 14.413045980338135, 14.382018154736896, 13.350854217818, 13.102337420151455, 12.96541527892479, 12.850232496013366, 12.399205941703272, 11.944733103115052, 11.829607907086096, 11.589868563032317, 11.533398829173946, 10.543523100417772, 9.657400001788504, 9.26481866497236, 8.828545014296582, 8.39591699450631, 8.357755133896376, 8.305179645059932, 8.3176062155432, 71.41947266233939, 40.54177778799874, 34.80340827613737, 31.570218336844807, 27.83495514622767, 27.395621381598193, 24.430982622810888, 20.828709415737197, 18.860253124970484, 18.14796536752625, 15.788928532619222, 14.874338378244989, 14.797090001618821, 13.592880461969605, 13.334735807149427, 12.024987818366188, 12.022837864420651, 11.865598414780447, 11.754037715189964, 10.012976097573015, 9.505999600105637, 8.730632330560015, 8.4286503496589, 8.264404394969798, 8.196092482638687, 8.032976482195554, 7.763784911987026, 7.308352466845577, 7.048476513454373, 6.829061997633487], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.1939001083374023, -3.3798999786376953, -3.5088000297546387, -3.526700019836426, -3.9000000953674316, -3.9305999279022217, -3.9791998863220215, -4.001100063323975, -4.277299880981445, -4.348599910736084, -4.361999988555908, -4.382699966430664, -4.401700019836426, -4.419600009918213, -4.436600208282471, -4.465099811553955, -4.508299827575684, -4.510799884796143, -4.52239990234375, -4.5553998947143555, -4.679900169372559, -4.6940999031066895, -4.697199821472168, -4.698200225830078, -4.734300136566162, -4.759300231933594, -4.770599842071533, -4.814300060272217, -4.828499794006348, -4.829999923706055, -2.444700002670288, -2.7284998893737793, -3.34089994430542, -3.360599994659424, -3.442199945449829, -3.6858999729156494, -3.746999979019165, -3.9114999771118164, -3.9579999446868896, -4.091899871826172, -4.105100154876709, -4.171000003814697, -4.208399772644043, -4.22629976272583, -4.324900150299072, -4.399899959564209, -4.466800212860107, -4.478499889373779, -4.591700077056885, -4.6020002365112305, -4.654699802398682, -4.74370002746582, -4.839200019836426, -4.94980001449585, -4.990099906921387, -5.107699871063232, -5.127200126647949, -5.173099994659424, -5.303199768066406, -5.332799911499023, -2.8104000091552734, -2.9688000679016113, -3.129499912261963, -3.464099884033203, -3.8355000019073486, -3.8970999717712402, -3.934000015258789, -3.9623000621795654, -4.163300037384033, -4.232399940490723, -4.352799892425537, -4.447800159454346, -4.460100173950195, -4.536600112915039, -4.582900047302246, -4.601399898529053, -4.660200119018555, -4.68310022354126, -4.6890997886657715, -4.820499897003174, -4.896900177001953, -4.9359002113342285, -4.999899864196777, -5.025599956512451, -5.078000068664551, -5.133900165557861, -5.146599769592285, -5.153600215911865, -5.189199924468994, -5.217100143432617, -2.9623000621795654, -3.0648999214172363, -3.28629994392395, -3.4307000637054443, -3.813999891281128, -3.8959999084472656, -3.9479000568389893, -4.276500225067139, -4.315299987792969, -4.418399810791016, -4.523900032043457, -4.549499988555908, -4.550899982452393, -4.650700092315674, -4.6616997718811035, -4.725399971008301, -4.730100154876709, -4.790900230407715, -4.825300216674805, -4.845399856567383, -4.906899929046631, -4.957499980926514, -4.991799831390381, -5.021399974822998, -5.02239990234375, -5.032700061798096, -5.0671000480651855, -5.125199794769287, -5.132800102233887, -5.146200180053711, -3.038100004196167, -3.2453999519348145, -3.617799997329712, -3.8498001098632812, -3.918100118637085, -3.9642999172210693, -4.057700157165527, -4.065999984741211, -4.337699890136719, -4.377299785614014, -4.3780999183654785, -4.414000034332275, -4.526400089263916, -4.541500091552734, -4.564899921417236, -4.582200050354004, -4.602799892425537, -4.619999885559082, -4.678800106048584, -4.7241997718811035, -4.731599807739258, -4.737599849700928, -4.7729997634887695, -4.8628997802734375, -4.911600112915039, -4.934000015258789, -4.951499938964844, -4.956900119781494, -4.963399887084961, -4.968299865722656, -2.837599992752075, -3.0004000663757324, -3.2353999614715576, -3.3405001163482666, -3.381200075149536, -3.4302000999450684, -3.5169999599456787, -3.604599952697754, -3.8382999897003174, -3.8548998832702637, -4.132599830627441, -4.300300121307373, -4.299900054931641, -4.600900173187256, -4.604599952697754, -4.726600170135498, -4.76609992980957, -4.77400016784668, -4.802800178527832, -4.905700206756592, -4.92549991607666, -4.93120002746582, -4.97189998626709, -4.9899001121521, -5.000999927520752, -5.123700141906738, -5.13700008392334, -5.141300201416016, -5.143700122833252, -5.159800052642822, -2.932300090789795, -2.9749999046325684, -3.200200080871582, -3.3443000316619873, -3.3587000370025635, -3.8008999824523926, -3.886399984359741, -4.433899879455566, -4.460700035095215, -4.4832000732421875, -4.50540018081665, -4.543099880218506, -4.545300006866455, -4.623799800872803, -4.643700122833252, -4.654799938201904, -4.6641998291015625, -4.702099800109863, -4.741799831390381, -4.752200126647949, -4.77400016784668, -4.779099941253662, -4.875100135803223, -4.969600200653076, -5.014500141143799, -5.0671000480651855, -5.121699810028076, -5.126699924468994, -5.133600234985352, -5.132199764251709, -2.75, -3.3239998817443848, -3.479599952697754, -3.5792999267578125, -3.7083001136779785, -3.7246999740600586, -3.8424999713897705, -4.007299900054932, -4.110300064086914, -4.150300025939941, -4.29580020904541, -4.358399868011475, -4.363900184631348, -4.4532999992370605, -4.473499774932861, -4.583199977874756, -4.583399772644043, -4.597400188446045, -4.607500076293945, -4.779099941253662, -4.83519983291626, -4.927599906921387, -4.966100215911865, -4.987500190734863, -4.996799945831299, -5.018799781799316, -5.056300163269043, -5.123300075531006, -5.16379976272583, -5.198500156402588], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.9543, 1.952, 1.9502, 1.9499, 1.943, 1.9423, 1.9411, 1.9406, 1.9329, 1.9305, 1.93, 1.9293, 1.9286, 1.928, 1.9273, 1.9262, 1.9246, 1.9245, 1.924, 1.9226, 1.9171, 1.9165, 1.9163, 1.9163, 1.9145, 1.9132, 1.9127, 1.9104, 1.9096, 1.9095, 1.9885, 1.9867, 1.9806, 1.9803, 1.9792, 1.9751, 1.9739, 1.9704, 1.9693, 1.9658, 1.9654, 1.9635, 1.9624, 1.9618, 1.9585, 1.9558, 1.9533, 1.9528, 1.9479, 1.9475, 1.945, 1.9406, 1.9354, 1.9288, 1.9262, 1.918, 1.9166, 1.9131, 1.9022, 1.8998, 2.0053, 2.0039, 2.0023, 1.998, 1.9911, 1.9897, 1.9889, 1.9882, 1.9826, 1.9805, 1.9763, 1.9728, 1.9723, 1.969, 1.967, 1.9661, 1.9634, 1.9623, 1.9619, 1.9549, 1.9504, 1.948, 1.9438, 1.9421, 1.9384, 1.9343, 1.9333, 1.9327, 1.929, 1.9276, 2.0389, 2.0379, 2.0352, 2.0332, 2.0261, 2.0242, 2.0229, 2.0131, 2.0117, 2.0078, 2.0033, 2.0022, 2.0021, 1.9974, 1.9968, 1.9934, 1.9932, 1.9898, 1.9878, 1.9866, 1.9829, 1.9795, 1.9772, 1.9751, 1.9749, 1.9742, 1.9719, 1.9675, 1.9669, 1.9658, 2.0779, 2.0754, 2.0694, 2.0644, 2.0628, 2.0615, 2.0589, 2.0587, 2.0495, 2.0479, 2.0479, 2.0464, 2.0415, 2.0408, 2.0397, 2.0389, 2.0378, 2.037, 2.0339, 2.0315, 2.0311, 2.0307, 2.0286, 2.0233, 2.02, 2.0186, 2.0174, 2.017, 2.0163, 2.0162, 2.0961, 2.0945, 2.0917, 2.0902, 2.0896, 2.0888, 2.0874, 2.0858, 2.0807, 2.0803, 2.0725, 2.0666, 2.0666, 2.0534, 2.0532, 2.0467, 2.0444, 2.0439, 2.0421, 2.0356, 2.0342, 2.0338, 2.031, 2.0296, 2.0289, 2.0184, 2.0182, 2.0179, 2.0175, 2.0163, 2.144, 2.1436, 2.1409, 2.1388, 2.1385, 2.1298, 2.1276, 2.1085, 2.1073, 2.1062, 2.1052, 2.1033, 2.1032, 2.0992, 2.0981, 2.0975, 2.097, 2.0948, 2.0924, 2.0918, 2.0904, 2.0902, 2.0839, 2.0772, 2.0738, 2.0695, 2.0651, 2.0646, 2.064, 2.0639, 2.296, 2.2882, 2.2852, 2.283, 2.2799, 2.2795, 2.2762, 2.2709, 2.2672, 2.2656, 2.2594, 2.2565, 2.2562, 2.2517, 2.2506, 2.2444, 2.2443, 2.2435, 2.2429, 2.2315, 2.2274, 2.2201, 2.2168, 2.215, 2.2141, 2.2122, 2.2087, 2.2022, 2.1979, 2.1948]}, \"token.table\": {\"Topic\": [3, 1, 4, 7, 1, 4, 6, 3, 2, 2, 1, 5, 6, 2, 4, 7, 1, 4, 2, 2, 7, 2, 7, 1, 7, 7, 2, 2, 6, 8, 5, 1, 6, 7, 8, 5, 4, 3, 7, 1, 4, 4, 4, 5, 3, 7, 4, 3, 3, 4, 8, 2, 3, 4, 1, 6, 3, 2, 6, 7, 4, 6, 5, 3, 6, 8, 1, 2, 5, 5, 2, 7, 7, 4, 8, 4, 5, 5, 2, 6, 1, 8, 2, 4, 7, 8, 7, 7, 8, 5, 7, 2, 1, 4, 1, 1, 8, 5, 1, 1, 4, 6, 7, 8, 5, 4, 6, 3, 4, 3, 4, 4, 5, 8, 5, 2, 1, 8, 1, 8, 3, 5, 2, 6, 1, 3, 8, 2, 8, 5, 3, 6, 5, 6, 6, 1, 3, 7, 7, 8, 3, 7, 3, 1, 4, 5, 5, 3, 6, 6, 3, 6, 6, 6, 7, 6, 5, 8, 3, 1, 7, 7, 2, 2, 5, 5, 1, 4, 8, 8, 2, 1, 2, 6, 5, 6, 4, 8, 6, 2, 3, 3, 4, 6, 1, 2, 3, 7, 6, 2, 3, 3, 5, 6, 3, 2, 7, 8, 1, 8, 6, 6, 7, 4, 7, 6, 2, 5, 7, 5, 8, 1, 4, 4, 5, 5, 8, 5, 8, 1, 7, 2, 8, 3, 3, 4, 4, 8, 7, 3, 8, 5, 3, 8, 1, 1, 2, 1, 2, 8], \"Freq\": [0.9703808724582734, 0.977752216608422, 0.9522686721032938, 0.9371318665083181, 0.9610828897935799, 0.9579145140571296, 0.9491564284422668, 0.9686489445226194, 0.9779398709607723, 0.9958279682086567, 0.99459870556288, 0.9911086775207892, 0.9852139889728436, 0.9626601598544661, 0.9732659496818655, 0.9678039107036209, 0.9435775379528224, 0.9853404460912278, 0.9405410399439623, 0.956522101651183, 0.9298702109484838, 0.9707805190981914, 0.9338352441267394, 0.9656783675358452, 0.9894209246668434, 0.9319278479024626, 0.9899504846379493, 0.9783114310500389, 0.9426630302371076, 0.9680068420743387, 0.9228406797372202, 0.9676736559161632, 0.9452113772820298, 0.983059982092383, 0.9563830150917331, 0.928712456964693, 0.9466410326973825, 0.9917027124635346, 0.9571948294530148, 0.9736296870898238, 0.9885190020996962, 0.8862066052850316, 0.9188947334300032, 0.947056450075794, 0.933042807720933, 0.9779737430445546, 0.9913154720013081, 0.9618554633983443, 0.9758063775561671, 0.8733512353504846, 0.9358486221108933, 0.9857966061742487, 0.9758989507928088, 0.9734909326568701, 0.9737021429339807, 0.9307879793672305, 0.945025630152984, 0.9693766459077833, 0.9443612168959732, 0.9622873261298873, 0.9930199149588687, 0.9810630497202861, 0.962135815831528, 0.9040438959016104, 0.9887163383188443, 0.8785979688102427, 0.9907906745762345, 0.9938646769385094, 0.9578651939937248, 0.9816410211950078, 0.9445689893804943, 0.9061515784362123, 0.9835589829620892, 0.9712412863304968, 0.950032801086576, 0.9756778286811512, 0.9638188151451964, 0.931578539147024, 0.9803182385489202, 0.8918955436307374, 0.961025006433573, 0.994126634561941, 0.872767985672144, 0.9479150049655608, 0.949104809961884, 0.9163139274571709, 0.9632542993526386, 0.9618151896936782, 0.9855589557145824, 0.9952608949578311, 0.9755311836238687, 0.936832268226401, 0.9924198984465524, 0.9463292831859261, 0.9550747130950599, 0.9867524766620495, 0.9543880392659454, 0.9352008611677021, 0.9415348650525214, 0.9380870576208742, 0.9697496590847277, 0.9231856153720536, 0.9255391934499725, 0.8988336646665377, 0.983744043904244, 0.977537710956388, 0.9111758838719304, 0.9522090532444323, 0.9016763333905993, 0.9894575603187793, 0.9831690970535341, 0.9730089249148903, 0.9405135663390561, 0.9149254214391804, 0.9730137028550283, 0.9690936479790203, 0.9124868029721822, 0.9578082107774055, 0.9137896003829068, 0.8512477822047069, 0.9472269563684561, 0.946202806030743, 0.9486145695546718, 0.9059847826205287, 0.9258773934870934, 0.9494191838826676, 0.9412183348253134, 0.9627868771135834, 0.9819380933397183, 0.9723735893526524, 0.9599141614113899, 0.987532587388271, 0.9761045706897644, 0.988730672226124, 0.9830316086892672, 0.9358946986433231, 0.9678568484669835, 0.9737204667136765, 0.9537518092390334, 0.9467705005899627, 0.9643597536959506, 0.9713422144839058, 0.9520569054360708, 0.9753739478971644, 0.978219016560297, 0.9682498412063173, 0.9657840815257517, 0.96746840426422, 0.9790318703018466, 0.9317620072985263, 0.9136018840172542, 0.9287893087013834, 0.9162554770994297, 0.9817447560278211, 0.971416746020776, 0.926884256690577, 0.9700051264289231, 0.8714080036851766, 0.969507037912088, 0.9245648737843578, 0.9734377922050479, 0.9484495746591349, 0.9465088733001435, 0.9512347499910861, 0.9454618062702265, 0.9698769430052382, 0.9407809031009198, 0.9278766502932543, 0.9700033593788339, 0.9016220927491477, 0.9842003359882167, 0.9371747261665865, 0.9291195013558892, 0.9789442425111793, 0.9675219554800805, 0.9721316198096162, 0.9578607449602496, 0.9270497462898956, 0.985026851333229, 0.9652795827493571, 0.9738293118921535, 0.9889712392387864, 0.886912185558144, 0.9439356194581546, 0.9734558335310333, 0.9775578258900346, 0.9567288182020323, 0.9528441032986189, 0.9824523510586562, 0.9652443359819534, 0.9764452544815195, 0.9780451388812763, 0.9793098718203169, 0.9868132907667276, 0.9341166185158458, 0.9125040551245366, 0.9158671170797315, 0.9491436550483735, 0.9601734135425777, 0.9769158161245887, 0.9068773463502133, 0.9541443258130198, 0.995888782350805, 0.9740860904209235, 0.9209079771846325, 0.9880212453091223, 0.9270706954141724, 0.9423104081800004, 0.9379364691222943, 0.9629162157724691, 0.9461319758458171, 0.9631501066227709, 0.8954546432339854, 0.8841777816081277, 0.988282668515665, 0.962679109502373, 0.9823591777103355, 0.9803271367438342, 0.9866365557319215, 0.9527040070517718, 0.9937252253537018, 0.9353317375626508, 0.854065521445457, 0.9898468106548862, 0.963341717421447, 0.9908437649128032, 0.9246118771027492, 0.914761841438152, 0.9585020184707503, 0.8896492557108514, 0.9748974548884607, 0.9636674556399987, 0.9410849189857294, 0.9602131174238254, 0.9399443710859197, 0.9877314539630928, 0.9087860271295544, 0.9925567291462343, 0.9752037043492159, 0.9367441283759332], \"Term\": [\"accord\", \"administration\", \"ago\", \"allow\", \"already\", \"always\", \"analysis\", \"announce\", \"apply\", \"approval\", \"approve\", \"ask\", \"authorization\", \"authorize\", \"available\", \"back\", \"bad\", \"base\", \"become\", \"begin\", \"believe\", \"benefit\", \"big\", \"bill_gate\", \"buy\", \"call\", \"candidate\", \"case\", \"cent\", \"child\", \"choice\", \"claim\", \"clearance\", \"clinical\", \"close\", \"cold\", \"come\", \"company\", \"confirm\", \"coronavirus\", \"country\", \"course\", \"crazy\", \"create\", \"credit\", \"datum\", \"day\", \"deal\", \"death\", \"decide\", \"deliver\", \"design\", \"develop\", \"die\", \"different\", \"discuss\", \"disease\", \"distribute\", \"distribution\", \"do\", \"dose\", \"drug\", \"early\", \"easy\", \"effective\", \"effectiveness\", \"efficacy\", \"emergency\", \"end\", \"enough\", \"eua\", \"european\", \"even\", \"expect\", \"fact\", \"far\", \"fast\", \"feel\", \"file\", \"final\", \"find\", \"first\", \"founder\", \"front\", \"full\", \"fund\", \"gain\", \"genetic\", \"get\", \"go\", \"good\", \"government\", \"great\", \"health\", \"help\", \"high\", \"hope\", \"hospital\", \"hour\", \"infection\", \"ingenuity\", \"interest\", \"interview\", \"invest\", \"keep\", \"know\", \"large\", \"last\", \"late\", \"let\", \"likely\", \"line\", \"listen\", \"live\", \"long\", \"look\", \"low\", \"mad\", \"make\", \"man\", \"manufacture\", \"market\", \"mean\", \"medical\", \"meet\", \"meeting\", \"modern\", \"money\", \"month\", \"morning\", \"move\", \"need\", \"never\", \"new\", \"news\", \"next\", \"note\", \"old\", \"open\", \"own\", \"pandemic\", \"part\", \"participant\", \"patient\", \"people\", \"percent\", \"person\", \"phase\", \"plan\", \"point\", \"possible\", \"potential\", \"prepare\", \"prevent\", \"price\", \"probably\", \"product\", \"promise\", \"prove\", \"public\", \"purchase\", \"put\", \"question\", \"race\", \"rate\", \"read\", \"ready\", \"real\", \"really\", \"reason\", \"receive\", \"record\", \"regulatory\", \"release\", \"report\", \"request\", \"require\", \"research\", \"result\", \"review\", \"right\", \"say\", \"school\", \"scientist\", \"second\", \"seek\", \"seem\", \"sell\", \"send\", \"set\", \"severe\", \"share\", \"shot\", \"show\", \"side\", \"side_effect\", \"soon\", \"sound\", \"speed\", \"start\", \"state\", \"step\", \"stock\", \"storage\", \"study\", \"submit\", \"success\", \"super\", \"sure\", \"take\", \"team\", \"tell\", \"temperature\", \"term\", \"test\", \"testing\", \"thank\", \"thing\", \"think\", \"time\", \"today\", \"top\", \"treatment\", \"trial\", \"true\", \"trump\", \"trust\", \"try\", \"update\", \"vacci\", \"vaccinate\", \"virus\", \"volunteer\", \"wait\", \"watch\", \"week\", \"well\", \"work\", \"year\", \"yet\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [6, 8, 3, 1, 7, 5, 4, 2]};\n\nfunction LDAvis_load_lib(url, callback){\n  var s = document.createElement('script');\n  s.src = url;\n  s.async = true;\n  s.onreadystatechange = s.onload = callback;\n  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n  document.getElementsByTagName(\"head\")[0].appendChild(s);\n}\n\nif(typeof(LDAvis) !== \"undefined\"){\n   // already loaded: just create the visualization\n   !function(LDAvis){\n       new LDAvis(\"#\" + \"ldavis_el279561125332771369678741069\", ldavis_el279561125332771369678741069_data);\n   }(LDAvis);\n}else if(typeof define === \"function\" && define.amd){\n   // require.js is available: use it to load d3/LDAvis\n   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n   require([\"d3\"], function(d3){\n      window.d3 = d3;\n      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n        new LDAvis(\"#\" + \"ldavis_el279561125332771369678741069\", ldavis_el279561125332771369678741069_data);\n      });\n    });\n}else{\n    // require.js not available: dynamically load d3 & LDAvis\n    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n                 new LDAvis(\"#\" + \"ldavis_el279561125332771369678741069\", ldavis_el279561125332771369678741069_data);\n            })\n         });\n}\n</script>"
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "# Visualize the topics-keywords\n",
    "\n",
    "# DONT RUN \n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_lemmas, corpus_lemmas, id2word_lemmas)\n",
    "vis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE MALLET FOR TOPIC MODELLING\n",
    "##### Finding best mallet lda model ##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mallet LDA model\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "# Previous models are created with a gensim inbuilt LDA algorithm. \n",
    "# However, the version of the LDA algorithm from Mallet often gives higher quality topics.\n",
    "# We can implement mallet in gensim though a gensim wrapper \n",
    "# First, download zipfile, unzip it and provide the path to mallet in the unzipped directory to gensim.models.wrappers.LdaMallet. \n",
    "# Download File: http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "#Users/Astridll/Desktop/Kandidat CogSci/1_Semester/Natural Language Processing/NLP_exam_code/mallet-2.0.8/bin/mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best mallet lda model \n",
    "# returns c_v cohenerence metrics for different number of topics \n",
    "def compute_coherence_values(dictionary, corpus, iterations_no, texts, limit, start=2, step=3, ): # optimize_interval_no,\n",
    "\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "\n",
    "    mallet_path = './mallet-2.0.8/bin/mallet'\n",
    "\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus_stemmed, num_topics=10, id2word=id2word_stemmed, optimize_interval=20, iterations=1000)\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=tweets_stemmed, dictionary=id2word_stemmed, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=dictionary, iterations=iterations_no)# optimize_interval=optimize_interval_no\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETURN TO DO THIS - FIRST TRYING TO RUN SINGLE MALLET MODEL BELOW TO SEE HOW LONG IT TAKES TO RUN \n",
    "# alpha and update interval parameter made performance worse!\n",
    "\n",
    "mallet_path = './mallet-2.0.8/bin/mallet'\n",
    "\n",
    "# use funtion from above: \n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word_lemmas, corpus=corpus_lemmas,iterations_no=1000, texts=tweets_lemmas, start=2, limit=26, step=2) # alpha=0.001 removed\n",
    "\n",
    "#model_list, coherence_values = compute_coherence_values(dictionary=id2word_tokens, corpus=corpus_tokens,optimize_interval_no=20, iterations_no=1000, texts=tweets_tokens, start=2, limit=18, step=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"278.703393pt\" version=\"1.1\" viewBox=\"0 0 419.064185 278.703393\" width=\"419.064185pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-01-02T20:24:37.349925</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 278.703393 \nL 419.064185 278.703393 \nL 419.064185 0 \nL -0 0 \nz\n\" style=\"fill:#ffffff;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 69.272109 226.452065 \nL 404.072109 226.452065 \nL 404.072109 9.012065 \nL 69.272109 9.012065 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p08a3598b0c)\" d=\"M 125.994423 226.452065 \nL 125.994423 9.012065 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_1\">\n      <!-- 5 -->\n      <g style=\"fill:#262626;\" transform=\"translate(121.40665 247.762455)scale(0.165 -0.165)\">\n       <defs>\n        <path d=\"M 4.15625 18.75 \nL 13.375 19.53125 \nQ 14.40625 12.796875 18.140625 9.390625 \nQ 21.875 6 27.15625 6 \nQ 33.5 6 37.890625 10.78125 \nQ 42.28125 15.578125 42.28125 23.484375 \nQ 42.28125 31 38.0625 35.34375 \nQ 33.84375 39.703125 27 39.703125 \nQ 22.75 39.703125 19.328125 37.765625 \nQ 15.921875 35.84375 13.96875 32.765625 \nL 5.71875 33.84375 \nL 12.640625 70.609375 \nL 48.25 70.609375 \nL 48.25 62.203125 \nL 19.671875 62.203125 \nL 15.828125 42.96875 \nQ 22.265625 47.46875 29.34375 47.46875 \nQ 38.71875 47.46875 45.15625 40.96875 \nQ 51.609375 34.46875 51.609375 24.265625 \nQ 51.609375 14.546875 45.953125 7.46875 \nQ 39.0625 -1.21875 27.15625 -1.21875 \nQ 17.390625 -1.21875 11.203125 4.25 \nQ 5.03125 9.71875 4.15625 18.75 \nz\n\" id=\"ArialMT-53\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <path clip-path=\"url(#p08a3598b0c)\" d=\"M 195.167977 226.452065 \nL 195.167977 9.012065 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_2\">\n      <!-- 10 -->\n      <g style=\"fill:#262626;\" transform=\"translate(185.99243 247.762455)scale(0.165 -0.165)\">\n       <defs>\n        <path d=\"M 37.25 0 \nL 28.46875 0 \nL 28.46875 56 \nQ 25.296875 52.984375 20.140625 49.953125 \nQ 14.984375 46.921875 10.890625 45.40625 \nL 10.890625 53.90625 \nQ 18.265625 57.375 23.78125 62.296875 \nQ 29.296875 67.234375 31.59375 71.875 \nL 37.25 71.875 \nz\n\" id=\"ArialMT-49\"/>\n        <path d=\"M 4.15625 35.296875 \nQ 4.15625 48 6.765625 55.734375 \nQ 9.375 63.484375 14.515625 67.671875 \nQ 19.671875 71.875 27.484375 71.875 \nQ 33.25 71.875 37.59375 69.546875 \nQ 41.9375 67.234375 44.765625 62.859375 \nQ 47.609375 58.5 49.21875 52.21875 \nQ 50.828125 45.953125 50.828125 35.296875 \nQ 50.828125 22.703125 48.234375 14.96875 \nQ 45.65625 7.234375 40.5 3 \nQ 35.359375 -1.21875 27.484375 -1.21875 \nQ 17.140625 -1.21875 11.234375 6.203125 \nQ 4.15625 15.140625 4.15625 35.296875 \nz\nM 13.1875 35.296875 \nQ 13.1875 17.671875 17.3125 11.828125 \nQ 21.4375 6 27.484375 6 \nQ 33.546875 6 37.671875 11.859375 \nQ 41.796875 17.71875 41.796875 35.296875 \nQ 41.796875 52.984375 37.671875 58.78125 \nQ 33.546875 64.59375 27.390625 64.59375 \nQ 21.34375 64.59375 17.71875 59.46875 \nQ 13.1875 52.9375 13.1875 35.296875 \nz\n\" id=\"ArialMT-48\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-49\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p08a3598b0c)\" d=\"M 264.341531 226.452065 \nL 264.341531 9.012065 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_3\">\n      <!-- 15 -->\n      <g style=\"fill:#262626;\" transform=\"translate(255.165984 247.762455)scale(0.165 -0.165)\">\n       <use xlink:href=\"#ArialMT-49\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <path clip-path=\"url(#p08a3598b0c)\" d=\"M 333.515085 226.452065 \nL 333.515085 9.012065 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_4\">\n      <!-- 20 -->\n      <g style=\"fill:#262626;\" transform=\"translate(324.339538 247.762455)scale(0.165 -0.165)\">\n       <defs>\n        <path d=\"M 50.34375 8.453125 \nL 50.34375 0 \nL 3.03125 0 \nQ 2.9375 3.171875 4.046875 6.109375 \nQ 5.859375 10.9375 9.828125 15.625 \nQ 13.8125 20.3125 21.34375 26.46875 \nQ 33.015625 36.03125 37.109375 41.625 \nQ 41.21875 47.21875 41.21875 52.203125 \nQ 41.21875 57.421875 37.46875 61 \nQ 33.734375 64.59375 27.734375 64.59375 \nQ 21.390625 64.59375 17.578125 60.78125 \nQ 13.765625 56.984375 13.71875 50.25 \nL 4.6875 51.171875 \nQ 5.609375 61.28125 11.65625 66.578125 \nQ 17.71875 71.875 27.9375 71.875 \nQ 38.234375 71.875 44.234375 66.15625 \nQ 50.25 60.453125 50.25 52 \nQ 50.25 47.703125 48.484375 43.546875 \nQ 46.734375 39.40625 42.65625 34.8125 \nQ 38.578125 30.21875 29.109375 22.21875 \nQ 21.1875 15.578125 18.9375 13.203125 \nQ 16.703125 10.84375 15.234375 8.453125 \nz\n\" id=\"ArialMT-50\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-50\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p08a3598b0c)\" d=\"M 402.688638 226.452065 \nL 402.688638 9.012065 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_5\">\n      <!-- 25 -->\n      <g style=\"fill:#262626;\" transform=\"translate(393.513091 247.762455)scale(0.165 -0.165)\">\n       <use xlink:href=\"#ArialMT-50\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- Num Topics -->\n     <g style=\"fill:#262626;\" transform=\"translate(189.822891 267.925893)scale(0.18 -0.18)\">\n      <defs>\n       <path d=\"M 7.625 0 \nL 7.625 71.578125 \nL 17.328125 71.578125 \nL 54.9375 15.375 \nL 54.9375 71.578125 \nL 64.015625 71.578125 \nL 64.015625 0 \nL 54.296875 0 \nL 16.703125 56.25 \nL 16.703125 0 \nz\n\" id=\"ArialMT-78\"/>\n       <path d=\"M 40.578125 0 \nL 40.578125 7.625 \nQ 34.515625 -1.171875 24.125 -1.171875 \nQ 19.53125 -1.171875 15.546875 0.578125 \nQ 11.578125 2.34375 9.640625 5 \nQ 7.71875 7.671875 6.9375 11.53125 \nQ 6.390625 14.109375 6.390625 19.734375 \nL 6.390625 51.859375 \nL 15.1875 51.859375 \nL 15.1875 23.09375 \nQ 15.1875 16.21875 15.71875 13.8125 \nQ 16.546875 10.359375 19.234375 8.375 \nQ 21.921875 6.390625 25.875 6.390625 \nQ 29.828125 6.390625 33.296875 8.421875 \nQ 36.765625 10.453125 38.203125 13.9375 \nQ 39.65625 17.4375 39.65625 24.078125 \nL 39.65625 51.859375 \nL 48.4375 51.859375 \nL 48.4375 0 \nz\n\" id=\"ArialMT-117\"/>\n       <path d=\"M 6.59375 0 \nL 6.59375 51.859375 \nL 14.453125 51.859375 \nL 14.453125 44.578125 \nQ 16.890625 48.390625 20.9375 50.703125 \nQ 25 53.03125 30.171875 53.03125 \nQ 35.9375 53.03125 39.625 50.640625 \nQ 43.3125 48.25 44.828125 43.953125 \nQ 50.984375 53.03125 60.84375 53.03125 \nQ 68.5625 53.03125 72.703125 48.75 \nQ 76.859375 44.484375 76.859375 35.59375 \nL 76.859375 0 \nL 68.109375 0 \nL 68.109375 32.671875 \nQ 68.109375 37.9375 67.25 40.25 \nQ 66.40625 42.578125 64.15625 43.984375 \nQ 61.921875 45.40625 58.890625 45.40625 \nQ 53.421875 45.40625 49.796875 41.765625 \nQ 46.1875 38.140625 46.1875 30.125 \nL 46.1875 0 \nL 37.40625 0 \nL 37.40625 33.6875 \nQ 37.40625 39.546875 35.25 42.46875 \nQ 33.109375 45.40625 28.21875 45.40625 \nQ 24.515625 45.40625 21.359375 43.453125 \nQ 18.21875 41.5 16.796875 37.734375 \nQ 15.375 33.984375 15.375 26.90625 \nL 15.375 0 \nz\n\" id=\"ArialMT-109\"/>\n       <path id=\"ArialMT-32\"/>\n       <path d=\"M 25.921875 0 \nL 25.921875 63.140625 \nL 2.34375 63.140625 \nL 2.34375 71.578125 \nL 59.078125 71.578125 \nL 59.078125 63.140625 \nL 35.40625 63.140625 \nL 35.40625 0 \nz\n\" id=\"ArialMT-84\"/>\n       <path d=\"M 3.328125 25.921875 \nQ 3.328125 40.328125 11.328125 47.265625 \nQ 18.015625 53.03125 27.640625 53.03125 \nQ 38.328125 53.03125 45.109375 46.015625 \nQ 51.90625 39.015625 51.90625 26.65625 \nQ 51.90625 16.65625 48.90625 10.90625 \nQ 45.90625 5.171875 40.15625 2 \nQ 34.421875 -1.171875 27.640625 -1.171875 \nQ 16.75 -1.171875 10.03125 5.8125 \nQ 3.328125 12.796875 3.328125 25.921875 \nz\nM 12.359375 25.921875 \nQ 12.359375 15.96875 16.703125 11.015625 \nQ 21.046875 6.0625 27.640625 6.0625 \nQ 34.1875 6.0625 38.53125 11.03125 \nQ 42.875 16.015625 42.875 26.21875 \nQ 42.875 35.84375 38.5 40.796875 \nQ 34.125 45.75 27.640625 45.75 \nQ 21.046875 45.75 16.703125 40.8125 \nQ 12.359375 35.890625 12.359375 25.921875 \nz\n\" id=\"ArialMT-111\"/>\n       <path d=\"M 6.59375 -19.875 \nL 6.59375 51.859375 \nL 14.59375 51.859375 \nL 14.59375 45.125 \nQ 17.4375 49.078125 21 51.046875 \nQ 24.5625 53.03125 29.640625 53.03125 \nQ 36.28125 53.03125 41.359375 49.609375 \nQ 46.4375 46.1875 49.015625 39.953125 \nQ 51.609375 33.734375 51.609375 26.3125 \nQ 51.609375 18.359375 48.75 11.984375 \nQ 45.90625 5.609375 40.453125 2.21875 \nQ 35.015625 -1.171875 29 -1.171875 \nQ 24.609375 -1.171875 21.109375 0.6875 \nQ 17.625 2.546875 15.375 5.375 \nL 15.375 -19.875 \nz\nM 14.546875 25.640625 \nQ 14.546875 15.625 18.59375 10.84375 \nQ 22.65625 6.0625 28.421875 6.0625 \nQ 34.28125 6.0625 38.453125 11.015625 \nQ 42.625 15.96875 42.625 26.375 \nQ 42.625 36.28125 38.546875 41.203125 \nQ 34.46875 46.140625 28.8125 46.140625 \nQ 23.1875 46.140625 18.859375 40.890625 \nQ 14.546875 35.640625 14.546875 25.640625 \nz\n\" id=\"ArialMT-112\"/>\n       <path d=\"M 6.640625 61.46875 \nL 6.640625 71.578125 \nL 15.4375 71.578125 \nL 15.4375 61.46875 \nz\nM 6.640625 0 \nL 6.640625 51.859375 \nL 15.4375 51.859375 \nL 15.4375 0 \nz\n\" id=\"ArialMT-105\"/>\n       <path d=\"M 40.4375 19 \nL 49.078125 17.875 \nQ 47.65625 8.9375 41.8125 3.875 \nQ 35.984375 -1.171875 27.484375 -1.171875 \nQ 16.84375 -1.171875 10.375 5.78125 \nQ 3.90625 12.75 3.90625 25.734375 \nQ 3.90625 34.125 6.6875 40.421875 \nQ 9.46875 46.734375 15.15625 49.875 \nQ 20.84375 53.03125 27.546875 53.03125 \nQ 35.984375 53.03125 41.359375 48.75 \nQ 46.734375 44.484375 48.25 36.625 \nL 39.703125 35.296875 \nQ 38.484375 40.53125 35.375 43.15625 \nQ 32.28125 45.796875 27.875 45.796875 \nQ 21.234375 45.796875 17.078125 41.03125 \nQ 12.9375 36.28125 12.9375 25.984375 \nQ 12.9375 15.53125 16.9375 10.796875 \nQ 20.953125 6.0625 27.390625 6.0625 \nQ 32.5625 6.0625 36.03125 9.234375 \nQ 39.5 12.40625 40.4375 19 \nz\n\" id=\"ArialMT-99\"/>\n       <path d=\"M 3.078125 15.484375 \nL 11.765625 16.84375 \nQ 12.5 11.625 15.84375 8.84375 \nQ 19.1875 6.0625 25.203125 6.0625 \nQ 31.25 6.0625 34.171875 8.515625 \nQ 37.109375 10.984375 37.109375 14.3125 \nQ 37.109375 17.28125 34.515625 19 \nQ 32.71875 20.171875 25.53125 21.96875 \nQ 15.875 24.421875 12.140625 26.203125 \nQ 8.40625 27.984375 6.46875 31.125 \nQ 4.546875 34.28125 4.546875 38.09375 \nQ 4.546875 41.546875 6.125 44.5 \nQ 7.71875 47.46875 10.453125 49.421875 \nQ 12.5 50.921875 16.03125 51.96875 \nQ 19.578125 53.03125 23.640625 53.03125 \nQ 29.734375 53.03125 34.34375 51.265625 \nQ 38.96875 49.515625 41.15625 46.5 \nQ 43.359375 43.5 44.1875 38.484375 \nL 35.59375 37.3125 \nQ 35.015625 41.3125 32.203125 43.546875 \nQ 29.390625 45.796875 24.265625 45.796875 \nQ 18.21875 45.796875 15.625 43.796875 \nQ 13.03125 41.796875 13.03125 39.109375 \nQ 13.03125 37.40625 14.109375 36.03125 \nQ 15.1875 34.625 17.484375 33.6875 \nQ 18.796875 33.203125 25.25 31.453125 \nQ 34.578125 28.953125 38.25 27.359375 \nQ 41.9375 25.78125 44.03125 22.75 \nQ 46.140625 19.734375 46.140625 15.234375 \nQ 46.140625 10.84375 43.578125 6.953125 \nQ 41.015625 3.078125 36.171875 0.953125 \nQ 31.34375 -1.171875 25.25 -1.171875 \nQ 15.140625 -1.171875 9.84375 3.03125 \nQ 4.546875 7.234375 3.078125 15.484375 \nz\n\" id=\"ArialMT-115\"/>\n      </defs>\n      <use xlink:href=\"#ArialMT-78\"/>\n      <use x=\"72.216797\" xlink:href=\"#ArialMT-117\"/>\n      <use x=\"127.832031\" xlink:href=\"#ArialMT-109\"/>\n      <use x=\"211.132812\" xlink:href=\"#ArialMT-32\"/>\n      <use x=\"237.166016\" xlink:href=\"#ArialMT-84\"/>\n      <use x=\"287.125\" xlink:href=\"#ArialMT-111\"/>\n      <use x=\"342.740234\" xlink:href=\"#ArialMT-112\"/>\n      <use x=\"398.355469\" xlink:href=\"#ArialMT-105\"/>\n      <use x=\"420.572266\" xlink:href=\"#ArialMT-99\"/>\n      <use x=\"470.572266\" xlink:href=\"#ArialMT-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <path clip-path=\"url(#p08a3598b0c)\" d=\"M 69.272109 212.318202 \nL 404.072109 212.318202 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.40 -->\n      <g style=\"fill:#262626;\" transform=\"translate(27.661563 218.223397)scale(0.165 -0.165)\">\n       <defs>\n        <path d=\"M 9.078125 0 \nL 9.078125 10.015625 \nL 19.09375 10.015625 \nL 19.09375 0 \nz\n\" id=\"ArialMT-46\"/>\n        <path d=\"M 32.328125 0 \nL 32.328125 17.140625 \nL 1.265625 17.140625 \nL 1.265625 25.203125 \nL 33.9375 71.578125 \nL 41.109375 71.578125 \nL 41.109375 25.203125 \nL 50.78125 25.203125 \nL 50.78125 17.140625 \nL 41.109375 17.140625 \nL 41.109375 0 \nz\nM 32.328125 25.203125 \nL 32.328125 57.46875 \nL 9.90625 25.203125 \nz\n\" id=\"ArialMT-52\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-48\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-46\"/>\n       <use x=\"83.398438\" xlink:href=\"#ArialMT-52\"/>\n       <use x=\"139.013672\" xlink:href=\"#ArialMT-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p08a3598b0c)\" d=\"M 69.272109 145.913866 \nL 404.072109 145.913866 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.45 -->\n      <g style=\"fill:#262626;\" transform=\"translate(27.661563 151.819062)scale(0.165 -0.165)\">\n       <use xlink:href=\"#ArialMT-48\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-46\"/>\n       <use x=\"83.398438\" xlink:href=\"#ArialMT-52\"/>\n       <use x=\"139.013672\" xlink:href=\"#ArialMT-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <path clip-path=\"url(#p08a3598b0c)\" d=\"M 69.272109 79.509531 \nL 404.072109 79.509531 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.50 -->\n      <g style=\"fill:#262626;\" transform=\"translate(27.661563 85.414726)scale(0.165 -0.165)\">\n       <use xlink:href=\"#ArialMT-48\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-46\"/>\n       <use x=\"83.398438\" xlink:href=\"#ArialMT-53\"/>\n       <use x=\"139.013672\" xlink:href=\"#ArialMT-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p08a3598b0c)\" d=\"M 69.272109 13.105195 \nL 404.072109 13.105195 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.55 -->\n      <g style=\"fill:#262626;\" transform=\"translate(27.661563 19.010391)scale(0.165 -0.165)\">\n       <use xlink:href=\"#ArialMT-48\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-46\"/>\n       <use x=\"83.398438\" xlink:href=\"#ArialMT-53\"/>\n       <use x=\"139.013672\" xlink:href=\"#ArialMT-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_11\">\n     <!-- Coherence score -->\n     <g style=\"fill:#262626;\" transform=\"translate(20.084063 186.264252)rotate(-90)scale(0.18 -0.18)\">\n      <defs>\n       <path d=\"M 58.796875 25.09375 \nL 68.265625 22.703125 \nQ 65.28125 11.03125 57.546875 4.90625 \nQ 49.8125 -1.21875 38.625 -1.21875 \nQ 27.046875 -1.21875 19.796875 3.484375 \nQ 12.546875 8.203125 8.765625 17.140625 \nQ 4.984375 26.078125 4.984375 36.328125 \nQ 4.984375 47.515625 9.25 55.828125 \nQ 13.53125 64.15625 21.40625 68.46875 \nQ 29.296875 72.796875 38.765625 72.796875 \nQ 49.515625 72.796875 56.828125 67.328125 \nQ 64.15625 61.859375 67.046875 51.953125 \nL 57.71875 49.75 \nQ 55.21875 57.5625 50.484375 61.125 \nQ 45.75 64.703125 38.578125 64.703125 \nQ 30.328125 64.703125 24.78125 60.734375 \nQ 19.234375 56.78125 16.984375 50.109375 \nQ 14.75 43.453125 14.75 36.375 \nQ 14.75 27.25 17.40625 20.4375 \nQ 20.0625 13.625 25.671875 10.25 \nQ 31.296875 6.890625 37.84375 6.890625 \nQ 45.796875 6.890625 51.3125 11.46875 \nQ 56.84375 16.0625 58.796875 25.09375 \nz\n\" id=\"ArialMT-67\"/>\n       <path d=\"M 6.59375 0 \nL 6.59375 71.578125 \nL 15.375 71.578125 \nL 15.375 45.90625 \nQ 21.53125 53.03125 30.90625 53.03125 \nQ 36.671875 53.03125 40.921875 50.75 \nQ 45.171875 48.484375 47 44.484375 \nQ 48.828125 40.484375 48.828125 32.859375 \nL 48.828125 0 \nL 40.046875 0 \nL 40.046875 32.859375 \nQ 40.046875 39.453125 37.1875 42.453125 \nQ 34.328125 45.453125 29.109375 45.453125 \nQ 25.203125 45.453125 21.75 43.421875 \nQ 18.3125 41.40625 16.84375 37.9375 \nQ 15.375 34.46875 15.375 28.375 \nL 15.375 0 \nz\n\" id=\"ArialMT-104\"/>\n       <path d=\"M 42.09375 16.703125 \nL 51.171875 15.578125 \nQ 49.03125 7.625 43.21875 3.21875 \nQ 37.40625 -1.171875 28.375 -1.171875 \nQ 17 -1.171875 10.328125 5.828125 \nQ 3.65625 12.84375 3.65625 25.484375 \nQ 3.65625 38.578125 10.390625 45.796875 \nQ 17.140625 53.03125 27.875 53.03125 \nQ 38.28125 53.03125 44.875 45.953125 \nQ 51.46875 38.875 51.46875 26.03125 \nQ 51.46875 25.25 51.421875 23.6875 \nL 12.75 23.6875 \nQ 13.234375 15.140625 17.578125 10.59375 \nQ 21.921875 6.0625 28.421875 6.0625 \nQ 33.25 6.0625 36.671875 8.59375 \nQ 40.09375 11.140625 42.09375 16.703125 \nz\nM 13.234375 30.90625 \nL 42.1875 30.90625 \nQ 41.609375 37.453125 38.875 40.71875 \nQ 34.671875 45.796875 27.984375 45.796875 \nQ 21.921875 45.796875 17.796875 41.75 \nQ 13.671875 37.703125 13.234375 30.90625 \nz\n\" id=\"ArialMT-101\"/>\n       <path d=\"M 6.5 0 \nL 6.5 51.859375 \nL 14.40625 51.859375 \nL 14.40625 44 \nQ 17.4375 49.515625 20 51.265625 \nQ 22.5625 53.03125 25.640625 53.03125 \nQ 30.078125 53.03125 34.671875 50.203125 \nL 31.640625 42.046875 \nQ 28.421875 43.953125 25.203125 43.953125 \nQ 22.3125 43.953125 20.015625 42.21875 \nQ 17.71875 40.484375 16.75 37.40625 \nQ 15.28125 32.71875 15.28125 27.15625 \nL 15.28125 0 \nz\n\" id=\"ArialMT-114\"/>\n       <path d=\"M 6.59375 0 \nL 6.59375 51.859375 \nL 14.5 51.859375 \nL 14.5 44.484375 \nQ 20.21875 53.03125 31 53.03125 \nQ 35.6875 53.03125 39.625 51.34375 \nQ 43.5625 49.65625 45.515625 46.921875 \nQ 47.46875 44.1875 48.25 40.4375 \nQ 48.734375 37.984375 48.734375 31.890625 \nL 48.734375 0 \nL 39.9375 0 \nL 39.9375 31.546875 \nQ 39.9375 36.921875 38.90625 39.578125 \nQ 37.890625 42.234375 35.28125 43.8125 \nQ 32.671875 45.40625 29.15625 45.40625 \nQ 23.53125 45.40625 19.453125 41.84375 \nQ 15.375 38.28125 15.375 28.328125 \nL 15.375 0 \nz\n\" id=\"ArialMT-110\"/>\n      </defs>\n      <use xlink:href=\"#ArialMT-67\"/>\n      <use x=\"72.216797\" xlink:href=\"#ArialMT-111\"/>\n      <use x=\"127.832031\" xlink:href=\"#ArialMT-104\"/>\n      <use x=\"183.447266\" xlink:href=\"#ArialMT-101\"/>\n      <use x=\"239.0625\" xlink:href=\"#ArialMT-114\"/>\n      <use x=\"272.363281\" xlink:href=\"#ArialMT-101\"/>\n      <use x=\"327.978516\" xlink:href=\"#ArialMT-110\"/>\n      <use x=\"383.59375\" xlink:href=\"#ArialMT-99\"/>\n      <use x=\"433.59375\" xlink:href=\"#ArialMT-101\"/>\n      <use x=\"489.208984\" xlink:href=\"#ArialMT-32\"/>\n      <use x=\"516.992188\" xlink:href=\"#ArialMT-115\"/>\n      <use x=\"566.992188\" xlink:href=\"#ArialMT-99\"/>\n      <use x=\"616.992188\" xlink:href=\"#ArialMT-111\"/>\n      <use x=\"672.607422\" xlink:href=\"#ArialMT-114\"/>\n      <use x=\"705.908203\" xlink:href=\"#ArialMT-101\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_10\">\n    <path clip-path=\"url(#p08a3598b0c)\" d=\"M 84.490291 190.437579 \nL 112.159713 216.568428 \nL 139.829134 133.573219 \nL 167.498556 120.452704 \nL 195.167977 89.222491 \nL 222.837399 93.174451 \nL 250.50682 84.997734 \nL 278.176242 50.243209 \nL 305.845663 41.182712 \nL 333.515085 26.680161 \nL 361.184506 23.771786 \nL 388.853928 18.895701 \n\" style=\"fill:none;stroke:#4c72b0;stroke-linecap:round;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 69.272109 226.452065 \nL 69.272109 9.012065 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 404.072109 226.452065 \nL 404.072109 9.012065 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 69.272109 226.452065 \nL 404.072109 226.452065 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 69.272109 9.012065 \nL 404.072109 9.012065 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 80.822109 45.55183 \nL 141.872109 45.55183 \nQ 145.172109 45.55183 145.172109 42.25183 \nL 145.172109 20.562065 \nQ 145.172109 17.262065 141.872109 17.262065 \nL 80.822109 17.262065 \nQ 77.522109 17.262065 77.522109 20.562065 \nL 77.522109 42.25183 \nQ 77.522109 45.55183 80.822109 45.55183 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_11\">\n     <path d=\"M 84.122109 29.897455 \nL 117.122109 29.897455 \n\" style=\"fill:none;stroke:#4c72b0;stroke-linecap:round;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_12\"/>\n    <g id=\"text_12\">\n     <!-- c -->\n     <g style=\"fill:#262626;\" transform=\"translate(130.322109 35.672455)scale(0.165 -0.165)\">\n      <use xlink:href=\"#ArialMT-99\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p08a3598b0c\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"69.272109\" y=\"9.012065\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Show graph of models with different numbers of topics vs coherence scores \n",
    "limit=26; start=2; step=2; \n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num Topics = 2  has Coherence Value of 0.4165\nNum Topics = 4  has Coherence Value of 0.3968\nNum Topics = 6  has Coherence Value of 0.4593\nNum Topics = 8  has Coherence Value of 0.4692\nNum Topics = 10  has Coherence Value of 0.4927\nNum Topics = 12  has Coherence Value of 0.4897\nNum Topics = 14  has Coherence Value of 0.4959\nNum Topics = 16  has Coherence Value of 0.522\nNum Topics = 18  has Coherence Value of 0.5289\nNum Topics = 20  has Coherence Value of 0.5398\nNum Topics = 22  has Coherence Value of 0.542\nNum Topics = 24  has Coherence Value of 0.5456\n"
     ]
    }
   ],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\n",
    "# LEMMAS: \n",
    "#Num Topics = 2  has Coherence Value of 0.6452\n",
    "#Num Topics = 5  has Coherence Value of 0.7227\n",
    "#Num Topics = 8  has Coherence Value of 0.752\n",
    "#Num Topics = 11  has Coherence Value of 0.7548\n",
    "#Num Topics = 14  has Coherence Value of 0.7626\n",
    "#Num Topics = 17  has Coherence Value of 0.7406\n",
    "#Num Topics = 20  has Coherence Value of 0.7652\n",
    "#Num Topics = 23  has Coherence Value of 0.7727\n",
    "#Num Topics = 26  has Coherence Value of 0.7688\n",
    "#Num Topics = 29  has Coherence Value of 0.7666\n",
    "\n",
    "#Num Topics = 2  has Coherence Value of 0.6421\n",
    "#Num Topics = 4  has Coherence Value of 0.7284\n",
    "#Num Topics = 6  has Coherence Value of 0.745\n",
    "#Num Topics = 8  has Coherence Value of 0.7591\n",
    "#Num Topics = 10  has Coherence Value of 0.7633\n",
    "#Num Topics = 12  has Coherence Value of 0.7355\n",
    "#Num Topics = 14  has Coherence Value of 0.7661\n",
    "# AND SO I LAND ON 7 TOPICS, AS IT CAN BE SEEN THAT THERE IS NO BIG ADDED VALUE OF ADDING MORE\n",
    "\n",
    "# LEMMAS - BUT WITH LOW ALPHA\n",
    "#Num Topics = 2  has Coherence Value of 0.4642\n",
    "#Num Topics = 5  has Coherence Value of 0.5013\n",
    "#Num Topics = 8  has Coherence Value of 0.5075\n",
    "#Num Topics = 11  has Coherence Value of 0.4931\n",
    "#Num Topics = 14  has Coherence Value of 0.5098\n",
    "\n",
    "# ADDED DATA 15000 - LEMMAS, NO ALPHA SPECIFIED \n",
    "#Num Topics = 2  has Coherence Value of 0.3852\n",
    "#Num Topics = 5  has Coherence Value of 0.4027\n",
    "#Num Topics = 8  has Coherence Value of 0.434\n",
    "#Num Topics = 11  has Coherence Value of 0.4728\n",
    "#Num Topics = 14  has Coherence Value of 0.4871\n",
    "#Num Topics = 17  has Coherence Value of 0.5101\n",
    "\n",
    "# extra data is CRAP! (errors?) \n",
    "\n",
    "# ADDED DATA 15000 - LEMMAS, LOW ALPHA - so bad\n",
    "#Num Topics = 2  has Coherence Value of 0.3936\n",
    "#Num Topics = 5  has Coherence Value of 0.3908\n",
    "#Num Topics = 8  has Coherence Value of 0.3647\n",
    "#Num Topics = 11  has Coherence Value of 0.3691\n",
    "#Num Topics = 14  has Coherence Value of 0.3515\n",
    "#Num Topics = 17  has Coherence Value of 0.3552\n",
    "\n",
    "# try with no alpha / default and more range\n",
    "#Num Topics = 2  has Coherence Value of 0.3157\n",
    "#Num Topics = 4  has Coherence Value of 0.3429\n",
    "#Num Topics = 6  has Coherence Value of 0.4025\n",
    "#Num Topics = 8  has Coherence Value of 0.4168\n",
    "#Num Topics = 10  has Coherence Value of 0.4872\n",
    "#Num Topics = 12  has Coherence Value of 0.4734\n",
    "#Num Topics = 14  has Coherence Value of 0.4859\n",
    "#Num Topics = 16  has Coherence Value of 0.501\n",
    "#Num Topics = 18  has Coherence Value of 0.4993\n",
    "#Num Topics = 20  has Coherence Value of 0.5064\n",
    "#Num Topics = 22  has Coherence Value of 0.5225\n",
    "#Num Topics = 24  has Coherence Value of 0.5298\n",
    "\n",
    "# dropped lemmas below 3\n",
    "#Num Topics = 2  has Coherence Value of 0.3474\n",
    "#Num Topics = 4  has Coherence Value of 0.3823\n",
    "#Num Topics = 6  has Coherence Value of 0.4075\n",
    "#Num Topics = 8  has Coherence Value of 0.4397\n",
    "#Num Topics = 10  has Coherence Value of 0.4688\n",
    "#Num Topics = 12  has Coherence Value of 0.4777\n",
    "#Num Topics = 14  has Coherence Value of 0.477\n",
    "#Num Topics = 16  has Coherence Value of 0.4964\n",
    "#Num Topics = 18  has Coherence Value of 0.5009\n",
    "#Num Topics = 20  has Coherence Value of 0.5254\n",
    "#Num Topics = 22  has Coherence Value of 0.5172\n",
    "#Num Topics = 24  has Coherence Value of 0.5342\n",
    "\n",
    "# now also removed duplicate lemma rows \n",
    "# did not help\n",
    "\n",
    "# NOW WITH DATA MINUS 10 MOST FREQUENT WORDS/LEMMAS - FINAL DATA\n",
    "#Num Topics = 2  has Coherence Value of 0.4165\n",
    "#Num Topics = 4  has Coherence Value of 0.3968\n",
    "#Num Topics = 6  has Coherence Value of 0.4593\n",
    "#Num Topics = 8  has Coherence Value of 0.4692\n",
    "#Num Topics = 10  has Coherence Value of 0.4927 # MY CHOSEN MODEL FOR LDA\n",
    "#Num Topics = 12  has Coherence Value of 0.4897\n",
    "#Num Topics = 14  has Coherence Value of 0.4959\n",
    "#Num Topics = 16  has Coherence Value of 0.522\n",
    "#Num Topics = 18  has Coherence Value of 0.5289\n",
    "#Num Topics = 20  has Coherence Value of 0.5398\n",
    "#Num Topics = 22  has Coherence Value of 0.542\n",
    "#Num Topics = 24  has Coherence Value of 0.5456\n",
    "\n",
    "# GONNA GO WITH 10 TOPICS - NOT EXACTLY GREATEST COHERENCE VALUES BUT TOPICS LOOK GOOD AND YOU ALSO HAVE TO THINK ABOUT IT AS A TRADE OF AT SOME POINT. EVEN THOUGH 24+ TOPICS MIGHT GIVE A HIGHER COHERENCE SCORE IT IS NOT SURE I WOULD GAIN MORE INFORMATION OUT OF IT BC I WOULD NOT GET A FEELING OF TOPICS THERE WOULD BE TOO MANY TO COMPREHEND I THINK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "execution_count": 211
    }
   ],
   "source": [
    "type(model_topics)\n",
    "\n",
    "#[(0,\n",
    "#  [('health', 0.0759565962307253),\n",
    "#   ('worker', 0.049971444888635065),\n",
    "#   ('care', 0.0422615648201028),\n",
    "#   ('live', 0.041119360365505425),\n",
    "#   ('official', 0.034837235865219876),\n",
    "#   ('administer', 0.034551684751570534),\n",
    "#   ('watch', 0.030553969160479727),\n",
    "#   ('case', 0.03026841804683038),\n",
    "#   ('report', 0.02655625356938892),\n",
    "#   ('facility', 0.023129640205596803)]),\n",
    "# (1,\n",
    "#  [('work', 0.06459874786568014),\n",
    "#   ('virus', 0.03699487763232783),\n",
    "#   ('great', 0.026750142287990893),\n",
    "#   ('scientist', 0.024758110415480934),\n",
    "#   ('initial', 0.016789982925441094),\n",
    "#   ('long', 0.01593625498007968),\n",
    "#   ('leave', 0.014228799089356859),\n",
    "#   ('science', 0.013944223107569721),\n",
    "#   ('total', 0.013375071143995446),\n",
    "#   ('truck', 0.012805919180421172)]),\n",
    "# (2,\n",
    "#  [('hospital', 0.05507990867579909),\n",
    "#   ('wait', 0.0502283105022831),\n",
    "#   ('distribute', 0.030251141552511414),\n",
    "#   ('country', 0.028824200913242008),\n",
    "#   ('share', 0.02454337899543379),\n",
    "#   ('deliver', 0.021974885844748857),\n",
    "#   ('million', 0.021974885844748857),\n",
    "#   ('government', 0.01997716894977169),\n",
    "#   ('yesterday', 0.01769406392694064),\n",
    "#   ('break', 0.017408675799086757)]),\n",
    "# (3,\n",
    "#  [('stock', 0.11947397873531057),\n",
    "#   ('trump', 0.11219921656407386),\n",
    "#   ('company', 0.03329602686066033),\n",
    "#   ('order', 0.030498041410184668),\n",
    "#   ('hold', 0.027700055959709008),\n",
    "#   ('invest', 0.02294348069390039),\n",
    "#   ('buy', 0.02238388360380526),\n",
    "#   ('family', 0.01650811415780638),\n",
    "#   ('true', 0.01259093452714046),\n",
    "#   ('vaccin', 0.011751538891997761)]),\n",
    "# (4,\n",
    "#  [('ship', 0.05554005023723137),\n",
    "#   ('state', 0.054981858777560705),\n",
    "#   ('start', 0.05386547585821937),\n",
    "#   ('vaccination', 0.040189785096288025),\n",
    "#   ('news', 0.03349148758024002),\n",
    "#   ('include', 0.023444041306168017),\n",
    "#   ('plan', 0.022048562656991347),\n",
    "#   ('set', 0.021769466927156013),\n",
    "#   ('ready', 0.020094892548144012),\n",
    "#   ('shoot', 0.019536701088473347)]),\n",
    "# (5,\n",
    "#  [('expect', 0.07158899494665918),\n",
    "#   ('arrive', 0.06878158338012352),\n",
    "#   ('good', 0.04997192588433464),\n",
    "#   ('update', 0.04211117349803481),\n",
    "#   ('end', 0.028916339135317236),\n",
    "#   ('day', 0.025547445255474453),\n",
    "#   ('hear', 0.018809657495788883),\n",
    "#   ('pandemic', 0.017967434025828188),\n",
    "#   ('feel', 0.017967434025828188),\n",
    "#   ('morning', 0.01768669286917462)]),\n",
    "# (6,\n",
    "#  [('emergency', 0.052146355517142036),\n",
    "#   ('money', 0.04436761740132527),\n",
    "#   ('authorize', 0.03486027081532699),\n",
    "#   ('authorization', 0.03399596658023624),\n",
    "#   ('approval', 0.032843560933448576),\n",
    "#   ('month', 0.02881014116969173),\n",
    "#   ('follow', 0.021031403053874964),\n",
    "#   ('question', 0.019878997407087293),\n",
    "#   ('reason', 0.017862287525208874),\n",
    "#   ('administration', 0.016709881878421203)]),\n",
    "# (7,\n",
    "#  [('people', 0.10596221959858323),\n",
    "#  ('vaccinate', 0.04693034238488784),\n",
    "#   ('shot', 0.04043683589138135),\n",
    "#   ('read', 0.03571428571428571),\n",
    "#   ('time', 0.029811097992916175),\n",
    "#   ('develop', 0.028630460448642266),\n",
    "#   ('recommend', 0.02154663518299882),\n",
    "#   ('require', 0.01829988193624557),\n",
    "#   ('safe', 0.01770956316410862),\n",
    "#   ('group', 0.015938606847697757)]),\n",
    "# (8,\n",
    "#  [('trial', 0.050400916380297825),\n",
    "#   ('year', 0.0352233676975945),\n",
    "#   ('find', 0.02720504009163803),\n",
    "#   ('hope', 0.02663230240549828),\n",
    "#   ('test', 0.02577319587628866),\n",
    "#   ('phase', 0.020332187857961054),\n",
    "#   ('strain', 0.02004581901489118),\n",
    "#   ('effective', 0.01918671248568156),\n",
    "#   ('datum', 0.018327605956471937),\n",
    "#   ('coronavirus', 0.017468499427262314)]),\n",
    "# (9,\n",
    "#  [('distribution', 0.0667818077144502),\n",
    "#   ('roll', 0.0256188831318365),\n",
    "#   ('speed', 0.025331030512377662),\n",
    "#   ('part', 0.025043177892918825),\n",
    "#   ('continue', 0.018998272884283247),\n",
    "#   ('big', 0.018422567645365574),\n",
    "#   ('give', 0.018422567645365574),\n",
    "#   ('back', 0.017846862406447898),\n",
    "#   ('announce', 0.017846862406447898),\n",
    "#   ('happen', 0.017271157167530225)])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_of_top10w_for_topics_df = df.groupby('sentiment')['Dominant_Topic'].value_counts().unstack().fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0,\n  '0.076*\"health\" + 0.050*\"worker\" + 0.042*\"care\" + 0.041*\"live\" + '\n  '0.035*\"official\" + 0.035*\"administer\" + 0.031*\"watch\" + 0.030*\"case\" + '\n  '0.027*\"report\" + 0.023*\"facility\" + 0.021*\"line\" + 0.020*\"death\" + '\n  '0.017*\"public\" + 0.017*\"stop\" + 0.016*\"patient\" + 0.015*\"top\" + '\n  '0.013*\"local\" + 0.011*\"spread\" + 0.010*\"reach\" + 0.010*\"vac\"'),\n (1,\n  '0.065*\"work\" + 0.037*\"virus\" + 0.027*\"great\" + 0.025*\"scientist\" + '\n  '0.017*\"initial\" + 0.016*\"long\" + 0.014*\"leave\" + 0.014*\"science\" + '\n  '0.013*\"total\" + 0.013*\"truck\" + 0.012*\"produce\" + 0.012*\"lead\" + '\n  '0.011*\"real\" + 0.011*\"protect\" + 0.011*\"world\" + 0.011*\"explain\" + '\n  '0.010*\"turn\" + 0.010*\"clear\" + 0.009*\"fast\" + 0.009*\"company\"'),\n (2,\n  '0.055*\"hospital\" + 0.050*\"wait\" + 0.030*\"distribute\" + 0.029*\"country\" + '\n  '0.025*\"share\" + 0.022*\"million\" + 0.022*\"deliver\" + 0.020*\"government\" + '\n  '0.018*\"yesterday\" + 0.017*\"break\" + 0.015*\"issue\" + 0.014*\"small\" + '\n  '0.014*\"send\" + 0.012*\"medical\" + 0.012*\"deal\" + 0.012*\"friend\" + '\n  '0.012*\"county\" + 0.011*\"trust\" + 0.010*\"shipping\" + 0.010*\"chief\"'),\n (3,\n  '0.119*\"stock\" + 0.112*\"trump\" + 0.033*\"company\" + 0.030*\"order\" + '\n  '0.028*\"hold\" + 0.023*\"invest\" + 0.022*\"buy\" + 0.017*\"family\" + 0.013*\"true\" '\n  '+ 0.012*\"vaccin\" + 0.012*\"market\" + 0.012*\"sell\" + 0.011*\"lie\" + '\n  '0.011*\"slow\" + 0.010*\"bad\" + 0.010*\"desantis\" + 0.009*\"warehouse\" + '\n  '0.008*\"put\" + 0.007*\"speak\" + 0.006*\"promise\"'),\n (4,\n  '0.056*\"ship\" + 0.055*\"state\" + 0.054*\"start\" + 0.040*\"vaccination\" + '\n  '0.033*\"news\" + 0.023*\"include\" + 0.022*\"plan\" + 0.022*\"set\" + 0.020*\"ready\" '\n  '+ 0.020*\"shoot\" + 0.018*\"show\" + 0.017*\"call\" + 0.017*\"tomorrow\" + '\n  '0.017*\"morning\" + 0.015*\"staff\" + 0.011*\"site\" + 0.011*\"information\" + '\n  '0.010*\"bring\" + 0.009*\"stimulus\" + 0.009*\"head\"'),\n (5,\n  '0.072*\"expect\" + 0.069*\"arrive\" + 0.050*\"good\" + 0.042*\"update\" + '\n  '0.029*\"end\" + 0.026*\"day\" + 0.019*\"hear\" + 0.018*\"pandemic\" + 0.018*\"feel\" '\n  '+ 0.018*\"morning\" + 0.015*\"study\" + 0.014*\"efficacy\" + 0.013*\"number\" + '\n  '0.013*\"fauci\" + 0.013*\"thing\" + 0.013*\"high\" + 0.012*\"pay\" + 0.011*\"story\" '\n  '+ 0.009*\"due\" + 0.008*\"add\"'),\n (6,\n  '0.052*\"emergency\" + 0.044*\"money\" + 0.035*\"authorize\" + '\n  '0.034*\"authorization\" + 0.033*\"approval\" + 0.029*\"month\" + 0.021*\"follow\" + '\n  '0.020*\"question\" + 0.018*\"reason\" + 0.017*\"administration\" + 0.016*\"push\" + '\n  '0.016*\"late\" + 0.014*\"lot\" + 0.013*\"base\" + 0.013*\"mutation\" + 0.011*\"hour\" '\n  '+ 0.010*\"newly\" + 0.010*\"grant\" + 0.010*\"vaccinated\" + 0.010*\"ago\"'),\n (7,\n  '0.106*\"people\" + 0.047*\"vaccinate\" + 0.040*\"shot\" + 0.036*\"read\" + '\n  '0.030*\"time\" + 0.029*\"develop\" + 0.022*\"recommend\" + 0.018*\"require\" + '\n  '0.018*\"safe\" + 0.016*\"group\" + 0.014*\"expert\" + 0.013*\"difference\" + '\n  '0.013*\"side_effect\" + 0.011*\"storage\" + 0.011*\"panel\" + 0.010*\"severe\" + '\n  '0.010*\"biontech\" + 0.010*\"mode\" + 0.009*\"life\" + 0.009*\"love\"'),\n (8,\n  '0.050*\"trial\" + 0.035*\"year\" + 0.027*\"find\" + 0.027*\"hope\" + 0.026*\"test\" + '\n  '0.020*\"phase\" + 0.020*\"strain\" + 0.019*\"effective\" + 0.018*\"datum\" + '\n  '0.017*\"coronavirus\" + 0.017*\"talk\" + 0.016*\"early\" + 0.014*\"variant\" + '\n  '0.014*\"article\" + 0.014*\"create\" + 0.013*\"check\" + 0.013*\"point\" + '\n  '0.013*\"release\" + 0.012*\"full\" + 0.011*\"doctor\"'),\n (9,\n  '0.067*\"distribution\" + 0.026*\"roll\" + 0.025*\"speed\" + 0.025*\"part\" + '\n  '0.019*\"continue\" + 0.018*\"big\" + 0.018*\"give\" + 0.018*\"announce\" + '\n  '0.018*\"back\" + 0.017*\"happen\" + 0.016*\"fact\" + 0.015*\"prepare\" + '\n  '0.014*\"center\" + 0.013*\"modern\" + 0.011*\"move\" + 0.011*\"team\" + '\n  '0.009*\"problem\" + 0.009*\"store\" + 0.009*\"medical\" + 0.009*\"close\"')]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n[(0,\\n  \\'0.076*\"health\" + 0.050*\"worker\" + 0.042*\"care\" + 0.041*\"live\" + \\'\\n  \\'0.035*\"official\" + 0.035*\"administer\" + 0.031*\"watch\" + 0.030*\"case\" + \\'\\n  \\'0.027*\"report\" + 0.023*\"facility\"\\'),\\n (1,\\n  \\'0.065*\"work\" + 0.037*\"virus\" + 0.027*\"great\" + 0.025*\"scientist\" + \\'\\n  \\'0.017*\"initial\" + 0.016*\"long\" + 0.014*\"leave\" + 0.014*\"science\" + \\'\\n  \\'0.013*\"total\" + 0.013*\"truck\"\\'),\\n (2,\\n  \\'0.055*\"hospital\" + 0.050*\"wait\" + 0.030*\"distribute\" + 0.029*\"country\" + \\'\\n  \\'0.025*\"share\" + 0.022*\"deliver\" + 0.022*\"million\" + 0.020*\"government\" + \\'\\n  \\'0.018*\"yesterday\" + 0.017*\"break\"\\'),\\n (3,\\n  \\'0.119*\"stock\" + 0.112*\"trump\" + 0.033*\"company\" + 0.030*\"order\" + \\'\\n  \\'0.028*\"hold\" + 0.023*\"invest\" + 0.022*\"buy\" + 0.017*\"family\" + 0.013*\"true\" \\'\\n  \\'+ 0.012*\"vaccin\"\\'),\\n (4,\\n  \\'0.056*\"ship\" + 0.055*\"state\" + 0.054*\"start\" + 0.040*\"vaccination\" + \\'\\n  \\'0.033*\"news\" + 0.023*\"include\" + 0.022*\"plan\" + 0.022*\"set\" + 0.020*\"ready\" \\'\\n  \\'+ 0.020*\"shoot\"\\'),\\n (5,\\n  \\'0.072*\"expect\" + 0.069*\"arrive\" + 0.050*\"good\" + 0.042*\"update\" + \\'\\n  \\'0.029*\"end\" + 0.026*\"day\" + 0.019*\"hear\" + 0.018*\"pandemic\" + 0.018*\"feel\" \\'\\n  \\'+ 0.018*\"morning\"\\'),\\n (6,\\n  \\'0.052*\"emergency\" + 0.044*\"money\" + 0.035*\"authorize\" + \\'\\n  \\'0.034*\"authorization\" + 0.033*\"approval\" + 0.029*\"month\" + 0.021*\"follow\" + \\'\\n  \\'0.020*\"question\" + 0.018*\"reason\" + 0.017*\"administration\"\\'),\\n (7,\\n  \\'0.106*\"people\" + 0.047*\"vaccinate\" + 0.040*\"shot\" + 0.036*\"read\" + \\'\\n  \\'0.030*\"time\" + 0.029*\"develop\" + 0.022*\"recommend\" + 0.018*\"require\" + \\'\\n  \\'0.018*\"safe\" + 0.016*\"group\"\\'),\\n (8,\\n  \\'0.050*\"trial\" + 0.035*\"year\" + 0.027*\"find\" + 0.027*\"hope\" + 0.026*\"test\" + \\'\\n  \\'0.020*\"phase\" + 0.020*\"strain\" + 0.019*\"effective\" + 0.018*\"datum\" + \\'\\n  \\'0.017*\"coronavirus\"\\'),\\n (9,\\n  \\'0.067*\"distribution\" + 0.026*\"roll\" + 0.025*\"speed\" + 0.025*\"part\" + \\'\\n  \\'0.019*\"continue\" + 0.018*\"big\" + 0.018*\"give\" + 0.018*\"back\" + \\'\\n  \\'0.018*\"announce\" + 0.017*\"happen\"\\')]\\n  '"
      ]
     },
     "metadata": {},
     "execution_count": 209
    }
   ],
   "source": [
    "# Select the model and print the topics\n",
    "optimal_model = model_list[4] # x topics\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=20)) # 10\n",
    "# show the distribution of the top 10 words for each topic\n",
    "\n",
    "\"\"\"\n",
    "[(0,\n",
    "  '0.076*\"health\" + 0.050*\"worker\" + 0.042*\"care\" + 0.041*\"live\" + '\n",
    "  '0.035*\"official\" + 0.035*\"administer\" + 0.031*\"watch\" + 0.030*\"case\" + '\n",
    "  '0.027*\"report\" + 0.023*\"facility\"'),\n",
    " (1,\n",
    "  '0.065*\"work\" + 0.037*\"virus\" + 0.027*\"great\" + 0.025*\"scientist\" + '\n",
    "  '0.017*\"initial\" + 0.016*\"long\" + 0.014*\"leave\" + 0.014*\"science\" + '\n",
    "  '0.013*\"total\" + 0.013*\"truck\"'),\n",
    " (2,\n",
    "  '0.055*\"hospital\" + 0.050*\"wait\" + 0.030*\"distribute\" + 0.029*\"country\" + '\n",
    "  '0.025*\"share\" + 0.022*\"deliver\" + 0.022*\"million\" + 0.020*\"government\" + '\n",
    "  '0.018*\"yesterday\" + 0.017*\"break\"'),\n",
    " (3,\n",
    "  '0.119*\"stock\" + 0.112*\"trump\" + 0.033*\"company\" + 0.030*\"order\" + '\n",
    "  '0.028*\"hold\" + 0.023*\"invest\" + 0.022*\"buy\" + 0.017*\"family\" + 0.013*\"true\" '\n",
    "  '+ 0.012*\"vaccin\"'),\n",
    " (4,\n",
    "  '0.056*\"ship\" + 0.055*\"state\" + 0.054*\"start\" + 0.040*\"vaccination\" + '\n",
    "  '0.033*\"news\" + 0.023*\"include\" + 0.022*\"plan\" + 0.022*\"set\" + 0.020*\"ready\" '\n",
    "  '+ 0.020*\"shoot\"'),\n",
    " (5,\n",
    "  '0.072*\"expect\" + 0.069*\"arrive\" + 0.050*\"good\" + 0.042*\"update\" + '\n",
    "  '0.029*\"end\" + 0.026*\"day\" + 0.019*\"hear\" + 0.018*\"pandemic\" + 0.018*\"feel\" '\n",
    "  '+ 0.018*\"morning\"'),\n",
    " (6,\n",
    "  '0.052*\"emergency\" + 0.044*\"money\" + 0.035*\"authorize\" + '\n",
    "  '0.034*\"authorization\" + 0.033*\"approval\" + 0.029*\"month\" + 0.021*\"follow\" + '\n",
    "  '0.020*\"question\" + 0.018*\"reason\" + 0.017*\"administration\"'),\n",
    " (7,\n",
    "  '0.106*\"people\" + 0.047*\"vaccinate\" + 0.040*\"shot\" + 0.036*\"read\" + '\n",
    "  '0.030*\"time\" + 0.029*\"develop\" + 0.022*\"recommend\" + 0.018*\"require\" + '\n",
    "  '0.018*\"safe\" + 0.016*\"group\"'),\n",
    " (8,\n",
    "  '0.050*\"trial\" + 0.035*\"year\" + 0.027*\"find\" + 0.027*\"hope\" + 0.026*\"test\" + '\n",
    "  '0.020*\"phase\" + 0.020*\"strain\" + 0.019*\"effective\" + 0.018*\"datum\" + '\n",
    "  '0.017*\"coronavirus\"'),\n",
    " (9,\n",
    "  '0.067*\"distribution\" + 0.026*\"roll\" + 0.025*\"speed\" + 0.025*\"part\" + '\n",
    "  '0.019*\"continue\" + 0.018*\"big\" + 0.018*\"give\" + 0.018*\"back\" + '\n",
    "  '0.018*\"announce\" + 0.017*\"happen\"')]\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "\n",
    "# dictionary = id2word_stemmed (what they call id2word)\n",
    "# corpus = corpus_stemmed (what they call corpus)\n",
    "# text = tweets_stemmed (what they call data_lemmatized)\n",
    "optimal_model.save('optimal_mallet_model_10_topics.gensim')\n",
    "# ldamallet = gensim.models.wrappers.LdaMallet.load('lda_mallet_model.gensim')\n",
    "#id2word_lemmas.save('id2word_lemmas_dictionary.gensim')\n",
    "# id2word_stemmed = corpora.Dictionary.load('id2word_stemmed_dictionary.gensim')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I AM HERE NOW  - RUNNING SINGLE MODEL TO TEST TOPICS AND DOMINANT TOPICS \n",
    "# RETURN TO DO MULTIPLE MODELS LATER \n",
    "\n",
    "# OR DO A SINGLE MALLET MODEL \n",
    "# NOT NECESSARY IF USING ONE OF THE MODELS FROM ABOVE (FROM MODEL LIST)\n",
    "#import os\n",
    "#from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "#os.environ.update({'MALLET_HOME':r'C:/Users/Astridll/Desktop/Kandidat CogSci/1_Semester/Natural Language Processing/NLP_exam_code/mallet-2.0.8/'})\n",
    "#os.environ.update({'MALLET_HOME':r'C:/Users/Astridll/Desktop/Kandidat CogSci/1_Semester/Natural Language Processing/NLP_exam_code/new_mallet/mallet-2.0.8/'})\n",
    "#mallet_path = './Users/Astridll/Desktop/Kandidat CogSci/1_semester/Natural Language Processing/NLP_exam_code/mallet-2.0.8/bin/mallet'\n",
    "#./bin/mallet \n",
    "#r'C:\\Users\\Astridll\\Desktop\\Kandidat CogSci\\1_semester\\Natural Language Processing\\NLP_exam_code\\mallet-2.0.8\\bin\\mallet'\n",
    "#mallet_path = r'C:\\Users\\Astridll\\Desktop\\Kandidat CogSci\\1_semester\\Natural Language Processing\\NLP_exam_code\\mallet-2.0.8\\bin\\mallet' # update this path\n",
    "#mallet_path = r'C:\\Users\\Astridll\\Desktop\\Kandidat CogSci\\1_semester\\Natural Language Processing\\NLP_exam_code\\new_mallet\\mallet-2.0.8\\bin\\mallet' # update this path\n",
    "mallet_path = './mallet-2.0.8/bin/mallet'\n",
    "\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus_lemmas, num_topics=10, id2word=id2word_lemmas, iterations=1000) # optimize interval changed from 20 to 100 # , optimize_interval=100\n",
    "\n",
    "#ldamallet7_alpha = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus_lemmas, num_topics=7, id2word=id2word_lemmas, alpha=0.001, iterations=1000)\n",
    "#ldamallet7_t = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus_tokens, num_topics=7, id2word=id2word_tokens, optimize_interval=20, iterations=1000)\n",
    "\n",
    "#mallet_path = 'C:/Users/Astridll/Desktop/Kandidat CogSci/1_Semester/Natural Language Processing/NLP_exam_code/mallet-2.0.8/bin/mallet' # update this path\n",
    "#ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus_stemmed, num_topics=10, id2word=id2word_stemmed)\n",
    "\n",
    "#### WOHOOOOOOOOOO IT FINALLY WORKED !!!!!!!!!!! ######## :D :D :D :D :D :D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0,\n",
      "  '0.059*\"state\" + 0.041*\"company\" + 0.032*\"case\" + 0.027*\"find\" + '\n",
      "  '0.026*\"report\" + 0.026*\"day\" + 0.022*\"ready\" + 0.021*\"death\" + 0.021*\"line\" '\n",
      "  '+ 0.019*\"continue\"'),\n",
      " (1,\n",
      "  '0.055*\"ship\" + 0.040*\"shot\" + 0.035*\"year\" + 0.030*\"approval\" + '\n",
      "  '0.027*\"hope\" + 0.021*\"morning\" + 0.021*\"follow\" + 0.018*\"require\" + '\n",
      "  '0.018*\"feel\" + 0.015*\"create\"'),\n",
      " (2,\n",
      "  '0.117*\"stock\" + 0.109*\"trump\" + 0.042*\"money\" + 0.030*\"order\" + '\n",
      "  '0.027*\"hold\" + 0.024*\"speed\" + 0.018*\"pay\" + 0.017*\"call\" + 0.016*\"family\" '\n",
      "  '+ 0.015*\"push\"'),\n",
      " (3,\n",
      "  '0.059*\"start\" + 0.054*\"hospital\" + 0.041*\"vaccination\" + 0.029*\"end\" + '\n",
      "  '0.024*\"government\" + 0.024*\"deliver\" + 0.023*\"care\" + 0.023*\"facility\" + '\n",
      "  '0.022*\"set\" + 0.022*\"rollout\"'),\n",
      " (4,\n",
      "  '0.075*\"health\" + 0.070*\"arrive\" + 0.051*\"emergency\" + 0.050*\"worker\" + '\n",
      "  '0.040*\"live\" + 0.034*\"official\" + 0.034*\"administer\" + '\n",
      "  '0.033*\"authorization\" + 0.027*\"watch\" + 0.019*\"care\"'),\n",
      " (5,\n",
      "  '0.075*\"expect\" + 0.067*\"distribution\" + 0.044*\"update\" + 0.039*\"virus\" + '\n",
      "  '0.036*\"read\" + 0.026*\"scientist\" + 0.023*\"strain\" + 0.018*\"announce\" + '\n",
      "  '0.016*\"expert\" + 0.015*\"recommend\"'),\n",
      " (6,\n",
      "  '0.104*\"people\" + 0.049*\"vaccinate\" + 0.035*\"time\" + 0.027*\"great\" + '\n",
      "  '0.025*\"part\" + 0.025*\"invest\" + 0.024*\"include\" + 0.019*\"big\" + '\n",
      "  '0.018*\"pandemic\" + 0.014*\"efficacy\"'),\n",
      " (7,\n",
      "  '0.051*\"trial\" + 0.050*\"wait\" + 0.030*\"month\" + 0.026*\"test\" + 0.025*\"share\" '\n",
      "  '+ 0.020*\"phase\" + 0.018*\"back\" + 0.015*\"show\" + 0.013*\"release\" + '\n",
      "  '0.012*\"buy\"'),\n",
      " (8,\n",
      "  '0.065*\"work\" + 0.053*\"good\" + 0.034*\"news\" + 0.028*\"develop\" + '\n",
      "  '0.022*\"happen\" + 0.021*\"medical\" + 0.019*\"datum\" + 0.018*\"thing\" + '\n",
      "  '0.018*\"late\" + 0.017*\"break\"'),\n",
      " (9,\n",
      "  '0.034*\"authorize\" + 0.028*\"country\" + 0.024*\"roll\" + 0.024*\"shoot\" + '\n",
      "  '0.022*\"million\" + 0.021*\"hear\" + 0.019*\"question\" + 0.018*\"give\" + '\n",
      "  '0.017*\"distribute\" + 0.017*\"talk\"')]\n",
      "\n",
      "Coherence Score:  0.5041156679218192\n"
     ]
    }
   ],
   "source": [
    "# THIS IS A CONTIUATION OF THE SINGLE MALLET LDA MODEL - ALSO NOT NECESSARY IF USING MODEL FROM MODEL LIST COMPUTING COHERENCE SCORES ... \n",
    "\n",
    "\n",
    "# Show Topics\n",
    "pprint(ldamallet.show_topics(num_words=10)) # changed from (formatted=False)\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=tweets_lemmas, dictionary=id2word_lemmas, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)\n",
    "\n",
    "# for 10 topics: 0.5009 - topics look quite good\n",
    "# for 9 topics: 0.4883\n",
    "# for 20 topics: 0.5521\n",
    "# for 16 topics: 0.5218\n",
    "# for 6 topics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<gensim.models.wrappers.ldamallet.LdaMallet object at 0x1a32702350>\n"
     ]
    }
   ],
   "source": [
    "# Checking the topics\n",
    "print(optimal_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of model optimal_model\n",
    "from tethne.networks import topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2     -0.160779 -0.119056       1        1  10.051714\n",
       "8      0.177601 -0.198996       2        1  10.012852\n",
       "6     -0.194613  0.191819       3        1  10.011018\n",
       "3      0.172958  0.195600       4        1   9.996144\n",
       "1      0.191514  0.000703       5        1   9.995233\n",
       "9     -0.002912 -0.143546       6        1   9.994553\n",
       "0     -0.033027 -0.221800       7        1   9.994077\n",
       "7     -0.156537  0.092326       8        1   9.984284\n",
       "4      0.166530  0.179964       9        1   9.982624\n",
       "5     -0.160736  0.022987      10        1   9.977501, topic_info=            Term        Freq       Total Category  logprob  loglift\n",
       "312        stock  491.000000  491.000000  Default  30.0000  30.0000\n",
       "350        trump  457.000000  457.000000  Default  29.0000  29.0000\n",
       "2         people  430.000000  430.000000  Default  28.0000  28.0000\n",
       "112       health  309.000000  309.000000  Default  27.0000  27.0000\n",
       "143       expect  310.000000  310.000000  Default  26.0000  26.0000\n",
       "...          ...         ...         ...      ...      ...      ...\n",
       "1410      review   26.768278   27.961824  Topic10  -5.0428   2.2612\n",
       "1579   recommend   60.836996   88.268679  Topic10  -4.2219   1.9327\n",
       "577   scientific   24.334798   25.517862  Topic10  -5.1381   2.2574\n",
       "721        newly   34.068718   43.389197  Topic10  -4.8017   2.0630\n",
       "860       chance   26.768278   35.115431  Topic10  -5.0428   2.0334\n",
       "\n",
       "[673 rows x 6 columns], token_table=      Topic      Freq        Term\n",
       "term                             \n",
       "277       8  0.990087  absolutely\n",
       "324       6  0.998947      accord\n",
       "371       7  0.920431         add\n",
       "371      10  0.055784         add\n",
       "506       4  0.991789  additional\n",
       "...     ...       ...         ...\n",
       "2366      2  0.850447         wow\n",
       "1125      2  0.992188       write\n",
       "206       7  0.998963       wrong\n",
       "144       5  1.002931        year\n",
       "412       9  0.996764   yesterday\n",
       "\n",
       "[772 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 9, 7, 4, 2, 10, 1, 8, 5, 6])"
      ],
      "text/html": "\n<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n\n\n<div id=\"ldavis_el482821131401924645757976357\"></div>\n<script type=\"text/javascript\">\n\nvar ldavis_el482821131401924645757976357_data = {\"mdsDat\": {\"x\": [-0.1607789064705303, 0.17760146422252499, -0.19461320253819517, 0.17295796720624199, 0.1915143847603769, -0.0029121877432340447, -0.033027027885951674, -0.1565369918219319, 0.16653036641975663, -0.16073586614905777], \"y\": [-0.11905595387499682, -0.19899623148935258, 0.19181898207239217, 0.19559959616687467, 0.000702908618483074, -0.14354552611972074, -0.22180022645717987, 0.09232570637680255, 0.17996398407465272, 0.022986760632044627], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [10.05171386454539, 10.0128517836552, 10.011018292720763, 9.996143891425668, 9.99523267169509, 9.994552916852346, 9.994077089314827, 9.984284060257517, 9.982623936733974, 9.977501492799206]}, \"tinfo\": {\"Term\": [\"stock\", \"trump\", \"people\", \"health\", \"expect\", \"arrive\", \"distribution\", \"work\", \"state\", \"start\", \"ship\", \"hospital\", \"good\", \"trial\", \"emergency\", \"worker\", \"wait\", \"vaccinate\", \"update\", \"money\", \"vaccination\", \"live\", \"shot\", \"company\", \"virus\", \"read\", \"year\", \"authorize\", \"official\", \"administer\", \"refuse\", \"disrupt\", \"obvious\", \"push\", \"liar\", \"deny\", \"authority\", \"dosage\", \"advantage\", \"facial\", \"dead\", \"keeping\", \"material\", \"cash\", \"search\", \"fuck\", \"crony\", \"butler\", \"lock\", \"enjoy\", \"screw\", \"appointment\", \"pfeizer\", \"financial\", \"hard\", \"tie\", \"corrupt\", \"press\", \"true\", \"family\", \"trump\", \"stock\", \"order\", \"hold\", \"money\", \"call\", \"friend\", \"slow\", \"life\", \"profit\", \"make\", \"bet\", \"apparently\", \"speed\", \"pretty\", \"american\", \"mind\", \"save\", \"recent\", \"stake\", \"pay\", \"chief\", \"put\", \"lie\", \"investment\", \"reason\", \"buy\", \"throttle\", \"disperse\", \"satan\", \"suicidal\", \"angle\", \"flow\", \"hudson\", \"cheap\", \"speculator\", \"industry\", \"hoboken\", \"wow\", \"pride\", \"ocugen\", \"teared\", \"semi\", \"eas\", \"woul\", \"cove\", \"mechanism\", \"noise\", \"substantially\", \"required\", \"thos\", \"amino\", \"spot\", \"oravec\", \"lije\", \"coivd\", \"additionally\", \"work\", \"good\", \"news\", \"happen\", \"medical\", \"datum\", \"thing\", \"late\", \"break\", \"fact\", \"science\", \"team\", \"truck\", \"head\", \"full\", \"move\", \"protect\", \"develop\", \"shipping\", \"choice\", \"cover\", \"amazing\", \"partner\", \"anticipate\", \"happy\", \"write\", \"carry\", \"focus\", \"candidate\", \"nice\", \"private\", \"drug\", \"important\", \"supply\", \"arm\", \"site\", \"fast\", \"turn\", \"leave\", \"trust\", \"morning\", \"anti\", \"limit\", \"count\", \"worried\", \"doubt\", \"german\", \"pass\", \"trading\", \"email\", \"body\", \"initially\", \"big\", \"damn\", \"normal\", \"pziser\", \"tear\", \"foe\", \"party\", \"political\", \"tout\", \"warehouse\", \"culture\", \"strike\", \"sucker\", \"worst\", \"filled\", \"challen\", \"rid\", \"calvary\", \"grad\", \"people\", \"vaccinate\", \"include\", \"pandemic\", \"efficacy\", \"point\", \"side_effect\", \"sell\", \"lead\", \"story\", \"employee\", \"great\", \"severe\", \"member\", \"side\", \"concern\", \"part\", \"invest\", \"age\", \"kill\", \"funding\", \"antibody\", \"sound\", \"suggest\", \"source\", \"fund\", \"moder\", \"time\", \"base\", \"biontech\", \"safe\", \"effect\", \"vaccin\", \"easy\", \"blanket\", \"owner\", \"romancescam\", \"obtainable\", \"start_arrive\", \"creature\", \"optionsflow\", \"approximately\", \"dufu\", \"solidify\", \"virtually\", \"happiness\", \"engage\", \"immun\", \"widely\", \"college\", \"instrumental\", \"giri\", \"ter\", \"serv\", \"hhs\", \"indicative\", \"nitpick\", \"quit\", \"timescale\", \"oxycontin\", \"decembe\", \"frontli\", \"divvy\", \"administering\", \"vaccination\", \"start\", \"end\", \"government\", \"facility\", \"set\", \"rollout\", \"staff\", \"small\", \"deal\", \"hospital\", \"information\", \"desantis\", \"interesting\", \"resident\", \"place\", \"additional\", \"package\", \"guy\", \"job\", \"track\", \"link\", \"date\", \"relief\", \"fake\", \"assume\", \"allocate\", \"politician\", \"thumb\", \"drop\", \"patient\", \"plan\", \"group\", \"deliver\", \"real\", \"schedule\", \"stop\", \"reach\", \"early\", \"care\", \"provider\", \"reinforce\", \"attorney\", \"donor\", \"sovereignty\", \"devise\", \"threaten\", \"incite\", \"council\", \"satire\", \"formally\", \"loser\", \"availble\", \"reassur\", \"garner\", \"tcot\", \"swab\", \"approving\", \"offend\", \"tariff\", \"ponder\", \"burn\", \"elpaso\", \"surpise\", \"fantasy\", \"synthe\", \"vince\", \"variant_cnn\", \"formula\", \"daybreak\", \"ball\", \"shot\", \"year\", \"hope\", \"follow\", \"require\", \"create\", \"difference\", \"remember\", \"storage\", \"stimulus\", \"guess\", \"product\", \"due\", \"learn\", \"technology\", \"pharmacy\", \"freezer\", \"governor\", \"rest\", \"folk\", \"curious\", \"stay\", \"suppose\", \"temperature\", \"type\", \"depend\", \"bill\", \"rush\", \"spend\", \"boost\", \"ship\", \"feel\", \"approval\", \"bring\", \"store\", \"check\", \"vote\", \"area\", \"morning\", \"lot\", \"brighten\", \"devotion\", \"cryogenic\", \"cross\", \"anaphyllactic\", \"counseling\", \"background\", \"anticipation\", \"dealer\", \"plummet\", \"authorized\", \"piss\", \"nuce\", \"traffiker\", \"busi\", \"kizzy\", \"perform\", \"uam\", \"producti\", \"mhealth\", \"gymnastic\", \"invent\", \"citizen\", \"club\", \"seize\", \"application\", \"visa\", \"literary\", \"morph\", \"join\", \"authorize\", \"give\", \"talk\", \"initial\", \"study\", \"prepare\", \"country\", \"answer\", \"accord\", \"process\", \"purchase\", \"person\", \"large\", \"die\", \"reaction\", \"round\", \"post\", \"increase\", \"step\", \"nation\", \"flu\", \"vial\", \"discuss\", \"forget\", \"question\", \"comment\", \"fine\", \"reduce\", \"shoot\", \"million\", \"roll\", \"research\", \"fight\", \"cell\", \"hear\", \"high\", \"coronavirus\", \"number\", \"distribute\", \"key\", \"mode\", \"reason\", \"message\", \"death\", \"situation\", \"election\", \"article\", \"statement\", \"relate\", \"surge\", \"pick\", \"student\", \"sit\", \"sue\", \"maker\", \"wit\", \"positive\", \"connection\", \"compare\", \"record\", \"term\", \"protective\", \"find\", \"husband\", \"refrigeration\", \"day\", \"claim\", \"wrong\", \"case\", \"design\", \"hour\", \"thousand\", \"state\", \"ready\", \"delivery\", \"send\", \"market\", \"info\", \"clear\", \"close\", \"report\", \"ingredient\", \"decide\", \"biden\", \"waiting\", \"phizer\", \"man\", \"line\", \"company\", \"continue\", \"world\", \"doctor\", \"vaccinated\", \"production\", \"county\", \"effective\", \"add\", \"distribute\", \"loot\", \"shar\", \"fighting\", \"adverse\", \"concede\", \"aviation\", \"psp\", \"incentive\", \"meeting\", \"illness\", \"shelter\", \"jab\", \"careful\", \"aur\", \"loose\", \"meaningful\", \"poison\", \"surg\", \"osbert\", \"reap\", \"overseas\", \"pottawattamie\", \"repre\", \"highest\", \"fda\", \"sex\", \"confid\", \"instance\", \"nose\", \"interest\", \"trial\", \"month\", \"test\", \"share\", \"back\", \"release\", \"bad\", \"prevent\", \"panel\", \"ago\", \"wait\", \"understand\", \"leader\", \"allocation\", \"offer\", \"option\", \"version\", \"confirm\", \"mutate\", \"placebo\", \"evidence\", \"child\", \"population\", \"infection\", \"excited\", \"disease\", \"nurse\", \"absolutely\", \"stand\", \"thought\", \"phase\", \"produce\", \"choose\", \"show\", \"agree\", \"participant\", \"rate\", \"buy\", \"inform\", \"understanding\", \"bolster\", \"clinic\", \"frontline\", \"vac\", \"department\", \"fedex\", \"goal\", \"safety\", \"arrive\", \"emergency\", \"air\", \"yesterday\", \"ning\", \"long\", \"lawmaker\", \"secretary\", \"vacci\", \"city\", \"azar\", \"fed\", \"front_line\", \"local\", \"miss\", \"health\", \"district\", \"possibility\", \"tempera\", \"admi\", \"worker\", \"live\", \"official\", \"administer\", \"public\", \"authorization\", \"fauci\", \"grant\", \"mask\", \"program\", \"pfize\", \"human_service\", \"total\", \"business\", \"healthcare\", \"tomorrow\", \"watch\", \"issue\", \"federal\", \"top\", \"eua\", \"administration\", \"care\", \"priority\", \"play\", \"spread\", \"final\", \"read\", \"diving\", \"summer\", \"megaphone\", \"ozark\", \"travel\", \"seemingly\", \"laughter\", \"sto\", \"fun\", \"donaldo\", \"truth\", \"viral\", \"allocating\", \"atm\", \"optic\", \"anytime\", \"nope\", \"observatio\", \"secur\", \"laugh\", \"cova\", \"lean\", \"metabolic\", \"flash\", \"recite\", \"expect\", \"scientist\", \"update\", \"strain\", \"distribution\", \"expert\", \"center\", \"variant\", \"modern\", \"mutation\", \"credit\", \"rural\", \"sign\", \"effort\", \"speak\", \"ahead\", \"cold\", \"role\", \"support\", \"recently\", \"super\", \"front\", \"regard\", \"warn\", \"virus\", \"announce\", \"review\", \"recommend\", \"scientific\", \"newly\", \"chance\"], \"Freq\": [491.0, 457.0, 430.0, 309.0, 310.0, 290.0, 283.0, 270.0, 247.0, 248.0, 237.0, 229.0, 219.0, 211.0, 210.0, 206.0, 212.0, 206.0, 182.0, 175.0, 171.0, 167.0, 164.0, 179.0, 167.0, 147.0, 146.0, 143.0, 142.0, 140.0, 26.216186217848655, 5.699170916923621, 11.398341833847242, 63.83071426954456, 5.699170916923621, 5.699170916923621, 5.699170916923621, 5.699170916923621, 5.699170916923621, 5.699170916923621, 19.37718111754031, 5.699170916923621, 5.699170916923621, 11.398341833847242, 5.699170916923621, 23.936517851079206, 19.37718111754031, 5.699170916923621, 5.699170916923621, 5.699170916923621, 7.97883928369307, 7.97883928369307, 5.699170916923621, 21.65684948430976, 37.6145280516959, 15.95767856738614, 7.97883928369307, 19.37718111754031, 51.29253825231259, 68.39005100308344, 457.0735075372744, 487.8490304886619, 124.24192598893492, 112.8435841550877, 174.3946300578628, 69.52988518646816, 46.73320151877369, 45.59336733538897, 42.17386478523479, 42.17386478523479, 42.17386478523479, 36.47469386831118, 33.055191318157, 99.16557395447101, 29.63568876800283, 29.63568876800283, 26.216186217848655, 25.076352034463934, 20.517015300925035, 19.37718111754031, 75.2290561033918, 39.89419641846534, 50.15270406892787, 41.03403060185007, 22.796683667694484, 39.89419641846534, 44.45353315200424, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 7.055113650105203, 1.1758522750175338, 4.703409100070135, 1.1758522750175338, 9.40681820014027, 2.3517045500350675, 1.1758522750175338, 3.5275568250526015, 1.1758522750175338, 1.1758522750175338, 3.5275568250526015, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 270.4460232540328, 219.88437542827882, 141.10227300210408, 90.5406251763501, 87.0130683512975, 79.95795470119229, 76.4303978761397, 75.25454560112216, 71.72698877606956, 65.8477274009819, 58.7926137508767, 52.91335237578902, 52.91335237578902, 49.38579555073642, 48.209943275718885, 45.85823872568382, 44.682386450666286, 116.40937522673585, 42.33068190063121, 31.74801142547341, 31.74801142547341, 30.572159150455878, 25.868750050385746, 23.517045500350676, 22.341193225333143, 21.165340950315606, 21.165340950315606, 19.989488675298073, 18.81363640028054, 17.63778412526301, 17.63778412526301, 38.80312507557862, 45.85823872568382, 48.209943275718885, 29.39630687543835, 42.33068190063121, 35.27556825052602, 38.80312507557862, 44.682386450666286, 36.45142052554355, 58.7926137508767, 8.40036181632042, 8.40036181632042, 8.40036181632042, 8.40036181632042, 8.40036181632042, 8.40036181632042, 33.60144726528168, 8.40036181632042, 8.40036181632042, 21.60093038482394, 8.40036181632042, 78.00335972297533, 8.40036181632042, 21.60093038482394, 1.2000516880457743, 3.6001550641373234, 1.2000516880457743, 7.200310128274647, 14.400620256549294, 3.6001550641373234, 39.60170570551056, 1.2000516880457743, 13.200568568503519, 1.2000516880457743, 1.2000516880457743, 1.2000516880457743, 1.2000516880457743, 2.4001033760915487, 1.2000516880457743, 1.2000516880457743, 430.81855600843306, 205.2088386558274, 100.80434179584506, 76.80330803492956, 58.80253271424295, 56.4024293381514, 51.602222585968306, 50.40217089792253, 49.202119209876756, 48.00206752183098, 48.00206752183098, 111.60480698825702, 40.801757393556336, 40.801757393556336, 34.80149895332745, 34.80149895332745, 103.20444517193661, 103.20444517193661, 32.40139557723591, 28.801240513098588, 26.401137137007037, 26.401137137007037, 25.201085448961265, 24.00103376091549, 24.00103376091549, 24.00103376091549, 22.800982072869715, 144.0062025654929, 51.602222585968306, 37.201602329419, 49.202119209876756, 28.801240513098588, 32.40139557723591, 28.801240513098588, 1.1883287761088268, 3.56498632832648, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 5.941643880544134, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 8.318301432761787, 4.753315104435307, 2.3766575522176536, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 3.56498632832648, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 2.3766575522176536, 1.1883287761088268, 1.1883287761088268, 171.11934375967104, 247.17238543063598, 122.39786393920915, 99.81961719314144, 96.25463086481497, 92.68964453648849, 91.50131576037965, 61.793096357658996, 58.22811002933251, 49.90980859657072, 225.78246746067708, 46.34482226824424, 41.59150716380894, 40.40317838770011, 40.40317838770011, 36.838192059373625, 33.27320573104715, 30.896548178829498, 28.51989062661184, 27.331561850503014, 27.331561850503014, 23.766575522176534, 22.57824674606771, 21.38991796995888, 20.201589193850054, 20.201589193850054, 17.8249316416324, 17.8249316416324, 16.636602865523574, 16.636602865523574, 64.16975390987665, 91.50131576037965, 62.981425133767814, 99.81961719314144, 45.15649349213542, 30.896548178829498, 61.793096357658996, 36.838192059373625, 67.73474023820313, 97.4429596409238, 29.708219402720673, 1.1916290012496928, 1.1916290012496928, 4.766516004998771, 1.1916290012496928, 1.1916290012496928, 2.3832580024993857, 4.766516004998771, 1.1916290012496928, 1.1916290012496928, 2.3832580024993857, 2.3832580024993857, 1.1916290012496928, 1.1916290012496928, 2.3832580024993857, 1.1916290012496928, 1.1916290012496928, 1.1916290012496928, 1.1916290012496928, 1.1916290012496928, 1.1916290012496928, 4.766516004998771, 1.1916290012496928, 1.1916290012496928, 1.1916290012496928, 1.1916290012496928, 1.1916290012496928, 1.1916290012496928, 1.1916290012496928, 1.1916290012496928, 2.3832580024993857, 164.44480217245766, 146.57036715371223, 110.82149711622145, 86.98891709122759, 75.07262707873066, 61.96470806498404, 56.00656305873557, 47.665160049987726, 42.898644044988956, 40.51538604248957, 39.32375704123987, 38.13212803999017, 36.940499038740484, 35.74887003749079, 34.5572410362411, 32.17398303374171, 32.17398303374171, 30.98235403249202, 30.98235403249202, 28.599096029992634, 27.40746702874294, 27.40746702874294, 25.024209026243554, 22.64095102374417, 21.449322022494478, 20.257693021244783, 19.066064019995085, 19.066064019995085, 17.874435018745395, 16.6828060174957, 228.79276823994107, 73.88099807748097, 126.31267413246746, 41.707015043739254, 36.940499038740484, 52.4316760549865, 52.4316760549865, 32.17398303374171, 86.98891709122759, 45.28190204748834, 1.1830635694228955, 1.1830635694228955, 1.1830635694228955, 7.098381416537372, 1.1830635694228955, 1.1830635694228955, 1.1830635694228955, 4.732254277691582, 1.1830635694228955, 1.1830635694228955, 23.661271388457905, 5.915317847114476, 1.1830635694228955, 1.1830635694228955, 1.1830635694228955, 1.1830635694228955, 4.732254277691582, 1.1830635694228955, 1.1830635694228955, 1.1830635694228955, 1.1830635694228955, 5.915317847114476, 9.464508555383164, 1.1830635694228955, 4.732254277691582, 4.732254277691582, 1.1830635694228955, 1.1830635694228955, 1.1830635694228955, 36.674970652109764, 143.15069190017036, 75.71606844306531, 70.98381416537373, 69.80075059595083, 61.51930560999057, 60.33624204056766, 118.30635694228954, 40.224161360378446, 39.04109779095555, 36.674970652109764, 34.30884351326397, 34.30884351326397, 33.12577994384107, 31.94271637441818, 30.759652804995284, 29.576589235572385, 27.210462096726594, 27.210462096726594, 26.0273985273037, 24.844334957880804, 23.661271388457905, 23.661271388457905, 23.661271388457905, 20.112080680189223, 80.44832272075689, 18.929017110766328, 18.929017110766328, 17.745953541343432, 100.56040340094611, 89.91283127614005, 101.74346697036901, 55.60398776287608, 39.04109779095555, 39.04109779095555, 86.36364056787137, 47.32254277691581, 59.15317847114477, 48.505606346338716, 72.16687773479661, 28.39352566614949, 29.576589235572385, 30.759652804995284, 8.35481869776849, 88.32236909069546, 8.35481869776849, 8.35481869776849, 62.06436746913735, 10.741909754273772, 21.483819508547544, 21.483819508547544, 8.35481869776849, 8.35481869776849, 33.41927479107396, 13.129000810779056, 13.129000810779056, 8.35481869776849, 27.451547149810754, 8.35481869776849, 34.6128203193266, 16.70963739553698, 13.129000810779056, 8.35481869776849, 113.38682518400094, 16.70963739553698, 8.35481869776849, 108.61264307099037, 27.451547149810754, 31.032183734568676, 133.67709916429584, 31.032183734568676, 46.548275601853014, 22.677365036800186, 247.06392434829678, 89.51591461894812, 65.64500405389529, 58.48373088437943, 50.12891218661094, 41.77409348884245, 41.77409348884245, 36.999911375831886, 109.80618859924301, 27.451547149810754, 25.06445609330547, 23.870910565052828, 20.290273980294906, 20.290273980294906, 19.096728452042264, 87.12882356244283, 170.67701054012772, 78.77400486467434, 45.35473007360037, 48.9353666583583, 39.38700243233717, 32.22572926282132, 44.16118454534773, 65.64500405389529, 33.41927479107396, 52.516003243116224, 1.202395959270639, 1.202395959270639, 1.202395959270639, 4.809583837082556, 1.202395959270639, 1.202395959270639, 1.202395959270639, 2.404791918541278, 13.226355551977028, 4.809583837082556, 1.202395959270639, 1.202395959270639, 4.809583837082556, 1.202395959270639, 1.202395959270639, 1.202395959270639, 10.82156363343575, 1.202395959270639, 1.202395959270639, 1.202395959270639, 1.202395959270639, 1.202395959270639, 1.202395959270639, 1.202395959270639, 1.202395959270639, 2.404791918541278, 1.202395959270639, 1.202395959270639, 1.202395959270639, 15.631147470518307, 211.62168883163244, 125.04917976414646, 109.41803229362814, 103.40605249727494, 74.54854947477962, 55.310214126449395, 43.286254533743, 43.286254533743, 43.286254533743, 40.88146261520173, 209.21689691309118, 38.47667069666045, 38.47667069666045, 37.27427473738981, 34.869482818848525, 32.464690900307254, 31.262294941036615, 31.262294941036615, 31.262294941036615, 30.059898981765972, 30.059898981765972, 28.857503022495337, 27.655107063224698, 27.655107063224698, 26.452711103954055, 26.452711103954055, 26.452711103954055, 25.250315144683416, 25.250315144683416, 25.250315144683416, 84.16771714894473, 49.2982343300962, 37.27427473738981, 63.72698584134387, 31.262294941036615, 32.464690900307254, 30.059898981765972, 49.2982343300962, 5.825299786726467, 12.815659530798229, 5.825299786726467, 25.631319061596457, 33.78673876301351, 40.77709850708528, 23.30119914690587, 33.78673876301351, 5.825299786726467, 33.78673876301351, 290.09992937897806, 210.87585227949813, 8.155419701417054, 72.2337173554082, 8.155419701417054, 72.2337173554082, 8.155419701417054, 5.825299786726467, 31.456618848322925, 16.31083940283411, 32.62167880566822, 12.815659530798229, 24.466259104251165, 51.262638123192914, 12.815659530798229, 309.9059486538481, 12.815659530798229, 1.1650599573452936, 1.1650599573452936, 1.1650599573452936, 206.21561245011696, 167.76863385772228, 142.13731479612582, 140.97225483878054, 71.06865739806291, 136.31201500939935, 52.427698080538214, 41.94215846443057, 22.13613918956058, 22.13613918956058, 15.145779445488817, 15.145779445488817, 53.592758037883506, 38.446978592394686, 36.1168586777041, 67.57347752602702, 111.84575590514818, 54.7578179952288, 27.961438976287045, 50.09757816584762, 36.1168586777041, 47.76745825115703, 76.89395718478937, 20.68457850555285, 41.3691570111057, 47.45285657156242, 30.418497802283603, 147.22552936305263, 1.2167399120913442, 3.6502197362740327, 1.2167399120913442, 1.2167399120913442, 21.901318417644195, 1.2167399120913442, 1.2167399120913442, 1.2167399120913442, 3.6502197362740327, 1.2167399120913442, 14.600878945096131, 6.08369956045672, 1.2167399120913442, 1.2167399120913442, 1.2167399120913442, 1.2167399120913442, 1.2167399120913442, 1.2167399120913442, 1.2167399120913442, 3.6502197362740327, 1.2167399120913442, 6.08369956045672, 1.2167399120913442, 1.2167399120913442, 1.2167399120913442, 309.0519376712014, 108.28985217612963, 181.29424690161028, 93.6889732310335, 279.85017978100916, 66.92069516502393, 59.620255692475865, 59.620255692475865, 57.186775868293175, 53.53655613201915, 53.53655613201915, 38.935677186923016, 37.71893727483167, 34.06871753855764, 29.201757890192262, 26.768278066009575, 26.768278066009575, 25.551538153918226, 24.33479824182688, 23.11805832973554, 21.901318417644195, 21.901318417644195, 21.901318417644195, 19.467838593461508, 161.82640830814879, 75.43787454966333, 26.768278066009575, 60.83699560456721, 24.33479824182688, 34.06871753855764, 26.768278066009575], \"Total\": [491.0, 457.0, 430.0, 309.0, 310.0, 290.0, 283.0, 270.0, 247.0, 248.0, 237.0, 229.0, 219.0, 211.0, 210.0, 206.0, 212.0, 206.0, 182.0, 175.0, 171.0, 167.0, 164.0, 179.0, 167.0, 147.0, 146.0, 143.0, 142.0, 140.0, 26.216186217848655, 5.699170916923621, 11.398341833847242, 63.83071426954456, 5.699170916923621, 5.699170916923621, 5.699170916923621, 5.699170916923621, 5.699170916923621, 5.699170916923621, 19.37718111754031, 5.699170916923621, 5.699170916923621, 11.398341833847242, 5.699170916923621, 23.936517851079206, 19.37718111754031, 5.699170916923621, 5.699170916923621, 5.699170916923621, 7.97883928369307, 7.97883928369307, 5.699170916923621, 21.65684948430976, 37.6145280516959, 15.95767856738614, 7.97883928369307, 19.37718111754031, 51.29253825231259, 68.39005100308344, 457.0735075372744, 491.4140168169884, 124.24192598893492, 112.8435841550877, 175.59702601713343, 69.52988518646816, 46.73320151877369, 45.59336733538897, 42.17386478523479, 42.17386478523479, 42.17386478523479, 36.47469386831118, 33.055191318157, 100.35390273057985, 29.63568876800283, 29.63568876800283, 26.216186217848655, 25.076352034463934, 20.517015300925035, 19.37718111754031, 76.42910779143757, 41.09659237773598, 58.419056500520824, 48.33447007439813, 23.961743625039777, 73.04094027996591, 93.75176748210043, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 7.055113650105203, 1.1758522750175338, 4.703409100070135, 1.1758522750175338, 9.40681820014027, 2.3517045500350675, 1.1758522750175338, 3.5275568250526015, 1.1758522750175338, 1.1758522750175338, 3.5275568250526015, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 1.1758522750175338, 270.4460232540328, 219.88437542827882, 141.10227300210408, 90.5406251763501, 87.0130683512975, 79.95795470119229, 76.4303978761397, 75.25454560112216, 71.72698877606956, 65.8477274009819, 58.7926137508767, 52.91335237578902, 52.91335237578902, 49.38579555073642, 48.209943275718885, 45.85823872568382, 44.682386450666286, 117.60292075498849, 42.33068190063121, 31.74801142547341, 31.74801142547341, 30.572159150455878, 25.868750050385746, 23.517045500350676, 22.341193225333143, 21.165340950315606, 21.165340950315606, 19.989488675298073, 18.81363640028054, 17.63778412526301, 17.63778412526301, 39.991453851687446, 48.23489627790147, 51.759133983987574, 30.598702834708988, 47.12230881647163, 38.82475895879471, 44.59928517795044, 58.931801350600345, 48.33316446320204, 145.7815308421043, 8.40036181632042, 8.40036181632042, 8.40036181632042, 8.40036181632042, 8.40036181632042, 8.40036181632042, 33.60144726528168, 8.40036181632042, 8.40036181632042, 21.60093038482394, 8.40036181632042, 78.00335972297533, 8.40036181632042, 21.60093038482394, 1.2000516880457743, 3.6001550641373234, 1.2000516880457743, 7.200310128274647, 14.400620256549294, 3.6001550641373234, 39.60170570551056, 1.2000516880457743, 13.200568568503519, 1.2000516880457743, 1.2000516880457743, 1.2000516880457743, 1.2000516880457743, 2.4001033760915487, 1.2000516880457743, 1.2000516880457743, 430.81855600843306, 206.4004676570771, 100.80434179584506, 76.80330803492956, 58.80253271424295, 56.4024293381514, 51.602222585968306, 50.40217089792253, 49.202119209876756, 48.00206752183098, 48.00206752183098, 112.74464117164175, 40.801757393556336, 40.801757393556336, 34.80149895332745, 34.80149895332745, 104.3695051292819, 104.38750874135951, 32.40139557723591, 28.801240513098588, 26.401137137007037, 26.401137137007037, 25.201085448961265, 24.00103376091549, 24.00103376091549, 24.00103376091549, 22.800982072869715, 147.5545205253376, 52.785286155391205, 39.60639424796028, 71.78036595594446, 32.39072752887451, 49.877294937415314, 34.68050188818626, 1.1883287761088268, 3.56498632832648, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 5.941643880544134, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 8.318301432761787, 4.753315104435307, 2.3766575522176536, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 3.56498632832648, 1.1883287761088268, 1.1883287761088268, 1.1883287761088268, 2.3766575522176536, 1.1883287761088268, 1.1883287761088268, 171.11934375967104, 248.37478138990662, 122.39786393920915, 99.81961719314144, 96.25463086481497, 92.68964453648849, 91.50131576037965, 61.793096357658996, 58.22811002933251, 49.90980859657072, 229.389655338489, 46.34482226824424, 41.59150716380894, 40.40317838770011, 40.40317838770011, 36.838192059373625, 33.27320573104715, 30.896548178829498, 28.51989062661184, 27.331561850503014, 27.331561850503014, 23.766575522176534, 22.57824674606771, 21.38991796995888, 20.201589193850054, 20.201589193850054, 17.8249316416324, 17.8249316416324, 16.636602865523574, 16.636602865523574, 65.36138291112634, 93.88457376287904, 64.17305413501751, 108.2363889080359, 47.523974768402645, 32.09894413810014, 70.14526632442778, 41.64777589645618, 92.7991963315086, 174.33691682571316, 34.44047368041225, 1.1916290012496928, 1.1916290012496928, 4.766516004998771, 1.1916290012496928, 1.1916290012496928, 2.3832580024993857, 4.766516004998771, 1.1916290012496928, 1.1916290012496928, 2.3832580024993857, 2.3832580024993857, 1.1916290012496928, 1.1916290012496928, 2.3832580024993857, 1.1916290012496928, 1.1916290012496928, 1.1916290012496928, 1.1916290012496928, 1.1916290012496928, 1.1916290012496928, 4.766516004998771, 1.1916290012496928, 1.1916290012496928, 1.1916290012496928, 1.1916290012496928, 1.1916290012496928, 1.1916290012496928, 1.1916290012496928, 1.1916290012496928, 2.3832580024993857, 164.44480217245766, 146.57036715371223, 110.82149711622145, 86.98891709122759, 75.07262707873066, 61.96470806498404, 56.00656305873557, 47.665160049987726, 42.898644044988956, 40.51538604248957, 39.32375704123987, 38.13212803999017, 36.940499038740484, 35.74887003749079, 34.5572410362411, 32.17398303374171, 32.17398303374171, 30.98235403249202, 30.98235403249202, 28.599096029992634, 27.40746702874294, 27.40746702874294, 25.024209026243554, 22.64095102374417, 21.449322022494478, 20.257693021244783, 19.066064019995085, 19.066064019995085, 17.874435018745395, 16.6828060174957, 237.11106967270285, 76.24712521632676, 135.8610383584886, 42.89007861316215, 38.12882781484931, 57.13508515505663, 57.2318828071696, 34.45365140051116, 145.7815308421043, 62.082625680129176, 1.1830635694228955, 1.1830635694228955, 1.1830635694228955, 7.098381416537372, 1.1830635694228955, 1.1830635694228955, 1.1830635694228955, 4.732254277691582, 1.1830635694228955, 1.1830635694228955, 23.661271388457905, 5.915317847114476, 1.1830635694228955, 1.1830635694228955, 1.1830635694228955, 1.1830635694228955, 4.732254277691582, 1.1830635694228955, 1.1830635694228955, 1.1830635694228955, 1.1830635694228955, 5.915317847114476, 9.464508555383164, 1.1830635694228955, 4.732254277691582, 4.732254277691582, 1.1830635694228955, 1.1830635694228955, 1.1830635694228955, 36.674970652109764, 143.15069190017036, 75.71606844306531, 70.98381416537373, 69.80075059595083, 61.51930560999057, 60.33624204056766, 119.49468571839837, 40.224161360378446, 39.04109779095555, 36.674970652109764, 34.30884351326397, 34.30884351326397, 33.12577994384107, 31.94271637441818, 30.759652804995284, 29.576589235572385, 27.210462096726594, 27.210462096726594, 26.0273985273037, 24.844334957880804, 23.661271388457905, 23.661271388457905, 23.661271388457905, 20.112080680189223, 81.61338267810218, 18.929017110766328, 18.929017110766328, 17.745953541343432, 102.90028927237661, 92.1924996429095, 105.32410355512694, 57.934107677566665, 40.20615774830084, 40.24114947900132, 94.68194200063316, 53.312988657227116, 72.01200727263196, 56.92237806123319, 132.91384690303556, 29.593577354195265, 38.99588393680395, 73.04094027996591, 8.35481869776849, 88.32236909069546, 8.35481869776849, 8.35481869776849, 62.06436746913735, 10.741909754273772, 21.483819508547544, 21.483819508547544, 8.35481869776849, 8.35481869776849, 33.41927479107396, 13.129000810779056, 13.129000810779056, 8.35481869776849, 27.451547149810754, 8.35481869776849, 34.6128203193266, 16.70963739553698, 13.129000810779056, 8.35481869776849, 113.38682518400094, 16.70963739553698, 8.35481869776849, 108.61264307099037, 27.451547149810754, 31.032183734568676, 133.67709916429584, 31.032183734568676, 46.548275601853014, 22.677365036800186, 247.06392434829678, 89.51591461894812, 65.64500405389529, 58.48373088437943, 50.12891218661094, 41.77409348884245, 41.77409348884245, 36.999911375831886, 111.02292851133436, 27.451547149810754, 25.06445609330547, 23.870910565052828, 20.290273980294906, 20.290273980294906, 19.096728452042264, 89.45894347713342, 179.85824578309143, 81.10412477936492, 46.55712603287101, 51.31862466085769, 41.77026043483656, 33.44246917491266, 48.92110010006477, 79.8235084615993, 35.85275461525665, 132.91384690303556, 1.202395959270639, 1.202395959270639, 1.202395959270639, 4.809583837082556, 1.202395959270639, 1.202395959270639, 1.202395959270639, 2.404791918541278, 13.226355551977028, 4.809583837082556, 1.202395959270639, 1.202395959270639, 4.809583837082556, 1.202395959270639, 1.202395959270639, 1.202395959270639, 10.82156363343575, 1.202395959270639, 1.202395959270639, 1.202395959270639, 1.202395959270639, 1.202395959270639, 1.202395959270639, 1.202395959270639, 1.202395959270639, 2.404791918541278, 1.202395959270639, 1.202395959270639, 1.202395959270639, 15.631147470518307, 211.62168883163244, 125.04917976414646, 109.41803229362814, 103.40605249727494, 74.54854947477962, 55.310214126449395, 43.286254533743, 43.286254533743, 43.286254533743, 40.88146261520173, 212.71207678512707, 38.47667069666045, 38.47667069666045, 37.27427473738981, 34.869482818848525, 32.464690900307254, 31.262294941036615, 31.262294941036615, 31.262294941036615, 30.059898981765972, 30.059898981765972, 28.857503022495337, 27.655107063224698, 27.655107063224698, 26.452711103954055, 26.452711103954055, 26.452711103954055, 25.250315144683416, 25.250315144683416, 25.250315144683416, 85.33277710629002, 50.489863331345894, 39.661365793895094, 75.613573827573, 32.45062371714544, 34.864794276398804, 32.49337880594866, 93.75176748210043, 5.825299786726467, 12.815659530798229, 5.825299786726467, 25.631319061596457, 33.78673876301351, 40.77709850708528, 23.30119914690587, 33.78673876301351, 5.825299786726467, 33.78673876301351, 290.09992937897806, 210.87585227949813, 8.155419701417054, 72.2337173554082, 8.155419701417054, 72.2337173554082, 8.155419701417054, 5.825299786726467, 31.456618848322925, 16.31083940283411, 32.62167880566822, 12.815659530798229, 24.466259104251165, 51.262638123192914, 12.815659530798229, 309.9059486538481, 12.815659530798229, 1.1650599573452936, 1.1650599573452936, 1.1650599573452936, 206.21561245011696, 167.76863385772228, 142.13731479612582, 140.97225483878054, 71.06865739806291, 137.5120666974451, 52.427698080538214, 41.94215846443057, 22.13613918956058, 22.13613918956058, 15.145779445488817, 15.145779445488817, 54.73259222126823, 39.649374551665325, 37.292710952721634, 72.3057318037186, 124.9536749188948, 63.09922100397665, 29.17817888837839, 60.68024864100543, 40.92644251478666, 67.890020890176, 174.33691682571316, 20.68457850555285, 41.3691570111057, 47.45285657156242, 30.418497802283603, 147.22552936305263, 1.2167399120913442, 3.6502197362740327, 1.2167399120913442, 1.2167399120913442, 21.901318417644195, 1.2167399120913442, 1.2167399120913442, 1.2167399120913442, 3.6502197362740327, 1.2167399120913442, 14.600878945096131, 6.08369956045672, 1.2167399120913442, 1.2167399120913442, 1.2167399120913442, 1.2167399120913442, 1.2167399120913442, 1.2167399120913442, 1.2167399120913442, 3.6502197362740327, 1.2167399120913442, 6.08369956045672, 1.2167399120913442, 1.2167399120913442, 1.2167399120913442, 310.25433363047205, 108.28985217612963, 182.48587590285996, 93.6889732310335, 283.3993704892778, 66.92069516502393, 59.620255692475865, 59.620255692475865, 57.186775868293175, 53.53655613201915, 53.53655613201915, 38.935677186923016, 37.71893727483167, 34.06871753855764, 29.201757890192262, 26.768278066009575, 26.768278066009575, 25.551538153918226, 24.33479824182688, 23.11805832973554, 21.901318417644195, 21.901318417644195, 21.901318417644195, 19.467838593461508, 167.70346248454612, 88.63844311816685, 27.961823594262217, 88.26867942144577, 25.517861811249777, 43.38919719731999, 35.115430655766275], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.0711, -6.5971, -5.904, -4.1812, -6.5971, -6.5971, -6.5971, -6.5971, -6.5971, -6.5971, -5.3734, -6.5971, -6.5971, -5.904, -6.5971, -5.1621, -5.3734, -6.5971, -6.5971, -6.5971, -6.2607, -6.2607, -6.5971, -5.2621, -4.7101, -5.5675, -6.2607, -5.3734, -4.3999, -4.1122, -2.2126, -2.1475, -3.5152, -3.6115, -3.1761, -4.0957, -4.493, -4.5177, -4.5957, -4.5957, -4.5957, -4.7408, -4.8393, -3.7407, -4.9485, -4.9485, -5.0711, -5.1155, -5.3162, -5.3734, -4.0169, -4.6512, -4.4224, -4.6231, -5.2109, -4.6512, -4.543, -8.1716, -8.1716, -8.1716, -8.1716, -8.1716, -6.3798, -8.1716, -6.7853, -8.1716, -6.0922, -7.4785, -8.1716, -7.073, -8.1716, -8.1716, -7.073, -8.1716, -8.1716, -8.1716, -8.1716, -8.1716, -8.1716, -8.1716, -8.1716, -8.1716, -8.1716, -8.1716, -8.1716, -8.1716, -8.1716, -2.7335, -2.9405, -3.3841, -3.8278, -3.8675, -3.9521, -3.9972, -4.0127, -4.0607, -4.1462, -4.2596, -4.3649, -4.3649, -4.4339, -4.458, -4.508, -4.534, -3.5765, -4.5881, -4.8758, -4.8758, -4.9135, -5.0806, -5.1759, -5.2272, -5.2812, -5.2812, -5.3384, -5.399, -5.4635, -5.4635, -4.6751, -4.508, -4.458, -4.9527, -4.5881, -4.7704, -4.6751, -4.534, -4.7376, -4.2596, -6.2051, -6.2051, -6.2051, -6.2051, -6.2051, -6.2051, -4.8188, -6.2051, -6.2051, -5.2607, -6.2051, -3.9767, -6.2051, -5.2607, -8.151, -7.0524, -8.151, -6.3593, -5.6661, -7.0524, -4.6545, -8.151, -5.7531, -8.151, -8.151, -8.151, -8.151, -7.4579, -8.151, -8.151, -2.2677, -3.0094, -3.7202, -3.9922, -4.2592, -4.3009, -4.3898, -4.4134, -4.4375, -4.4622, -4.4622, -3.6184, -4.6247, -4.6247, -4.7837, -4.7837, -3.6967, -3.6967, -4.8552, -4.973, -5.06, -5.06, -5.1065, -5.1553, -5.1553, -5.1553, -5.2066, -3.3636, -4.3898, -4.7171, -4.4375, -4.973, -4.8552, -4.973, -8.1594, -7.0608, -8.1594, -8.1594, -8.1594, -8.1594, -8.1594, -6.5499, -8.1594, -8.1594, -8.1594, -8.1594, -8.1594, -8.1594, -6.2135, -6.7731, -7.4662, -8.1594, -8.1594, -8.1594, -8.1594, -8.1594, -8.1594, -7.0608, -8.1594, -8.1594, -8.1594, -7.4662, -8.1594, -8.1594, -3.1896, -2.8218, -3.5246, -3.7286, -3.7649, -3.8027, -3.8156, -4.2081, -4.2676, -4.4217, -2.9124, -4.4958, -4.604, -4.633, -4.633, -4.7254, -4.8272, -4.9013, -4.9813, -5.0239, -5.0239, -5.1636, -5.2149, -5.269, -5.3262, -5.3262, -5.4513, -5.4513, -5.5203, -5.5203, -4.1704, -3.8156, -4.1891, -3.7286, -4.5218, -4.9013, -4.2081, -4.7254, -4.1163, -3.7527, -4.9405, -8.1565, -8.1565, -6.7702, -8.1565, -8.1565, -7.4634, -6.7702, -8.1565, -8.1565, -7.4634, -7.4634, -8.1565, -8.1565, -7.4634, -8.1565, -8.1565, -8.1565, -8.1565, -8.1565, -8.1565, -6.7702, -8.1565, -8.1565, -8.1565, -8.1565, -8.1565, -8.1565, -8.1565, -8.1565, -7.4634, -3.2293, -3.3443, -3.6239, -3.8661, -4.0134, -4.2053, -4.3064, -4.4676, -4.573, -4.6301, -4.66, -4.6908, -4.7225, -4.7553, -4.7892, -4.8607, -4.8607, -4.8984, -4.8984, -4.9785, -5.021, -5.021, -5.112, -5.2121, -5.2661, -5.3233, -5.3839, -5.3839, -5.4485, -5.5175, -2.899, -4.0294, -3.4931, -4.6012, -4.7225, -4.3723, -4.3723, -4.8607, -3.8661, -4.5189, -8.1637, -8.1637, -8.1637, -6.3719, -8.1637, -8.1637, -8.1637, -6.7774, -8.1637, -8.1637, -5.1679, -6.5542, -8.1637, -8.1637, -8.1637, -8.1637, -6.7774, -8.1637, -8.1637, -8.1637, -8.1637, -6.5542, -6.0842, -8.1637, -6.7774, -6.7774, -8.1637, -8.1637, -8.1637, -4.7297, -3.3679, -4.0048, -4.0693, -4.0861, -4.2124, -4.2318, -3.5585, -4.6373, -4.6671, -4.7297, -4.7964, -4.7964, -4.8315, -4.8678, -4.9056, -4.9448, -5.0282, -5.0282, -5.0726, -5.1191, -5.1679, -5.1679, -5.1679, -5.3304, -3.9441, -5.3911, -5.3911, -5.4556, -3.721, -3.8329, -3.7093, -4.3135, -4.6671, -4.6671, -3.8732, -4.4748, -4.2516, -4.4501, -4.0528, -4.9856, -4.9448, -4.9056, -6.2089, -3.8507, -6.2089, -6.2089, -4.2035, -5.9576, -5.2644, -5.2644, -6.2089, -6.2089, -4.8226, -5.7569, -5.7569, -6.2089, -5.0193, -6.2089, -4.7875, -5.5157, -5.7569, -6.2089, -3.6009, -5.5157, -6.2089, -3.6439, -5.0193, -4.8967, -3.4363, -4.8967, -4.4912, -5.2103, -2.8221, -3.8373, -4.1475, -4.263, -4.4171, -4.5994, -4.5994, -4.7208, -3.633, -5.0193, -5.1103, -5.1591, -5.3216, -5.3216, -5.3822, -3.8643, -3.1919, -3.9651, -4.5172, -4.4412, -4.6583, -4.859, -4.5439, -4.1475, -4.8226, -4.3706, -8.1464, -8.1464, -8.1464, -6.7601, -8.1464, -8.1464, -8.1464, -7.4533, -5.7485, -6.7601, -8.1464, -8.1464, -6.7601, -8.1464, -8.1464, -8.1464, -5.9492, -8.1464, -8.1464, -8.1464, -8.1464, -8.1464, -8.1464, -8.1464, -8.1464, -7.4533, -8.1464, -8.1464, -8.1464, -5.5815, -2.9759, -3.502, -3.6356, -3.6921, -4.0193, -4.3178, -4.5629, -4.5629, -4.5629, -4.6201, -2.9874, -4.6807, -4.6807, -4.7124, -4.7791, -4.8506, -4.8883, -4.8883, -4.8883, -4.9275, -4.9275, -4.9684, -5.0109, -5.0109, -5.0554, -5.0554, -5.0554, -5.1019, -5.1019, -5.1019, -3.8979, -4.4328, -4.7124, -4.1761, -4.8883, -4.8506, -4.9275, -4.4328, -6.5684, -5.7799, -6.5684, -5.0868, -4.8105, -4.6224, -5.1821, -4.8105, -6.5684, -4.8105, -2.6603, -2.9793, -6.2319, -4.0507, -6.2319, -4.0507, -6.2319, -6.5684, -4.882, -5.5387, -4.8456, -5.7799, -5.1333, -4.3936, -5.7799, -2.5943, -5.7799, -8.1778, -8.1778, -8.1778, -3.0016, -3.208, -3.3738, -3.382, -4.0669, -3.4156, -4.3711, -4.5943, -5.2334, -5.2334, -5.6128, -5.6128, -4.3492, -4.6813, -4.7438, -4.1174, -3.6134, -4.3276, -4.9997, -4.4166, -4.7438, -4.4642, -3.9881, -5.3007, -4.6075, -4.4703, -4.915, -3.3381, -8.1339, -7.0353, -8.1339, -8.1339, -5.2435, -8.1339, -8.1339, -8.1339, -7.0353, -8.1339, -5.649, -6.5244, -8.1339, -8.1339, -8.1339, -8.1339, -8.1339, -8.1339, -8.1339, -7.0353, -8.1339, -6.5244, -8.1339, -8.1339, -8.1339, -2.5965, -3.6452, -3.1299, -3.7901, -2.6958, -4.1265, -4.2421, -4.2421, -4.2837, -4.3497, -4.3497, -4.6681, -4.6999, -4.8017, -4.9558, -5.0428, -5.0428, -5.0894, -5.1381, -5.1894, -5.2435, -5.2435, -5.2435, -5.3613, -3.2435, -4.0067, -5.0428, -4.2219, -5.1381, -4.8017, -5.0428], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2901, 2.2974, 2.2974, 2.2906, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2855, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2974, 2.2816, 2.2677, 2.1449, 2.1337, 2.2476, 1.6926, 1.5512, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.2911, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.3013, 2.2711, 2.2508, 2.2303, 2.2612, 2.1941, 2.2054, 2.1621, 2.0245, 2.0192, 1.3932, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.2957, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.2913, 2.3015, 2.3015, 2.3015, 2.3015, 2.2903, 2.2901, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.3015, 2.2771, 2.2788, 2.2388, 1.9238, 2.184, 1.8701, 2.1157, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.2981, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.2871, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.303, 2.2846, 2.2773, 2.2842, 2.222, 2.2519, 2.2648, 2.1762, 2.1803, 1.9881, 1.7212, 2.1552, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.2673, 2.2715, 2.2302, 2.2751, 2.2714, 2.2172, 2.2155, 2.2346, 1.7867, 1.9875, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.2931, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.3031, 2.2888, 2.3031, 2.3031, 2.3031, 2.2801, 2.2781, 2.2685, 2.2621, 2.2737, 2.2729, 2.2112, 2.1839, 2.1064, 2.1431, 1.6924, 2.2617, 2.0267, 1.4383, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.2922, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.3032, 2.2768, 2.2508, 2.274, 2.277, 2.2556, 2.2444, 2.2661, 2.2008, 2.1076, 2.2329, 1.3746, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.2876, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.3042, 2.2904, 2.2803, 2.2421, 2.1331, 2.2669, 2.2328, 2.2263, 1.6614, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.2956, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.3043, 2.2833, 2.2735, 2.2723, 2.2366, 2.1935, 2.1625, 2.2617, 2.1127, 2.1793, 1.9528, 1.4858, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.301, 2.3048, 2.2983, 2.3048, 2.2922, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.3048, 2.2692, 2.1436, 2.2612, 1.9327, 2.2574, 2.063, 2.0334]}, \"token.table\": {\"Topic\": [8, 6, 7, 10, 4, 2, 9, 9, 4, 6, 7, 9, 1, 8, 3, 8, 4, 8, 10, 9, 4, 10, 8, 2, 1, 2, 6, 2, 3, 10, 6, 3, 3, 2, 6, 10, 1, 6, 1, 5, 7, 5, 4, 1, 5, 2, 8, 9, 7, 4, 10, 5, 8, 1, 3, 9, 6, 6, 5, 8, 9, 8, 6, 8, 5, 3, 6, 1, 7, 3, 5, 3, 8, 4, 3, 9, 5, 2, 6, 5, 6, 5, 6, 8, 9, 1, 1, 8, 1, 3, 2, 4, 9, 8, 2, 7, 1, 3, 6, 10, 3, 5, 7, 10, 2, 2, 5, 1, 8, 8, 2, 7, 8, 6, 9, 7, 7, 9, 7, 6, 2, 10, 4, 6, 1, 7, 8, 7, 8, 3, 8, 8, 7, 7, 9, 2, 6, 9, 1, 5, 6, 3, 4, 6, 4, 5, 7, 10, 2, 2, 5, 4, 10, 1, 6, 6, 3, 5, 3, 4, 2, 7, 5, 1, 4, 6, 7, 4, 7, 4, 8, 7, 1, 9, 5, 4, 7, 2, 7, 5, 6, 6, 5, 6, 8, 2, 1, 2, 6, 7, 6, 10, 9, 10, 4, 5, 7, 10, 5, 1, 3, 4, 2, 4, 5, 4, 4, 7, 2, 2, 3, 3, 7, 8, 1, 7, 8, 3, 10, 7, 5, 3, 9, 3, 4, 4, 1, 8, 9, 8, 8, 8, 10, 10, 1, 4, 2, 4, 1, 5, 2, 6, 9, 8, 9, 9, 10, 9, 5, 6, 6, 9, 8, 3, 10, 1, 7, 6, 10, 2, 6, 2, 3, 5, 5, 6, 5, 5, 5, 1, 10, 9, 4, 9, 1, 2, 10, 3, 3, 5, 3, 4, 6, 9, 2, 4, 5, 3, 9, 1, 3, 4, 5, 5, 4, 6, 2, 4, 2, 1, 2, 9, 2, 9, 4, 6, 4, 5, 6, 8, 8, 2, 1, 5, 4, 8, 7, 2, 9, 7, 8, 4, 2, 4, 8, 5, 3, 6, 4, 2, 8, 7, 9, 4, 7, 6, 3, 8, 4, 8, 4, 6, 3, 6, 1, 9, 5, 9, 8, 4, 6, 1, 3, 6, 3, 6, 6, 2, 10, 10, 9, 3, 8, 10, 5, 2, 4, 6, 1, 1, 10, 1, 2, 3, 7, 9, 4, 6, 9, 9, 1, 9, 8, 8, 5, 3, 5, 1, 7, 7, 7, 9, 1, 8, 2, 2, 8, 10, 3, 7, 10, 6, 1, 6, 1, 9, 2, 4, 6, 3, 10, 1, 8, 8, 2, 5, 6, 2, 8, 10, 6, 9, 10, 2, 2, 9, 4, 2, 10, 3, 8, 6, 6, 8, 8, 10, 4, 1, 2, 5, 8, 9, 10, 8, 4, 2, 1, 8, 8, 4, 4, 10, 4, 3, 8, 3, 9, 3, 8, 2, 3, 3, 4, 5, 1, 3, 3, 6, 6, 1, 9, 5, 8, 9, 7, 7, 6, 4, 8, 4, 5, 10, 6, 3, 8, 3, 4, 5, 8, 7, 9, 6, 8, 6, 1, 1, 8, 2, 10, 2, 6, 5, 8, 5, 6, 7, 10, 1, 9, 2, 7, 4, 6, 8, 9, 6, 1, 1, 2, 7, 3, 6, 9, 4, 8, 10, 4, 8, 6, 10, 7, 2, 4, 5, 8, 1, 6, 7, 5, 1, 10, 10, 5, 6, 8, 10, 7, 6, 7, 1, 10, 5, 7, 8, 4, 5, 7, 10, 8, 5, 2, 6, 9, 4, 5, 7, 10, 3, 10, 6, 7, 4, 4, 6, 10, 5, 3, 4, 9, 2, 5, 1, 4, 8, 2, 6, 10, 10, 1, 1, 9, 10, 10, 6, 3, 2, 7, 4, 4, 3, 8, 8, 8, 8, 4, 5, 2, 1, 3, 6, 5, 4, 5, 8, 3, 3, 10, 7, 2, 5, 10, 7, 1, 4, 4, 3, 3, 5, 10, 2, 1, 4, 5, 2, 10, 4, 1, 8, 4, 8, 4, 7, 7, 5, 6, 5, 10, 1, 4, 4, 5, 8, 5, 4, 5, 3, 10, 3, 7, 6, 2, 3, 7, 3, 2, 10, 10, 2, 6, 10, 5, 8, 7, 5, 5, 5, 6, 5, 5, 2, 3, 2, 5, 9, 5, 4, 7, 8, 2, 2, 8, 7, 5, 2, 4, 1, 3, 5, 9, 4, 6, 9, 2, 9, 1, 9, 3, 4, 3, 6, 10, 8, 2, 1, 1, 1, 2, 7, 10, 1, 2, 4, 5, 6, 8, 9, 5, 10, 9, 9, 3, 9, 3, 5, 5, 7, 4, 10, 5, 8, 6, 5, 10, 4, 4, 7, 9, 10, 6, 3, 5, 8, 9, 7, 3, 10, 5, 9, 4, 7, 2, 9, 7, 8, 3, 3, 2, 2, 2, 7, 5, 9], \"Freq\": [0.9900866526516949, 0.9989473197916819, 0.9204313686390306, 0.0557837193114564, 0.991789016866739, 0.8504469662101801, 0.8583249245631965, 1.0001968129207495, 0.8415179537051118, 0.2798644005535928, 0.014729705292294358, 0.7070258540301292, 1.0527847098221024, 1.0395909852842795, 0.9876117812185251, 1.0028995387448343, 0.030816048674948745, 0.955297508923411, 1.008656587226829, 0.9809427709293674, 1.0098215444461345, 0.8218683303329718, 0.9926417149811184, 1.013994459712138, 1.01229299021356, 0.8504469662101801, 0.8452631167468078, 0.8504469662101801, 0.14666322582707447, 0.8461339951561989, 0.994427196172715, 0.9523399318892923, 0.9848060659309725, 1.020536359452216, 1.0565788959335096, 0.8218683303329718, 0.99833032827958, 1.0565788959335096, 1.0026521045924786, 0.9274182026161992, 0.07360461925525391, 0.8391873636436119, 1.0098215444461343, 0.058048999705451465, 0.9287839952872234, 0.9477525945022894, 0.032681123948354804, 0.9996555346318354, 0.9989628917241546, 0.9900211220060141, 0.8218683303329718, 0.8391873636436119, 0.8316727882274236, 1.0527847098221024, 0.007272089090190217, 0.9890041162658695, 0.9989473197916818, 1.0143157400961695, 0.8391873636436119, 0.8316727882274236, 1.0115972325209102, 1.006055792210593, 0.8452631167468078, 0.9933869414938672, 0.8391873636436119, 0.985123010357859, 0.018944673276112673, 0.986985665458221, 1.0054078136062459, 0.9999569284837568, 0.996534994326789, 0.9341925894176921, 0.050496896725280654, 0.8415179537051118, 1.0184746493816041, 1.0299899094758358, 1.0190132272815287, 1.003806255198901, 0.8452631167468078, 0.979247447383111, 0.023315415413883597, 1.0489842045545148, 0.8452631167468078, 0.025221078801557002, 0.958400994459166, 1.0527847098221024, 0.46932448509198194, 0.5226568129433435, 1.0067613345293331, 0.8332974404031307, 1.009905772374589, 0.5563939168258444, 0.4416735216040208, 1.0395909852842795, 0.9921881272452102, 1.0024155284467036, 0.9650526506702606, 0.02485018477222727, 0.9691572061168635, 1.00636938408119, 0.8332974404031307, 0.14238754606243037, 0.1139100368499443, 0.768892748737124, 1.0630587077627252, 0.08751190247517264, 0.9101237857417955, 0.9733167079241816, 0.024332917698104542, 1.00493795244147, 1.0079371451379913, 0.05042690688952148, 0.9328977774561473, 0.9509210063401587, 0.9809427709293674, 0.9835511220061101, 1.0054078136062459, 1.014384001756505, 1.0000023952535242, 0.8452631167468078, 0.8504469662101801, 1.008656587226829, 1.0518974421313898, 1.0037499511368342, 0.044479473071520885, 0.9507487369037589, 0.005559934133940111, 1.0111860194315692, 0.8316727882274236, 1.0057038073830888, 0.8316727882274236, 0.9916098628865435, 0.9575312510535675, 0.9740565010091784, 0.024659658253396922, 0.06943286528690114, 0.8193078103854334, 0.11109258445904183, 1.0026521045924786, 0.8391873636436119, 0.8452631167468078, 0.9523399318892923, 0.00836857299542679, 0.9874916134603613, 0.0408821550600689, 0.0408821550600689, 0.8994074113215158, 0.8218683303329718, 0.8504469662101801, 1.0079371451379913, 1.0005695489596909, 0.8415179537051118, 1.008656587226829, 0.9805347787558798, 0.9861403028712757, 0.8452631167468078, 0.8332974404031307, 0.9851329921033704, 0.9523399318892923, 1.018679628169346, 1.000525842600212, 1.0035664073542199, 0.8391873636436119, 0.9805347787558798, 1.001807087744181, 0.8452631167468078, 0.9963500855557392, 0.8415179537051118, 0.9974283865141328, 0.9239036982743942, 0.07391229586195154, 1.0054078136062456, 1.0527847098221024, 0.987073663247676, 0.9872792513454255, 1.0098215444461343, 0.9989628917241546, 0.9863700599891734, 0.00850319017232046, 0.8391873636436119, 0.8452631167468078, 1.0017933235517722, 0.9998828162562182, 1.0143157400961695, 0.9828860224505916, 0.8504469662101801, 1.0527847098221024, 0.060189364662932586, 0.5417042819663932, 0.39875454089192836, 0.014114357392869849, 0.9880050175008894, 1.014384001756505, 0.8218683303329718, 0.8415179537051118, 0.03897220576773294, 0.9548190413094572, 0.8218683303329718, 1.0489842045545148, 1.0527847098221024, 0.9523399318892923, 1.0218432294990645, 0.9752083568813388, 0.02500534248413689, 1.0016107243488268, 0.8415179537051118, 0.7327649665960696, 0.26939888477796675, 0.8504469662101801, 0.17300787685670346, 0.8362047381407335, 0.8953179570958428, 0.06174606600660985, 0.030873033003304923, 0.06263818887897354, 0.8268240932024508, 0.1252763777579471, 1.0033581425262184, 0.9979829725471799, 0.9575312510535675, 0.8391873636436119, 0.9523399318892923, 1.000588724214555, 0.9999569284837566, 0.9967494208934337, 0.8415179537051118, 1.0527847098221024, 0.122170403601376, 0.8796269059299072, 0.9980073458729084, 0.9828860224505916, 0.003223162069320355, 0.9959570794199896, 1.0011850569510747, 1.0527847098221024, 0.9973546117986511, 1.0023124958905694, 0.9900211220060141, 0.9942966703875413, 0.8391873636436119, 0.9014866012985687, 0.10302704014840786, 0.9918421350508048, 0.8316727882274236, 1.014384001756505, 0.9596212329465271, 0.034272186890947395, 1.0063119805223684, 0.9705283942187818, 0.026230497141048158, 0.9700006711446628, 0.02487181208063238, 0.8316727882274236, 0.8332974404031307, 0.9862419963995662, 1.0158448954423795, 0.9965884468202262, 1.0037499511368342, 0.8218683303329718, 0.9921881272452101, 1.0143157400961695, 1.000525842600212, 0.8332974404031307, 1.0140180644026975, 1.0001274059862222, 0.994427196172715, 0.8391873636436119, 0.8391873636436119, 0.9945924309850213, 1.0057089707650166, 1.0045057370736323, 0.9809427709293674, 0.8415179537051118, 1.0063119805223684, 1.0026521045924786, 0.9956452287338694, 1.0958244404439623, 0.9999569284837566, 0.9848060659309725, 0.8391873636436119, 0.9523399318892923, 0.8415179537051118, 1.0037499511368342, 1.0299899094758358, 1.000525842600212, 1.001807087744181, 1.0005695489596909, 0.8332974404031307, 1.0013790786570624, 0.008869601158937623, 0.9933953298010137, 0.9817204564933212, 0.015582864388782876, 0.9917668843060865, 1.0168341940603436, 0.8452631167468078, 1.0050736873393038, 0.8415179537051118, 0.9847280661381033, 1.0102479538696942, 0.9921881272452102, 1.0003034835134996, 0.026814891555289832, 0.965336095990434, 0.0844934084679685, 0.9083041410306614, 0.8415179537051118, 0.037514310309236056, 0.8815862922670472, 0.07502862061847211, 0.8316727882274236, 0.8504469662101801, 1.0013861297129425, 1.0016107243488268, 0.9852231551876774, 0.017437577967923494, 1.0097044281943068, 0.8504469662101801, 0.9903749129575343, 1.0173769542444155, 1.0395909852842795, 0.8415179537051118, 0.9536664023277817, 0.041463756622947025, 0.8316727882274236, 1.0489842045545148, 1.001940970008526, 0.9922653979201657, 0.8415179537051118, 0.9567528369864526, 1.0124712204507766, 1.0054078136062459, 1.0299899094758358, 0.9925596377034653, 0.9835511220061101, 1.0028545452928228, 0.9523399318892923, 0.8316727882274236, 0.8415179537051118, 1.0235972778183675, 0.9900211220060141, 1.0143157400961695, 0.9867080960347724, 0.00957969025276478, 0.959863370542252, 0.04173319002357617, 0.12678444951793974, 0.8716430904358357, 0.8316727882274236, 0.9878689021755662, 1.0088624296655446, 1.0527847098221024, 0.033791115823252685, 0.9461512430510752, 1.0069010738204494, 0.8452631167468078, 0.9962029590230236, 0.9966175385275549, 1.0958244404439623, 0.8218683303329718, 0.9809427709293674, 0.9958920629208146, 0.9876114360200655, 0.9862419963995663, 1.0070248363723342, 0.7635945104118488, 0.20362520277649301, 0.03393753379608217, 1.0527847098221024, 0.8482559121242323, 0.14482418011877138, 0.9958774282100971, 0.8504469662101801, 0.9523399318892923, 0.9725131621103714, 0.022356624416330375, 1.0098215444461343, 0.8452631167468078, 1.0013790786570624, 0.9948766171073414, 1.0527847098221024, 0.9967644285250024, 0.8316727882274236, 0.8316727882274236, 0.8391873636436119, 0.27382862457509105, 0.7248404768164175, 0.9958774282100971, 0.9901743618849391, 0.9949348155478475, 0.9974283865141328, 0.9938499126521221, 1.0527847098221024, 0.8316727882274236, 0.8504469662101801, 0.999849811625482, 0.9828860224505916, 0.8218683303329718, 1.0048586781331867, 0.9575312510535675, 0.8218683303329718, 0.8452631167468078, 0.02169373872871034, 0.9762182427919653, 0.991753712151256, 1.014384001756505, 0.20514985666088914, 0.025643732082611143, 0.7693119624783343, 1.0087284804880001, 0.9967339325314766, 0.9909051647778043, 0.0056948572688379565, 0.9996067166194995, 0.40471519032066405, 0.5967834162355554, 0.8452631167468078, 1.003091293478674, 0.9916098628865435, 1.008656587226829, 1.006265615174771, 0.2074249025413151, 0.7836051873783014, 0.9992751852969615, 1.0205363594522159, 0.9809427709293674, 0.8415179537051118, 0.8504469662101801, 0.8218683303329718, 1.0184746493816041, 0.8316727882274236, 0.8452631167468078, 0.8608213793754218, 0.14054226602047704, 0.9828860224505916, 0.8218683303329718, 0.8415179537051118, 0.9650526506702606, 0.8504469662101801, 0.8391873636436119, 1.003743020274477, 0.99903392858995, 0.8218683303329718, 0.9856862675287983, 0.8415179537051118, 0.8504469662101801, 0.9980527830117792, 0.8316727882274236, 0.8316727882274236, 1.1220239382734827, 0.8415179537051118, 0.8218683303329718, 1.0033483294176333, 1.0025609829850166, 0.9933869414938672, 0.9868783019754142, 0.00958134273762538, 0.05736445722709656, 0.9178313156335449, 1.0050736873393038, 0.9721803471369856, 1.0118611776323732, 0.9791714487899155, 0.01529955388734243, 0.9813015246058168, 0.013084020328077556, 1.0004211610410843, 1.0565788959335096, 0.9909981368755676, 1.0527847098221024, 0.9903749129575343, 0.9945924309850213, 0.9843814164792747, 0.011718826386658032, 0.9856939349080841, 0.9575312510535675, 1.0143157400961695, 1.0043923963577144, 0.9980073458729084, 0.9799266941591613, 0.021302754220851334, 0.9910765159897602, 0.8452631167468078, 0.9928650353739428, 1.0164889633890735, 0.9721803471369856, 1.0098215444461345, 0.8391873636436119, 1.0124712204507766, 0.9835511220061101, 0.8583249245631965, 0.9922653979201657, 0.8316727882274236, 0.9944271961727151, 0.9805347787558798, 1.01229299021356, 0.9933869414938672, 1.13392928828024, 1.01524911394073, 1.0205363594522159, 1.0088624296655446, 0.01980595577051532, 0.9704918327552506, 0.996534994326789, 0.8452631167468078, 0.9568671449656371, 0.02990209828017616, 0.9958774282100971, 0.9938499126521221, 1.007108249459424, 0.9575312510535675, 0.8710681588872067, 0.14517802648120112, 0.8316727882274236, 0.99903392858995, 0.9909981368755676, 1.0026521045924786, 0.8558850997457351, 0.10270621196948822, 0.034235403989829404, 0.8332974404031307, 0.9802313955731298, 0.012252892444664123, 1.1220239382734827, 0.9232650189800456, 0.06155100126533637, 0.8884027827077396, 0.1200544300956405, 1.0078137161211937, 0.9984681368508006, 1.0054078136062459, 0.021042011003357234, 0.9468904951510755, 0.021042011003357234, 0.8316727882274236, 0.5476380759431629, 0.4244195088559513, 0.027381903797158148, 0.8391873636436119, 1.0235406901048218, 0.9948932419820186, 0.8218683303329718, 0.011329046798416698, 0.14727760837941706, 0.14727760837941706, 0.6910718547034186, 1.0173769542444155, 1.0143157400961693, 0.9575312510535675, 0.991753712151256, 1.0045057370736323, 0.8391873636436119, 0.9774798187838503, 0.9943913772284413, 0.9817709459892974, 1.007024836372334, 0.9907863310304418, 0.009007148463913107, 0.8316727882274236, 0.9990325757662044, 0.8504469662101801, 0.9666153884973775, 0.03452197816062062, 0.9900211220060141, 1.0005695489596909, 0.03576304659203989, 0.965602257985077, 0.8332974404031307, 1.0175512661265367, 0.9684392893656381, 0.03797801134767208, 1.0054500226087053, 0.8415179537051118, 1.0143157400961693, 1.0016520275933094, 0.996534994326789, 0.6826379239982419, 0.3204218826930523, 1.0063119805223684, 0.8504469662101801, 0.8391873636436119, 0.9969552176345666, 0.965763853995567, 0.031153672709534418, 1.0035274201280124, 0.03918823635760662, 0.9405176725825589, 0.9973233671456289, 1.0026521045924786, 1.0527847098221024, 1.0299899094758358, 0.8218683303329718, 0.8218683303329718, 1.0565788959335096, 0.9920207623846792, 1.13392928828024, 0.991728795734052, 0.8415179537051118, 1.0033483294176335, 1.0048586781331867, 0.8316727882274236, 0.8316727882274236, 0.9960732231095888, 0.8316727882274236, 0.03373946231630109, 0.9657921088041187, 0.9921881272452102, 0.00971814566383778, 0.00971814566383778, 0.9815327120476158, 0.9972951278083501, 0.14547652548580867, 0.013225138680528062, 0.846408875553796, 1.0057038073830888, 1.00770853258053, 1.0074515016984817, 0.9874541026489915, 0.8912975839867777, 0.08488548418921693, 0.021221371047304233, 0.9575312510535675, 1.0089186802461816, 0.996082475814214, 0.8415179537051118, 0.9920207623846792, 0.9999569284837566, 0.8391873636436119, 0.993090899152341, 0.8504469662101801, 0.9865087187070873, 0.009964734532394822, 1.0070248363723342, 0.8504469662101801, 0.9904567057858892, 1.0033483294176333, 0.9805347787558798, 0.9900866526516949, 0.9944648913943142, 0.004026173649369693, 0.8415179537051118, 0.9997412639321366, 1.0240264768211764, 0.9851329921033704, 0.9989473197916819, 1.011961232629061, 0.8218683303329718, 0.9930526669973684, 0.008139775958994823, 0.8838800285288642, 0.09979290644680724, 0.014256129492401034, 1.0023626843520916, 0.026226874973862925, 0.9703943740329283, 0.9999569284837566, 1.0033197798870046, 0.9848060659309725, 0.9575312510535675, 1.0078137161211937, 0.8504469662101801, 0.8332974404031307, 0.9901743618849391, 0.9999569284837566, 0.8504469662101801, 1.0958244404439623, 1.0045057370736323, 0.9273725486761328, 0.07728104572301107, 0.9862419963995663, 0.9990325757662044, 0.8316727882274236, 0.9774798187838503, 0.8391873636436119, 0.8391873636436119, 0.8391873636436119, 1.0002280214837225, 0.8391873636436119, 0.8391873636436119, 1.0016375379808788, 1.1110632538708407, 0.8504469662101801, 1.0128123354319452, 0.8583249245631965, 1.0158583875685825, 0.8415179537051118, 0.9901743618849391, 0.9961794935910898, 0.9943687604919029, 0.8504469662101801, 0.9900866526516949, 1.0142271803922656, 0.8391873636436119, 0.8504469662101801, 1.0218432294990645, 1.0026521045924786, 0.9759104599934826, 0.013554311944353926, 0.006777155972176963, 0.8415179537051118, 0.06915081108055193, 0.9404510306955062, 0.18127809701436545, 0.8239913500652976, 0.01827064934102309, 0.986615064415247, 1.1110632538708407, 0.9878689021755662, 0.9523399318892923, 0.8452631167468078, 1.0045057370736323, 1.0017876767284877, 1.0016375379808788, 0.9942966703875412, 0.9998391778651305, 0.020689727459523983, 0.7448301885428634, 0.22758700205476382, 1.0273354129162147, 0.06726565208455802, 0.8744534770992544, 0.04484376805637202, 0.9790519242508802, 0.8452631167468078, 0.9876114360200655, 1.014384001756505, 0.005479876155085643, 0.9918575840705014, 1.0054663402026016, 0.985484172646633, 0.6415744887559106, 0.3408364471515775, 0.9932148038569182, 0.004844950262716674, 0.047880955952383585, 0.9336786410714799, 0.9993025700248205, 1.00636938408119, 0.8391873636436119, 0.9916098628865435, 1.0143157400961695, 0.8391873636436119, 0.9862419963995663, 0.8415179537051118, 0.005962906103337908, 0.005962906103337908, 0.017888718310013724, 0.9659907887407412, 0.8452631167468078, 0.08736389150163755, 0.9085844716170305, 0.9825488197885592, 0.014103571575912334, 0.9856939349080841, 1.010057503518946, 0.975968642270404, 0.10403855675663855, 0.8963321812879629, 0.9617348042344136, 0.9575312510535675, 0.9983507864206461, 0.9989544319775054, 0.9665545069991729, 0.021478989044426064, 0.9523399318892923, 0.8332974404031307, 0.8504469662101801, 0.8504469662101801, 0.9921881272452102, 0.9989628917241546, 1.0029312394765117, 0.9967644285250024], \"Term\": [\"absolutely\", \"accord\", \"add\", \"add\", \"additional\", \"additionally\", \"admi\", \"administer\", \"administering\", \"administration\", \"administration\", \"administration\", \"advantage\", \"adverse\", \"age\", \"ago\", \"agree\", \"agree\", \"ahead\", \"air\", \"allocate\", \"allocating\", \"allocation\", \"amazing\", \"american\", \"amino\", \"anaphyllactic\", \"angle\", \"announce\", \"announce\", \"answer\", \"anti\", \"antibody\", \"anticipate\", \"anticipation\", \"anytime\", \"apparently\", \"application\", \"appointment\", \"approval\", \"approval\", \"approving\", \"approximately\", \"area\", \"area\", \"arm\", \"arm\", \"arrive\", \"article\", \"assume\", \"atm\", \"attorney\", \"aur\", \"authority\", \"authorization\", \"authorization\", \"authorize\", \"authorized\", \"availble\", \"aviation\", \"azar\", \"back\", \"background\", \"bad\", \"ball\", \"base\", \"base\", \"bet\", \"biden\", \"big\", \"bill\", \"biontech\", \"biontech\", \"blanket\", \"body\", \"bolster\", \"boost\", \"break\", \"brighten\", \"bring\", \"bring\", \"burn\", \"busi\", \"business\", \"business\", \"butler\", \"buy\", \"buy\", \"call\", \"calvary\", \"candidate\", \"care\", \"care\", \"careful\", \"carry\", \"case\", \"cash\", \"cell\", \"cell\", \"center\", \"challen\", \"chance\", \"chance\", \"chance\", \"cheap\", \"check\", \"check\", \"chief\", \"chief\", \"child\", \"choice\", \"choose\", \"choose\", \"citizen\", \"city\", \"claim\", \"clear\", \"clinic\", \"close\", \"club\", \"coivd\", \"cold\", \"college\", \"comment\", \"company\", \"company\", \"company\", \"compare\", \"concede\", \"concern\", \"confid\", \"confirm\", \"connection\", \"continue\", \"continue\", \"coronavirus\", \"coronavirus\", \"coronavirus\", \"corrupt\", \"council\", \"counseling\", \"count\", \"country\", \"country\", \"county\", \"county\", \"county\", \"cova\", \"cove\", \"cover\", \"create\", \"creature\", \"credit\", \"crony\", \"cross\", \"cryogenic\", \"culture\", \"curious\", \"damn\", \"date\", \"datum\", \"day\", \"daybreak\", \"dead\", \"deal\", \"dealer\", \"death\", \"decembe\", \"decide\", \"deliver\", \"deliver\", \"delivery\", \"deny\", \"department\", \"depend\", \"desantis\", \"design\", \"develop\", \"develop\", \"devise\", \"devotion\", \"die\", \"difference\", \"discuss\", \"disease\", \"disperse\", \"disrupt\", \"distribute\", \"distribute\", \"distribute\", \"distribution\", \"distribution\", \"district\", \"diving\", \"divvy\", \"doctor\", \"doctor\", \"donaldo\", \"donor\", \"dosage\", \"doubt\", \"drop\", \"drug\", \"drug\", \"due\", \"dufu\", \"early\", \"early\", \"eas\", \"easy\", \"easy\", \"effect\", \"effect\", \"effect\", \"effective\", \"effective\", \"effective\", \"efficacy\", \"effort\", \"election\", \"elpaso\", \"email\", \"emergency\", \"employee\", \"end\", \"engage\", \"enjoy\", \"eua\", \"eua\", \"evidence\", \"excited\", \"expect\", \"expect\", \"expert\", \"facial\", \"facility\", \"fact\", \"fake\", \"family\", \"fantasy\", \"fast\", \"fast\", \"fauci\", \"fda\", \"fed\", \"federal\", \"federal\", \"fedex\", \"feel\", \"feel\", \"fight\", \"fight\", \"fighting\", \"filled\", \"final\", \"financial\", \"find\", \"fine\", \"flash\", \"flow\", \"flu\", \"focus\", \"foe\", \"folk\", \"follow\", \"forget\", \"formally\", \"formula\", \"freezer\", \"friend\", \"front\", \"front_line\", \"frontli\", \"frontline\", \"fuck\", \"full\", \"fun\", \"fund\", \"funding\", \"garner\", \"german\", \"giri\", \"give\", \"goal\", \"good\", \"government\", \"governor\", \"grad\", \"grant\", \"great\", \"great\", \"group\", \"group\", \"guess\", \"guy\", \"gymnastic\", \"happen\", \"happiness\", \"happy\", \"hard\", \"head\", \"health\", \"healthcare\", \"healthcare\", \"hear\", \"hear\", \"hhs\", \"high\", \"high\", \"high\", \"highest\", \"hoboken\", \"hold\", \"hope\", \"hospital\", \"hospital\", \"hour\", \"hudson\", \"human_service\", \"husband\", \"illness\", \"immun\", \"important\", \"important\", \"incentive\", \"incite\", \"include\", \"increase\", \"indicative\", \"industry\", \"infection\", \"info\", \"inform\", \"information\", \"ingredient\", \"initial\", \"initially\", \"instance\", \"instrumental\", \"interest\", \"interesting\", \"invent\", \"invest\", \"invest\", \"investment\", \"investment\", \"issue\", \"issue\", \"jab\", \"job\", \"join\", \"keeping\", \"key\", \"key\", \"kill\", \"kizzy\", \"large\", \"late\", \"laugh\", \"laughter\", \"lawmaker\", \"lead\", \"leader\", \"lean\", \"learn\", \"leave\", \"leave\", \"leave\", \"liar\", \"lie\", \"lie\", \"life\", \"lije\", \"limit\", \"line\", \"line\", \"link\", \"literary\", \"live\", \"local\", \"lock\", \"long\", \"loose\", \"loot\", \"loser\", \"lot\", \"lot\", \"make\", \"maker\", \"man\", \"market\", \"mask\", \"material\", \"meaningful\", \"mechanism\", \"medical\", \"meeting\", \"megaphone\", \"member\", \"message\", \"metabolic\", \"mhealth\", \"million\", \"million\", \"mind\", \"miss\", \"mode\", \"mode\", \"mode\", \"moder\", \"modern\", \"money\", \"money\", \"month\", \"morning\", \"morning\", \"morph\", \"move\", \"mutate\", \"mutation\", \"nation\", \"newly\", \"newly\", \"news\", \"nice\", \"ning\", \"nitpick\", \"noise\", \"nope\", \"normal\", \"nose\", \"nuce\", \"number\", \"number\", \"nurse\", \"observatio\", \"obtainable\", \"obvious\", \"ocugen\", \"offend\", \"offer\", \"official\", \"optic\", \"option\", \"optionsflow\", \"oravec\", \"order\", \"osbert\", \"overseas\", \"owner\", \"oxycontin\", \"ozark\", \"package\", \"pandemic\", \"panel\", \"part\", \"part\", \"participant\", \"participant\", \"partner\", \"party\", \"pass\", \"patient\", \"patient\", \"pay\", \"pay\", \"people\", \"perform\", \"person\", \"pfeizer\", \"pfize\", \"pharmacy\", \"phase\", \"phase\", \"phizer\", \"pick\", \"piss\", \"place\", \"placebo\", \"plan\", \"plan\", \"play\", \"plummet\", \"point\", \"poison\", \"political\", \"politician\", \"ponder\", \"population\", \"positive\", \"possibility\", \"post\", \"pottawattamie\", \"prepare\", \"press\", \"pretty\", \"prevent\", \"pride\", \"priority\", \"private\", \"process\", \"produce\", \"produce\", \"product\", \"producti\", \"production\", \"production\", \"profit\", \"program\", \"protect\", \"protective\", \"provider\", \"provider\", \"psp\", \"public\", \"purchase\", \"push\", \"put\", \"put\", \"put\", \"pziser\", \"question\", \"question\", \"quit\", \"rate\", \"rate\", \"reach\", \"reach\", \"reaction\", \"read\", \"ready\", \"real\", \"real\", \"real\", \"reap\", \"reason\", \"reason\", \"reason\", \"reassur\", \"recent\", \"recently\", \"recite\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"record\", \"reduce\", \"refrigeration\", \"refuse\", \"regard\", \"reinforce\", \"relate\", \"release\", \"relief\", \"remember\", \"report\", \"report\", \"repre\", \"require\", \"required\", \"research\", \"research\", \"resident\", \"rest\", \"review\", \"review\", \"rid\", \"role\", \"roll\", \"roll\", \"rollout\", \"romancescam\", \"round\", \"rural\", \"rush\", \"safe\", \"safe\", \"safety\", \"satan\", \"satire\", \"save\", \"schedule\", \"schedule\", \"science\", \"scientific\", \"scientific\", \"scientist\", \"screw\", \"search\", \"secretary\", \"secur\", \"seemingly\", \"seize\", \"sell\", \"semi\", \"send\", \"serv\", \"set\", \"severe\", \"sex\", \"shar\", \"share\", \"shelter\", \"ship\", \"ship\", \"shipping\", \"shoot\", \"shoot\", \"shoot\", \"shot\", \"show\", \"show\", \"show\", \"side\", \"side_effect\", \"sign\", \"sit\", \"site\", \"site\", \"site\", \"situation\", \"slow\", \"small\", \"solidify\", \"sound\", \"source\", \"sovereignty\", \"speak\", \"speculator\", \"speed\", \"speed\", \"spend\", \"spot\", \"spread\", \"staff\", \"stake\", \"stand\", \"start\", \"start\", \"start_arrive\", \"state\", \"statement\", \"stay\", \"step\", \"stimulus\", \"sto\", \"stock\", \"stock\", \"stop\", \"stop\", \"stop\", \"storage\", \"store\", \"store\", \"story\", \"strain\", \"strike\", \"student\", \"study\", \"substantially\", \"sucker\", \"sue\", \"suggest\", \"suicidal\", \"summer\", \"super\", \"supply\", \"supply\", \"support\", \"suppose\", \"surg\", \"surge\", \"surpise\", \"swab\", \"synthe\", \"talk\", \"tariff\", \"tcot\", \"team\", \"tear\", \"teared\", \"technology\", \"tempera\", \"temperature\", \"ter\", \"term\", \"test\", \"thing\", \"thos\", \"thought\", \"thousand\", \"threaten\", \"throttle\", \"thumb\", \"tie\", \"time\", \"time\", \"time\", \"timescale\", \"tomorrow\", \"tomorrow\", \"top\", \"top\", \"total\", \"total\", \"tout\", \"track\", \"trading\", \"traffiker\", \"travel\", \"trial\", \"truck\", \"true\", \"trump\", \"trust\", \"trust\", \"trust\", \"truth\", \"turn\", \"turn\", \"turn\", \"type\", \"uam\", \"understand\", \"understanding\", \"update\", \"update\", \"vac\", \"vacci\", \"vaccin\", \"vaccin\", \"vaccinate\", \"vaccinate\", \"vaccinated\", \"vaccinated\", \"vaccination\", \"variant\", \"variant_cnn\", \"version\", \"vial\", \"vince\", \"viral\", \"virtually\", \"virus\", \"virus\", \"virus\", \"virus\", \"visa\", \"vote\", \"vote\", \"wait\", \"wait\", \"waiting\", \"warehouse\", \"warn\", \"watch\", \"watch\", \"widely\", \"wit\", \"work\", \"worker\", \"world\", \"world\", \"worried\", \"worst\", \"woul\", \"wow\", \"write\", \"wrong\", \"year\", \"yesterday\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 9, 7, 4, 2, 10, 1, 8, 5, 6]};\n\nfunction LDAvis_load_lib(url, callback){\n  var s = document.createElement('script');\n  s.src = url;\n  s.async = true;\n  s.onreadystatechange = s.onload = callback;\n  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n  document.getElementsByTagName(\"head\")[0].appendChild(s);\n}\n\nif(typeof(LDAvis) !== \"undefined\"){\n   // already loaded: just create the visualization\n   !function(LDAvis){\n       new LDAvis(\"#\" + \"ldavis_el482821131401924645757976357\", ldavis_el482821131401924645757976357_data);\n   }(LDAvis);\n}else if(typeof define === \"function\" && define.amd){\n   // require.js is available: use it to load d3/LDAvis\n   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n   require([\"d3\"], function(d3){\n      window.d3 = d3;\n      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n        new LDAvis(\"#\" + \"ldavis_el482821131401924645757976357\", ldavis_el482821131401924645757976357_data);\n      });\n    });\n}else{\n    // require.js not available: dynamically load d3 & LDAvis\n    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n                 new LDAvis(\"#\" + \"ldavis_el482821131401924645757976357\", ldavis_el482821131401924645757976357_data);\n            })\n         });\n}\n</script>"
     },
     "metadata": {},
     "execution_count": 260
    }
   ],
   "source": [
    "ldamal = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)\n",
    "###undo ### model_10 = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(optimal_model) \n",
    "#model_optimal = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(optimal_model) # currently 10\n",
    "\n",
    "#from tethne.networks import topics\n",
    "#from tethne import gensim_to_theta_featureset\n",
    "#theta = gensim_to_theta_featureset(lda_model_lemmas, corpus_lemmas)\n",
    "#from tethne import feature_cooccurrence\n",
    "#graph = feature_cooccurrence(theta, min_weight=0.05)\n",
    "#termGraph = topics.terms(ldamallet, threshold=0.01)\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(ldamal, corpus_lemmas, id2word_lemmas)\n",
    "vis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Can only append a Series if ignore_index=True or if the Series has a name",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-281-07eec6d72918>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# call function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mdf_topic_sents_keywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_topics_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_lemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtweets_lemmas\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# maybe change texts=tweets_stemmed to the sentence instead of the tokens?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-281-07eec6d72918>\u001b[0m in \u001b[0;36mformat_topics_sentences\u001b[0;34m(ldamodel, corpus, texts)\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mwp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mldamodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mtopic_keywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0msent_topics_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_topics_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop_topic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_keywords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/nlp/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   7708\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7709\u001b[0m                 raise TypeError(\n\u001b[0;32m-> 7710\u001b[0;31m                     \u001b[0;34m\"Can only append a Series if ignore_index=True \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7711\u001b[0m                     \u001b[0;34m\"or if the Series has a name\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7712\u001b[0m                 )\n",
      "\u001b[0;31mTypeError\u001b[0m: Can only append a Series if ignore_index=True or if the Series has a name"
     ]
    }
   ],
   "source": [
    "# Finding the dominant topic in each sentence\n",
    "\n",
    "# Topic modeling can help us in discovering what each document is about (what topic).\n",
    "# In order to so, we locate the highest precentage-wise contributing topic for each document \n",
    "\n",
    " \n",
    "# function that finds dominant topic and returns overview of topic contribution in a table:\n",
    "\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "# call function\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus_lemmas, texts=tweets_lemmas) # maybe change texts=tweets_stemmed to the sentence instead of the tokens? \n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8406"
      ]
     },
     "metadata": {},
     "execution_count": 280
    }
   ],
   "source": [
    "len(tweets_lemmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dominant topic df\n",
    "# save to csv file:\n",
    "df_dominant_topic.to_csv('df_dominant_topic_10_topics.csv', header=True, mode='a')\n",
    "# save to pickle \n",
    "df_dominant_topic.to_pickle('df_dominant_topic_10_topics.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1163\n"
     ]
    }
   ],
   "source": [
    "liste = []\n",
    "for i in df_dominant_topic.Dominant_Topic:\n",
    "    if i == 0:\n",
    "        liste.append(i)\n",
    "print(len(liste))\n",
    "\n",
    "# 0 = 1163\n",
    "# 1 = 1041\n",
    "# 2 = 943\n",
    "# 3 = 996\n",
    "# 4 = 771\n",
    "# 5 = 772\n",
    "# 6 = 742\n",
    "# 7 = 676\n",
    "# 8 = 671\n",
    "# 9 = 631\n",
    "\n",
    "# IN TOTAL 8406"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}