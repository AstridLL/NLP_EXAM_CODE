{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bitnlpcondaed063720e340471382fe7eca751f187d",
   "display_name": "Python 3.7.9 64-bit ('nlp': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysing tweets with VADER - a rule- and lexicon based tool for sentiment analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "#import nltk\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "#tweets_df = pd.read_pickle('final_data_version_minustop10.pkl')\n",
    "df_dominant_topic = pd.read_pickle('df_dominant_topic_10_topics.pkl')\n",
    "\n",
    "# LINE BELOW ADDED AFTER SYNOPSIS HAND IN\n",
    "# import new updated df with less preprocessed column\n",
    "tweets_df = pd.read_pickle('oralexamupdated_final_data_version_minustop10.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data \n",
    "# print(tweets_df.shape)\n",
    "#print(df_dominant_topic.shape)\n",
    "#(8406, 16)\n",
    "#(9378, 5)\n",
    "\n",
    "# Remove last rows (they are just there bc there is mismatch with the added textcolumn, the rest is in sync and goes from rows[0-8405])\n",
    "df_dominant_topic = df_dominant_topic.drop(df_dominant_topic.index[8406:])\n",
    "\n",
    "tweets_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# concatenate dataframes \n",
    "df = pd.concat([tweets_df,df_dominant_topic],axis=1)\n",
    "# EVT drop last text xcolumn - very confusing! \n",
    "# drop \"Text\" column, not \"text\" - which is the original tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   text             location  \\\n",
       "0     The Future of Vaccines\\nhttps://t.co/GI23fycj1...                Earth   \n",
       "1     The immunization task force can likely deliver...  Mississauga, Canada   \n",
       "2     Question if city staff will remain out working...              Toronto   \n",
       "3     @iamIqraKhalid @GovCanHealth @moderna_tx Enoug...      Ontario, Canada   \n",
       "4     .@USForcesKorea will start administering the @...       Washington, DC   \n",
       "...                                                 ...                  ...   \n",
       "8401  @Lya1224 @SenateDems @PattyMurray Wrong\\nThe F...     Philadelphia, PA   \n",
       "8402  A second COVID-19 vaccine has been authorized ...          Cullman, AL   \n",
       "8403  @no_silenced They already have...\\nExhibit A: ...                   NC   \n",
       "8404  F.D.A. Authorizes Moderna #Coronavirus Vaccine...             Montréal   \n",
       "8405  @atrupar He's throttling back Pfizer's vaccine...         Homewood, AL   \n",
       "\n",
       "      statuses_count  followers_count  friends_count  listed_count  \\\n",
       "0              11485              774           3240             2   \n",
       "1                280              225            675             3   \n",
       "2             251890            19164           9952           607   \n",
       "3              41897             1317            883             5   \n",
       "4              33658            16782           1181           529   \n",
       "...              ...              ...            ...           ...   \n",
       "8401           11432              125            207             0   \n",
       "8402            5806               25            145             0   \n",
       "8403          175485            15101           7543            20   \n",
       "8404          141794           117020           1847          2908   \n",
       "8405            4394              287           1258            17   \n",
       "\n",
       "      favourites_count                                          tidy_text  \\\n",
       "0                 1107  the future never before have billions people b...   \n",
       "1                   26  the immunization task force can likely deliver...   \n",
       "2                30736  question city staff will remain out working wi...   \n",
       "3               156959  enough for people big whoppy population canada...   \n",
       "4                  558  will start administering the its members this ...   \n",
       "...                ...                                                ...   \n",
       "8401             12899  wrong the feds gave billion for advance purcha...   \n",
       "8402                 3  second has been authorized for emergency use c...   \n",
       "8403            136847  they already have exhibit cause death unknown ...   \n",
       "8404              1810  authorizes coronavirus adding millions doses s...   \n",
       "8405            113206  throttling back push warp speed ahead his baby...   \n",
       "\n",
       "                                         less_prep_text  \\\n",
       "0     The Future of Vaccines\\n\\nNever before have bi...   \n",
       "1     The immunization task force can likely deliver...   \n",
       "2     Question if city staff will remain out working...   \n",
       "3        Enough for 84,000 people!  Big Whoppy Do.  ...   \n",
       "4     . will start administering the  COVID-19  to i...   \n",
       "...                                                 ...   \n",
       "8401     Wrong\\nThe Feds gave Pfizer $1.95 BILLION f...   \n",
       "8402  A second COVID-19  has been authorized for eme...   \n",
       "8403   They already have...\\nExhibit A: Cause of Dea...   \n",
       "8404  F.D.A. Authorizes Moderna #Coronavirus Vaccine...   \n",
       "8405   He's throttling back Pfizer's  to push Modern...   \n",
       "\n",
       "                                       tidy_text_tokens  ...  \\\n",
       "0     [the, future, never, before, have, billions, p...  ...   \n",
       "1     [the, immunization, task, force, can, likely, ...  ...   \n",
       "2     [question, city, staff, will, remain, out, wor...  ...   \n",
       "3     [enough, for, people, big, whoppy, population,...  ...   \n",
       "4     [will, start, administering, the, its, members...  ...   \n",
       "...                                                 ...  ...   \n",
       "8401  [wrong, the, feds, gave, billion, for, advance...  ...   \n",
       "8402  [second, has, been, authorized, for, emergency...  ...   \n",
       "8403  [they, already, have, exhibit, cause, death, u...  ...   \n",
       "8404  [authorizes, coronavirus, adding, millions, do...  ...   \n",
       "8405  [throttling, back, push, warp, speed, ahead, h...  ...   \n",
       "\n",
       "                                    no_stopwords_string  \\\n",
       "0         future never billions people pressured submit   \n",
       "1     immunization task force likely deliver shots s...   \n",
       "2     question city staff remain working homeless pe...   \n",
       "3     enough people big whoppy population canada millio   \n",
       "4                      start administering members week   \n",
       "...                                                 ...   \n",
       "8401  wrong feds gave billion advance purchase agree...   \n",
       "8402  second authorized emergency california hot spo...   \n",
       "8403  already exhibit cause death unknown volunteeri...   \n",
       "8404  authorizes coronavirus adding millions doses s...   \n",
       "8405  throttling back push warp speed ahead baby bet...   \n",
       "\n",
       "                                                 lemmas  \\\n",
       "0            [never, billion, people, pressure, submit]   \n",
       "1     [immunization, likely, deliver, shot, small, r...   \n",
       "2     [question, city, staff, remain, work, homeless...   \n",
       "3                                 [enough, people, big]   \n",
       "4                     [start, administer, member, week]   \n",
       "...                                                 ...   \n",
       "8401   [wrong, fed, give, advance, purchase, agreement]   \n",
       "8402   [second, authorize, emergency, hot, spot, biden]   \n",
       "8403  [already, exhibit, death, unknown, volunteer, ...   \n",
       "8404              [authorize, dose, supply, next, week]   \n",
       "8405  [throttle, push, warp_speed, ahead, baby, bet,...   \n",
       "\n",
       "                                          lemmas_string  \\\n",
       "0                  never billion people pressure submit   \n",
       "1     immunization likely deliver shot small rural h...   \n",
       "2     question city staff remain work homeless peopl...   \n",
       "3                                     enough people big   \n",
       "4                              start administer member    \n",
       "...                                                 ...   \n",
       "8401          wrong fed give advance purchase agreement   \n",
       "8402          second authorize emergency hot spot biden   \n",
       "8403  already exhibit death unknown volunteer partic...   \n",
       "8404                            authorize  supply next    \n",
       "8405     throttle push warp_speed ahead baby bet charge   \n",
       "\n",
       "                                                stemmed  \\\n",
       "0       [futur, never, billion, peopl, pressur, submit]   \n",
       "1     [immun, task, forc, like, deliv, shot, small, ...   \n",
       "2     [question, citi, staff, remain, work, homeless...   \n",
       "3     [enough, peopl, big, whoppi, popul, canada, mi...   \n",
       "4                       [start, administ, member, week]   \n",
       "...                                                 ...   \n",
       "8401  [wrong, fed, gave, billion, advanc, purchas, a...   \n",
       "8402  [second, author, emerg, california, hot, spot,...   \n",
       "8403  [alreadi, exhibit, caus, death, unknown, volun...   \n",
       "8404  [author, coronaviru, ad, million, dose, suppli...   \n",
       "8405  [throttl, back, push, warp, speed, ahead, babi...   \n",
       "\n",
       "                                         lemmas_reduced Document_No  \\\n",
       "0            [never, billion, people, pressure, submit]           0   \n",
       "1     [immunization, likely, deliver, shot, small, r...           1   \n",
       "2     [question, city, staff, remain, work, homeless...           2   \n",
       "3                                 [enough, people, big]           3   \n",
       "4                           [start, administer, member]           4   \n",
       "...                                                 ...         ...   \n",
       "8401   [wrong, fed, give, advance, purchase, agreement]        8401   \n",
       "8402   [second, authorize, emergency, hot, spot, biden]        8402   \n",
       "8403  [already, exhibit, death, unknown, volunteer, ...        8403   \n",
       "8404                          [authorize, supply, next]        8404   \n",
       "8405  [throttle, push, warp_speed, ahead, baby, bet,...        8405   \n",
       "\n",
       "      Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0                2.0              0.1255   \n",
       "1                2.0              0.1638   \n",
       "2                2.0              0.1290   \n",
       "3                7.0              0.1154   \n",
       "4                0.0              0.1132   \n",
       "...              ...                 ...   \n",
       "8401             2.0              0.1071   \n",
       "8402             6.0              0.1455   \n",
       "8403             9.0              0.1250   \n",
       "8404             6.0              0.1154   \n",
       "8405             2.0              0.1248   \n",
       "\n",
       "                                               Keywords  \\\n",
       "0     hospital, wait, distribute, country, share, de...   \n",
       "1     hospital, wait, distribute, country, share, de...   \n",
       "2     hospital, wait, distribute, country, share, de...   \n",
       "3     people, vaccinate, shot, read, time, develop, ...   \n",
       "4     health, worker, care, live, official, administ...   \n",
       "...                                                 ...   \n",
       "8401  hospital, wait, distribute, country, share, de...   \n",
       "8402  emergency, money, authorize, authorization, ap...   \n",
       "8403  distribution, roll, speed, part, continue, big...   \n",
       "8404  emergency, money, authorize, authorization, ap...   \n",
       "8405  hospital, wait, distribute, country, share, de...   \n",
       "\n",
       "                                                   Text  \n",
       "0            [never, billion, people, pressure, submit]  \n",
       "1     [immunization, likely, deliver, shot, small, r...  \n",
       "2     [question, city, staff, remain, work, homeless...  \n",
       "3                                 [enough, people, big]  \n",
       "4                           [start, administer, member]  \n",
       "...                                                 ...  \n",
       "8401   [vaccinate, placebo, group, people, help, prove]  \n",
       "8402                  [crucial, datum, come, day, look]  \n",
       "8403                          [fall, sword, investment]  \n",
       "8404  [understanding, test, find, high, quantity, ne...  \n",
       "8405  [research, find, evidence, own, stock, post, s...  \n",
       "\n",
       "[8406 rows x 21 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>location</th>\n      <th>statuses_count</th>\n      <th>followers_count</th>\n      <th>friends_count</th>\n      <th>listed_count</th>\n      <th>favourites_count</th>\n      <th>tidy_text</th>\n      <th>less_prep_text</th>\n      <th>tidy_text_tokens</th>\n      <th>...</th>\n      <th>no_stopwords_string</th>\n      <th>lemmas</th>\n      <th>lemmas_string</th>\n      <th>stemmed</th>\n      <th>lemmas_reduced</th>\n      <th>Document_No</th>\n      <th>Dominant_Topic</th>\n      <th>Topic_Perc_Contrib</th>\n      <th>Keywords</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The Future of Vaccines\\nhttps://t.co/GI23fycj1...</td>\n      <td>Earth</td>\n      <td>11485</td>\n      <td>774</td>\n      <td>3240</td>\n      <td>2</td>\n      <td>1107</td>\n      <td>the future never before have billions people b...</td>\n      <td>The Future of Vaccines\\n\\nNever before have bi...</td>\n      <td>[the, future, never, before, have, billions, p...</td>\n      <td>...</td>\n      <td>future never billions people pressured submit</td>\n      <td>[never, billion, people, pressure, submit]</td>\n      <td>never billion people pressure submit</td>\n      <td>[futur, never, billion, peopl, pressur, submit]</td>\n      <td>[never, billion, people, pressure, submit]</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0.1255</td>\n      <td>hospital, wait, distribute, country, share, de...</td>\n      <td>[never, billion, people, pressure, submit]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The immunization task force can likely deliver...</td>\n      <td>Mississauga, Canada</td>\n      <td>280</td>\n      <td>225</td>\n      <td>675</td>\n      <td>3</td>\n      <td>26</td>\n      <td>the immunization task force can likely deliver...</td>\n      <td>The immunization task force can likely deliver...</td>\n      <td>[the, immunization, task, force, can, likely, ...</td>\n      <td>...</td>\n      <td>immunization task force likely deliver shots s...</td>\n      <td>[immunization, likely, deliver, shot, small, r...</td>\n      <td>immunization likely deliver shot small rural h...</td>\n      <td>[immun, task, forc, like, deliv, shot, small, ...</td>\n      <td>[immunization, likely, deliver, shot, small, r...</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>0.1638</td>\n      <td>hospital, wait, distribute, country, share, de...</td>\n      <td>[immunization, likely, deliver, shot, small, r...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Question if city staff will remain out working...</td>\n      <td>Toronto</td>\n      <td>251890</td>\n      <td>19164</td>\n      <td>9952</td>\n      <td>607</td>\n      <td>30736</td>\n      <td>question city staff will remain out working wi...</td>\n      <td>Question if city staff will remain out working...</td>\n      <td>[question, city, staff, will, remain, out, wor...</td>\n      <td>...</td>\n      <td>question city staff remain working homeless pe...</td>\n      <td>[question, city, staff, remain, work, homeless...</td>\n      <td>question city staff remain work homeless peopl...</td>\n      <td>[question, citi, staff, remain, work, homeless...</td>\n      <td>[question, city, staff, remain, work, homeless...</td>\n      <td>2</td>\n      <td>2.0</td>\n      <td>0.1290</td>\n      <td>hospital, wait, distribute, country, share, de...</td>\n      <td>[question, city, staff, remain, work, homeless...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@iamIqraKhalid @GovCanHealth @moderna_tx Enoug...</td>\n      <td>Ontario, Canada</td>\n      <td>41897</td>\n      <td>1317</td>\n      <td>883</td>\n      <td>5</td>\n      <td>156959</td>\n      <td>enough for people big whoppy population canada...</td>\n      <td>Enough for 84,000 people!  Big Whoppy Do.  ...</td>\n      <td>[enough, for, people, big, whoppy, population,...</td>\n      <td>...</td>\n      <td>enough people big whoppy population canada millio</td>\n      <td>[enough, people, big]</td>\n      <td>enough people big</td>\n      <td>[enough, peopl, big, whoppi, popul, canada, mi...</td>\n      <td>[enough, people, big]</td>\n      <td>3</td>\n      <td>7.0</td>\n      <td>0.1154</td>\n      <td>people, vaccinate, shot, read, time, develop, ...</td>\n      <td>[enough, people, big]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>.@USForcesKorea will start administering the @...</td>\n      <td>Washington, DC</td>\n      <td>33658</td>\n      <td>16782</td>\n      <td>1181</td>\n      <td>529</td>\n      <td>558</td>\n      <td>will start administering the its members this ...</td>\n      <td>. will start administering the  COVID-19  to i...</td>\n      <td>[will, start, administering, the, its, members...</td>\n      <td>...</td>\n      <td>start administering members week</td>\n      <td>[start, administer, member, week]</td>\n      <td>start administer member</td>\n      <td>[start, administ, member, week]</td>\n      <td>[start, administer, member]</td>\n      <td>4</td>\n      <td>0.0</td>\n      <td>0.1132</td>\n      <td>health, worker, care, live, official, administ...</td>\n      <td>[start, administer, member]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8401</th>\n      <td>@Lya1224 @SenateDems @PattyMurray Wrong\\nThe F...</td>\n      <td>Philadelphia, PA</td>\n      <td>11432</td>\n      <td>125</td>\n      <td>207</td>\n      <td>0</td>\n      <td>12899</td>\n      <td>wrong the feds gave billion for advance purcha...</td>\n      <td>Wrong\\nThe Feds gave Pfizer $1.95 BILLION f...</td>\n      <td>[wrong, the, feds, gave, billion, for, advance...</td>\n      <td>...</td>\n      <td>wrong feds gave billion advance purchase agree...</td>\n      <td>[wrong, fed, give, advance, purchase, agreement]</td>\n      <td>wrong fed give advance purchase agreement</td>\n      <td>[wrong, fed, gave, billion, advanc, purchas, a...</td>\n      <td>[wrong, fed, give, advance, purchase, agreement]</td>\n      <td>8401</td>\n      <td>2.0</td>\n      <td>0.1071</td>\n      <td>hospital, wait, distribute, country, share, de...</td>\n      <td>[vaccinate, placebo, group, people, help, prove]</td>\n    </tr>\n    <tr>\n      <th>8402</th>\n      <td>A second COVID-19 vaccine has been authorized ...</td>\n      <td>Cullman, AL</td>\n      <td>5806</td>\n      <td>25</td>\n      <td>145</td>\n      <td>0</td>\n      <td>3</td>\n      <td>second has been authorized for emergency use c...</td>\n      <td>A second COVID-19  has been authorized for eme...</td>\n      <td>[second, has, been, authorized, for, emergency...</td>\n      <td>...</td>\n      <td>second authorized emergency california hot spo...</td>\n      <td>[second, authorize, emergency, hot, spot, biden]</td>\n      <td>second authorize emergency hot spot biden</td>\n      <td>[second, author, emerg, california, hot, spot,...</td>\n      <td>[second, authorize, emergency, hot, spot, biden]</td>\n      <td>8402</td>\n      <td>6.0</td>\n      <td>0.1455</td>\n      <td>emergency, money, authorize, authorization, ap...</td>\n      <td>[crucial, datum, come, day, look]</td>\n    </tr>\n    <tr>\n      <th>8403</th>\n      <td>@no_silenced They already have...\\nExhibit A: ...</td>\n      <td>NC</td>\n      <td>175485</td>\n      <td>15101</td>\n      <td>7543</td>\n      <td>20</td>\n      <td>136847</td>\n      <td>they already have exhibit cause death unknown ...</td>\n      <td>They already have...\\nExhibit A: Cause of Dea...</td>\n      <td>[they, already, have, exhibit, cause, death, u...</td>\n      <td>...</td>\n      <td>already exhibit cause death unknown volunteeri...</td>\n      <td>[already, exhibit, death, unknown, volunteer, ...</td>\n      <td>already exhibit death unknown volunteer partic...</td>\n      <td>[alreadi, exhibit, caus, death, unknown, volun...</td>\n      <td>[already, exhibit, death, unknown, volunteer, ...</td>\n      <td>8403</td>\n      <td>9.0</td>\n      <td>0.1250</td>\n      <td>distribution, roll, speed, part, continue, big...</td>\n      <td>[fall, sword, investment]</td>\n    </tr>\n    <tr>\n      <th>8404</th>\n      <td>F.D.A. Authorizes Moderna #Coronavirus Vaccine...</td>\n      <td>Montréal</td>\n      <td>141794</td>\n      <td>117020</td>\n      <td>1847</td>\n      <td>2908</td>\n      <td>1810</td>\n      <td>authorizes coronavirus adding millions doses s...</td>\n      <td>F.D.A. Authorizes Moderna #Coronavirus Vaccine...</td>\n      <td>[authorizes, coronavirus, adding, millions, do...</td>\n      <td>...</td>\n      <td>authorizes coronavirus adding millions doses s...</td>\n      <td>[authorize, dose, supply, next, week]</td>\n      <td>authorize  supply next</td>\n      <td>[author, coronaviru, ad, million, dose, suppli...</td>\n      <td>[authorize, supply, next]</td>\n      <td>8404</td>\n      <td>6.0</td>\n      <td>0.1154</td>\n      <td>emergency, money, authorize, authorization, ap...</td>\n      <td>[understanding, test, find, high, quantity, ne...</td>\n    </tr>\n    <tr>\n      <th>8405</th>\n      <td>@atrupar He's throttling back Pfizer's vaccine...</td>\n      <td>Homewood, AL</td>\n      <td>4394</td>\n      <td>287</td>\n      <td>1258</td>\n      <td>17</td>\n      <td>113206</td>\n      <td>throttling back push warp speed ahead his baby...</td>\n      <td>He's throttling back Pfizer's  to push Modern...</td>\n      <td>[throttling, back, push, warp, speed, ahead, h...</td>\n      <td>...</td>\n      <td>throttling back push warp speed ahead baby bet...</td>\n      <td>[throttle, push, warp_speed, ahead, baby, bet,...</td>\n      <td>throttle push warp_speed ahead baby bet charge</td>\n      <td>[throttl, back, push, warp, speed, ahead, babi...</td>\n      <td>[throttle, push, warp_speed, ahead, baby, bet,...</td>\n      <td>8405</td>\n      <td>2.0</td>\n      <td>0.1248</td>\n      <td>hospital, wait, distribute, country, share, de...</td>\n      <td>[research, find, evidence, own, stock, post, s...</td>\n    </tr>\n  </tbody>\n</table>\n<p>8406 rows × 21 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(8406, 21)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VADER Sentiment Analysis\n",
    "# Get/call SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "#Add VADER metrics to dataframe\n",
    "#df['compound'] = [analyzer.polarity_scores(v)['compound'] for v in df['lemmas_string']]\n",
    "#df['neg'] = [analyzer.polarity_scores(v)['neg'] for v in df['lemmas_string']]\n",
    "#df['neu'] = [analyzer.polarity_scores(v)['neu'] for v in df['lemmas_string']]\n",
    "#df['pos'] = [analyzer.polarity_scores(v)['pos'] for v in df['lemmas_string']]\n",
    "#df.head(3)\n",
    "\n",
    "\n",
    "# CODE BELOW ADDED AFTER SYNOPSIS HAND IN \n",
    "df['new_compound'] = [analyzer.polarity_scores(v)['compound'] for v in df['less_prep_text']]\n",
    "df['new_neg'] = [analyzer.polarity_scores(v)['neg'] for v in df['less_prep_text']]\n",
    "df['new_neu'] = [analyzer.polarity_scores(v)['neu'] for v in df['less_prep_text']]\n",
    "df['new_pos'] = [analyzer.polarity_scores(v)['pos'] for v in df['less_prep_text']]\n",
    "#df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg, neu, pos are multidimensional measures of text sentiment, they each represent what ratio of the text that belong to each category \n",
    "# compound score is the recommended metric if you want to have a unidimensional measure of text sentiment. It is a \"normalized, wegihted composite score\" which is derived the lexicon words' valence scores that is adjusted and normalized between -1 (mega neg) and 1 (mega pos). \n",
    "# see more info here: https://github.com/cjhutto/vaderSentiment\n",
    "\n",
    "# according to the github repo reffered above, it is \n",
    "# \"useful for researchers who would like to set standardized thresholds for classifying sentences as either positive, neutral, or negative. Typical threshold values (used in the literature cited on this page) are:\n",
    "# positive sentiment: compound score >= 0.05\n",
    "# neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "# negative sentiment: compound score <= -0.05\" \n",
    "\n",
    "# refers to turning compound scores into classes pos, neu or neg\n",
    "# POS = compound score >= 0.05\n",
    "# NEU = -0.05 < compoundscore < 0.05\n",
    "# NEG = compound score <= -0.05\n",
    "\n",
    "# get sentiment scores:\n",
    "#sentiment = []\n",
    "#for row in df['compound']:\n",
    "#    if row >= 0.05:                     sentiment.append('POS')\n",
    "#    elif row > -0.05 and row < 0.05:    sentiment.append('NEU')\n",
    "#    elif row <= -0.05:                  sentiment.append('NEG')\n",
    "#    else:                               sentiment.append('Not_Rated')\n",
    "\n",
    "# CODE BELOW ADDED AFTER SYNOPSIS HAND IN\n",
    "# get sentiment scores:\n",
    "new_sentiment = []\n",
    "for row in df['new_compound']:\n",
    "    if row >= 0.05:                     new_sentiment.append('POS')\n",
    "    elif row > -0.05 and row < 0.05:    new_sentiment.append('NEU')\n",
    "    elif row <= -0.05:                  new_sentiment.append('NEG')\n",
    "    else:                               new_sentiment.append('Not_Rated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                   text             location  \\\n0     The Future of Vaccines\\nhttps://t.co/GI23fycj1...                Earth   \n1     The immunization task force can likely deliver...  Mississauga, Canada   \n2     Question if city staff will remain out working...              Toronto   \n3     @iamIqraKhalid @GovCanHealth @moderna_tx Enoug...      Ontario, Canada   \n4     .@USForcesKorea will start administering the @...       Washington, DC   \n...                                                 ...                  ...   \n8401  @Lya1224 @SenateDems @PattyMurray Wrong\\nThe F...     Philadelphia, PA   \n8402  A second COVID-19 vaccine has been authorized ...          Cullman, AL   \n8403  @no_silenced They already have...\\nExhibit A: ...                   NC   \n8404  F.D.A. Authorizes Moderna #Coronavirus Vaccine...             Montréal   \n8405  @atrupar He's throttling back Pfizer's vaccine...         Homewood, AL   \n\n      statuses_count  followers_count  friends_count  listed_count  \\\n0              11485              774           3240             2   \n1                280              225            675             3   \n2             251890            19164           9952           607   \n3              41897             1317            883             5   \n4              33658            16782           1181           529   \n...              ...              ...            ...           ...   \n8401           11432              125            207             0   \n8402            5806               25            145             0   \n8403          175485            15101           7543            20   \n8404          141794           117020           1847          2908   \n8405            4394              287           1258            17   \n\n      favourites_count                                          tidy_text  \\\n0                 1107  the future never before have billions people b...   \n1                   26  the immunization task force can likely deliver...   \n2                30736  question city staff will remain out working wi...   \n3               156959  enough for people big whoppy population canada...   \n4                  558  will start administering the its members this ...   \n...                ...                                                ...   \n8401             12899  wrong the feds gave billion for advance purcha...   \n8402                 3  second has been authorized for emergency use c...   \n8403            136847  they already have exhibit cause death unknown ...   \n8404              1810  authorizes coronavirus adding millions doses s...   \n8405            113206  throttling back push warp speed ahead his baby...   \n\n                                         less_prep_text  \\\n0     The Future of Vaccines\\n\\nNever before have bi...   \n1     The immunization task force can likely deliver...   \n2     Question if city staff will remain out working...   \n3        Enough for 84,000 people!  Big Whoppy Do.  ...   \n4     . will start administering the  COVID-19  to i...   \n...                                                 ...   \n8401     Wrong\\nThe Feds gave Pfizer $1.95 BILLION f...   \n8402  A second COVID-19  has been authorized for eme...   \n8403   They already have...\\nExhibit A: Cause of Dea...   \n8404  F.D.A. Authorizes Moderna #Coronavirus Vaccine...   \n8405   He's throttling back Pfizer's  to push Modern...   \n\n                                       tidy_text_tokens  ... compound    neg  \\\n0     [the, future, never, before, have, billions, p...  ...   0.2235  0.000   \n1     [the, immunization, task, force, can, likely, ...  ...   0.0000  0.000   \n2     [question, city, staff, will, remain, out, wor...  ...   0.4019  0.000   \n3     [enough, for, people, big, whoppy, population,...  ...   0.0000  0.000   \n4     [will, start, administering, the, its, members...  ...   0.0000  0.000   \n...                                                 ...  ...      ...    ...   \n8401  [wrong, the, feds, gave, billion, for, advance...  ...   0.0258  0.301   \n8402  [second, has, been, authorized, for, emergency...  ...  -0.3818  0.342   \n8403  [they, already, have, exhibit, cause, death, u...  ...  -0.5994  0.394   \n8404  [authorizes, coronavirus, adding, millions, do...  ...   0.0000  0.000   \n8405  [throttling, back, push, warp, speed, ahead, h...  ...   0.0000  0.000   \n\n        neu    pos sentiment new_compound  new_neg  new_neu  new_pos  \\\n0     0.679  0.321       POS      -0.2263    0.106    0.894    0.000   \n1     1.000  0.000       NEU       0.0000    0.000    1.000    0.000   \n2     0.803  0.197       POS       0.4019    0.000    0.870    0.130   \n3     1.000  0.000       NEU       0.0000    0.000    1.000    0.000   \n4     1.000  0.000       NEU       0.0000    0.000    1.000    0.000   \n...     ...    ...       ...          ...      ...      ...      ...   \n8401  0.388  0.311       NEU       0.0258    0.190    0.613    0.196   \n8402  0.658  0.000       NEG      -0.3818    0.157    0.843    0.000   \n8403  0.606  0.000       NEG       0.0000    0.000    1.000    0.000   \n8404  1.000  0.000       NEU       0.0000    0.000    1.000    0.000   \n8405  1.000  0.000       NEU       0.0000    0.000    1.000    0.000   \n\n     new_sentiment  \n0              NEG  \n1              NEU  \n2              POS  \n3              NEU  \n4              NEU  \n...            ...  \n8401           NEU  \n8402           NEG  \n8403           NEU  \n8404           NEU  \n8405           NEU  \n\n[8406 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "#df['sentiment'] = sentiment\n",
    "df['new_sentiment'] = new_sentiment\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df - main df\n",
    "# save to csv file:\n",
    "#df.to_csv('final_main_df_topics_sentiment_added.csv', header=True, mode='a')\n",
    "# save to pickle \n",
    "#df.to_pickle('final_main_df_topics_sentiment_added.pkl')\n",
    "\n",
    "# CODE BELOW ADDED AFTER SYNOPSIS HAND IN\n",
    "# save to csv file:\n",
    "df.to_csv('oralexamupdated_final_main_df_topics_sentiment_added.csv', header=True, mode='a')\n",
    "# save to pickle \n",
    "df.to_pickle('oralexamupdated_final_main_df_topics_sentiment_added.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    Dominant_Topic sentiment    0\n",
       "0              0.0       NEG  152\n",
       "1              0.0       NEU  615\n",
       "2              0.0       POS  396\n",
       "3              1.0       NEG  186\n",
       "4              1.0       NEU  447\n",
       "5              1.0       POS  408\n",
       "6              2.0       NEG  119\n",
       "7              2.0       NEU  512\n",
       "8              2.0       POS  312\n",
       "9              3.0       NEG  246\n",
       "10             3.0       NEU  485\n",
       "11             3.0       POS  265\n",
       "12             4.0       NEG  105\n",
       "13             4.0       NEU  447\n",
       "14             4.0       POS  219\n",
       "15             5.0       NEG  121\n",
       "16             5.0       NEU  378\n",
       "17             5.0       POS  273\n",
       "18             6.0       NEG  198\n",
       "19             6.0       NEU  323\n",
       "20             6.0       POS  221\n",
       "21             7.0       NEG  137\n",
       "22             7.0       NEU  288\n",
       "23             7.0       POS  251\n",
       "24             8.0       NEG  122\n",
       "25             8.0       NEU  264\n",
       "26             8.0       POS  285\n",
       "27             9.0       NEG  107\n",
       "28             9.0       NEU  347\n",
       "29             9.0       POS  177"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dominant_Topic</th>\n      <th>sentiment</th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>NEG</td>\n      <td>152</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>NEU</td>\n      <td>615</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>POS</td>\n      <td>396</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>NEG</td>\n      <td>186</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>NEU</td>\n      <td>447</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1.0</td>\n      <td>POS</td>\n      <td>408</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2.0</td>\n      <td>NEG</td>\n      <td>119</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2.0</td>\n      <td>NEU</td>\n      <td>512</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2.0</td>\n      <td>POS</td>\n      <td>312</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>3.0</td>\n      <td>NEG</td>\n      <td>246</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>3.0</td>\n      <td>NEU</td>\n      <td>485</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>3.0</td>\n      <td>POS</td>\n      <td>265</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>4.0</td>\n      <td>NEG</td>\n      <td>105</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>4.0</td>\n      <td>NEU</td>\n      <td>447</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>4.0</td>\n      <td>POS</td>\n      <td>219</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>5.0</td>\n      <td>NEG</td>\n      <td>121</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>5.0</td>\n      <td>NEU</td>\n      <td>378</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>5.0</td>\n      <td>POS</td>\n      <td>273</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>6.0</td>\n      <td>NEG</td>\n      <td>198</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>6.0</td>\n      <td>NEU</td>\n      <td>323</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>6.0</td>\n      <td>POS</td>\n      <td>221</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>7.0</td>\n      <td>NEG</td>\n      <td>137</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>7.0</td>\n      <td>NEU</td>\n      <td>288</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>7.0</td>\n      <td>POS</td>\n      <td>251</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>8.0</td>\n      <td>NEG</td>\n      <td>122</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>8.0</td>\n      <td>NEU</td>\n      <td>264</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>8.0</td>\n      <td>POS</td>\n      <td>285</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>9.0</td>\n      <td>NEG</td>\n      <td>107</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>9.0</td>\n      <td>NEU</td>\n      <td>347</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>9.0</td>\n      <td>POS</td>\n      <td>177</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "#test = df1.stack()\n",
    "test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nDominant_Topic  sentiment\\n0.0             NEU          568\\n                POS          469\\n                NEG          126\\n1.0             POS          447\\n                NEU          401\\n                NEG          193\\n2.0             NEU          433\\n                POS          381\\n                NEG          129\\n3.0             NEU          464\\n                POS          288\\n                NEG          244\\n4.0             NEU          382\\n                POS          295\\n                NEG           94\\n5.0             NEU          349\\n                POS          308\\n                NEG          115\\n6.0             POS          279\\n                NEU          241\\n                NEG          222\\n7.0             POS          288\\n                NEU          259\\n                NEG          129\\n8.0             POS          309\\n                NEU          242\\n                NEG          120\\n9.0             NEU          294\\n                POS          236\\n                NEG          101\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "#df.groupby('Dominant_Topic')['sentiment'].value_counts()\n",
    "dftest = df.groupby('Dominant_Topic')['sentiment'].value_counts().fillna(0)\n",
    "\n",
    "\"\"\"\n",
    "Dominant_Topic  sentiment\n",
    "0.0             NEU          568\n",
    "                POS          469\n",
    "                NEG          126\n",
    "1.0             POS          447\n",
    "                NEU          401\n",
    "                NEG          193\n",
    "2.0             NEU          433\n",
    "                POS          381\n",
    "                NEG          129\n",
    "3.0             NEU          464\n",
    "                POS          288\n",
    "                NEG          244\n",
    "4.0             NEU          382\n",
    "                POS          295\n",
    "                NEG           94\n",
    "5.0             NEU          349\n",
    "                POS          308\n",
    "                NEG          115\n",
    "6.0             POS          279\n",
    "                NEU          241\n",
    "                NEG          222\n",
    "7.0             POS          288\n",
    "                NEU          259\n",
    "                NEG          129\n",
    "8.0             POS          309\n",
    "                NEU          242\n",
    "                NEG          120\n",
    "9.0             NEU          294\n",
    "                POS          236\n",
    "                NEG          101\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.groupby('Dominant_Topic')['sentiment'].value_counts().unstack().fillna(0)\n",
    "\n",
    "# LINE BELOW ADDED AFTER SYNOPSIS HAND IN\n",
    "df1_new = df.groupby('Dominant_Topic')['new_sentiment'].value_counts().unstack().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new_sentiment   NEG  NEU  POS\nDominant_Topic               \n0.0             130  580  453\n1.0             206  389  446\n2.0             143  443  357\n3.0             253  452  291\n4.0             108  386  277\n5.0             123  350  299\n6.0             236  246  260\n7.0             143  242  291\n8.0             124  238  309\n9.0             111  306  214\n"
     ]
    }
   ],
   "source": [
    "print(df1_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1:\n",
    "sentiment       NEG  NEU  POS\n",
    "Dominant_Topic               \n",
    "0.0             152  615  396\n",
    "1.0             186  447  408\n",
    "2.0             119  512  312\n",
    "3.0             246  485  265\n",
    "4.0             105  447  219\n",
    "5.0             121  378  273\n",
    "6.0             198  323  221\n",
    "7.0             137  288  251\n",
    "8.0             122  264  285\n",
    "9.0             107  347  177\n",
    "\n",
    "df1_new:\n",
    "new_sentiment   NEG  NEU  POS\n",
    "Dominant_Topic               \n",
    "0.0             130  580  453\n",
    "1.0             206  389  446\n",
    "2.0             143  443  357\n",
    "3.0             253  452  291\n",
    "4.0             108  386  277\n",
    "5.0             123  350  299\n",
    "6.0             236  246  260\n",
    "7.0             143  242  291\n",
    "8.0             124  238  309\n",
    "9.0             111  306  214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.groupby('sentiment')['Dominant_Topic'].value_counts().unstack().fillna(0)\n",
    "\n",
    "#df2\n",
    "#Dominant_Topic  0.0  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0\n",
    "#sentiment                                                       \n",
    "#NEG             152  186  119  246  105  121  198  137  122  107\n",
    "#NEU             615  447  512  485  447  378  323  288  264  347\n",
    "#POS             396  408  312  265  219  273  221  251  285  177\n",
    "\n",
    "# LINE BELOW ADDED AFTER SYNOPSIS HAND IN\n",
    "df2_new = df.groupby('new_sentiment')['Dominant_Topic'].value_counts().unstack().fillna(0)\n",
    "\n",
    "#df2_new\n",
    "#Dominant_Topic  0.0  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0\n",
    "#new_sentiment                                                   \n",
    "#NEG             130  206  143  253  108  123  236  143  124  111\n",
    "#NEU             580  389  443  452  386  350  246  242  238  306\n",
    "#POS             453  446  357  291  277  299  260  291  309  214\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dominant_Topic  0.0  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0\nnew_sentiment                                                   \nNEG             130  206  143  253  108  123  236  143  124  111\nNEU             580  389  443  452  386  350  246  242  238  306\nPOS             453  446  357  291  277  299  260  291  309  214\n"
     ]
    }
   ],
   "source": [
    "print(df2_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK ADDED AFTER SYNOPSIS HAND IN\n",
    "# create visualisation of how the distribution between neg, neu, pos has changed with the less preprocessed text version \n",
    "# but first, prepare data to plot \n",
    "\n",
    "# data to use \n",
    "# df1 \n",
    "# df1_new\n",
    "\n",
    "def prep_df(df_name, df_callname):\n",
    "    df_name = df_name.stack().reset_index()\n",
    "    df_name.columns = ['Dominant_Topic', 'Sentiment_Cat', 'Values']\n",
    "    df_name['Text_Version'] = df_callname\n",
    "    return df_name\n",
    "\n",
    "plot_df1 = prep_df(df1, 'Original')\n",
    "plot_df1_new = prep_df(df1_new, 'Updated')\n",
    "\n",
    "df_test_plot = pd.concat([plot_df1, plot_df1_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "\n<div id=\"altair-viz-b4e90f75827e43cca56ff252be4fdab3\"></div>\n<script type=\"text/javascript\">\n  (function(spec, embedOpt){\n    let outputDiv = document.currentScript.previousElementSibling;\n    if (outputDiv.id !== \"altair-viz-b4e90f75827e43cca56ff252be4fdab3\") {\n      outputDiv = document.getElementById(\"altair-viz-b4e90f75827e43cca56ff252be4fdab3\");\n    }\n    const paths = {\n      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n    };\n\n    function loadScript(lib) {\n      return new Promise(function(resolve, reject) {\n        var s = document.createElement('script');\n        s.src = paths[lib];\n        s.async = true;\n        s.onload = () => resolve(paths[lib]);\n        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n        document.getElementsByTagName(\"head\")[0].appendChild(s);\n      });\n    }\n\n    function showError(err) {\n      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n      throw err;\n    }\n\n    function displayChart(vegaEmbed) {\n      vegaEmbed(outputDiv, spec, embedOpt)\n        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n    }\n\n    if(typeof define === \"function\" && define.amd) {\n      requirejs.config({paths});\n      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n    } else if (typeof vegaEmbed === \"function\") {\n      displayChart(vegaEmbed);\n    } else {\n      loadScript(\"vega\")\n        .then(() => loadScript(\"vega-lite\"))\n        .then(() => loadScript(\"vega-embed\"))\n        .catch(showError)\n        .then(() => displayChart(vegaEmbed));\n    }\n  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300, \"strokeOpacity\": 0}}, \"data\": {\"name\": \"data-7fe68e98424b03fe5d3efd6b2b3f7f0c\"}, \"mark\": \"bar\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Sentiment_Cat\", \"scale\": {\"range\": [\"#e25822\", \"#FCCE45\", \"#88B04B\"]}}, \"column\": {\"type\": \"nominal\", \"field\": \"Dominant_Topic\", \"title\": null}, \"x\": {\"type\": \"nominal\", \"field\": \"Text_Version\", \"title\": null}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"sum\", \"axis\": {\"grid\": false, \"title\": null}, \"field\": \"Values\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-7fe68e98424b03fe5d3efd6b2b3f7f0c\": [{\"Dominant_Topic\": 0.0, \"Sentiment_Cat\": \"NEG\", \"Values\": 152, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 0.0, \"Sentiment_Cat\": \"NEU\", \"Values\": 615, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 0.0, \"Sentiment_Cat\": \"POS\", \"Values\": 396, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 1.0, \"Sentiment_Cat\": \"NEG\", \"Values\": 186, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 1.0, \"Sentiment_Cat\": \"NEU\", \"Values\": 447, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 1.0, \"Sentiment_Cat\": \"POS\", \"Values\": 408, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 2.0, \"Sentiment_Cat\": \"NEG\", \"Values\": 119, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 2.0, \"Sentiment_Cat\": \"NEU\", \"Values\": 512, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 2.0, \"Sentiment_Cat\": \"POS\", \"Values\": 312, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 3.0, \"Sentiment_Cat\": \"NEG\", \"Values\": 246, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 3.0, \"Sentiment_Cat\": \"NEU\", \"Values\": 485, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 3.0, \"Sentiment_Cat\": \"POS\", \"Values\": 265, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 4.0, \"Sentiment_Cat\": \"NEG\", \"Values\": 105, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 4.0, \"Sentiment_Cat\": \"NEU\", \"Values\": 447, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 4.0, \"Sentiment_Cat\": \"POS\", \"Values\": 219, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 5.0, \"Sentiment_Cat\": \"NEG\", \"Values\": 121, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 5.0, \"Sentiment_Cat\": \"NEU\", \"Values\": 378, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 5.0, \"Sentiment_Cat\": \"POS\", \"Values\": 273, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 6.0, \"Sentiment_Cat\": \"NEG\", \"Values\": 198, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 6.0, \"Sentiment_Cat\": \"NEU\", \"Values\": 323, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 6.0, \"Sentiment_Cat\": \"POS\", \"Values\": 221, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 7.0, \"Sentiment_Cat\": \"NEG\", \"Values\": 137, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 7.0, \"Sentiment_Cat\": \"NEU\", \"Values\": 288, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 7.0, \"Sentiment_Cat\": \"POS\", \"Values\": 251, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 8.0, \"Sentiment_Cat\": \"NEG\", \"Values\": 122, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 8.0, \"Sentiment_Cat\": \"NEU\", \"Values\": 264, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 8.0, \"Sentiment_Cat\": \"POS\", \"Values\": 285, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 9.0, \"Sentiment_Cat\": \"NEG\", \"Values\": 107, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 9.0, \"Sentiment_Cat\": \"NEU\", \"Values\": 347, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 9.0, \"Sentiment_Cat\": \"POS\", \"Values\": 177, \"Text_Version\": \"Original\"}, {\"Dominant_Topic\": 0.0, \"Sentiment_Cat\": \"NEG\", \"Values\": 130, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 0.0, \"Sentiment_Cat\": \"NEU\", \"Values\": 580, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 0.0, \"Sentiment_Cat\": \"POS\", \"Values\": 453, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 1.0, \"Sentiment_Cat\": \"NEG\", \"Values\": 206, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 1.0, \"Sentiment_Cat\": \"NEU\", \"Values\": 389, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 1.0, \"Sentiment_Cat\": \"POS\", \"Values\": 446, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 2.0, \"Sentiment_Cat\": \"NEG\", \"Values\": 143, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 2.0, \"Sentiment_Cat\": \"NEU\", \"Values\": 443, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 2.0, \"Sentiment_Cat\": \"POS\", \"Values\": 357, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 3.0, \"Sentiment_Cat\": \"NEG\", \"Values\": 253, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 3.0, \"Sentiment_Cat\": \"NEU\", \"Values\": 452, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 3.0, \"Sentiment_Cat\": \"POS\", \"Values\": 291, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 4.0, \"Sentiment_Cat\": \"NEG\", \"Values\": 108, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 4.0, \"Sentiment_Cat\": \"NEU\", \"Values\": 386, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 4.0, \"Sentiment_Cat\": \"POS\", \"Values\": 277, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 5.0, \"Sentiment_Cat\": \"NEG\", \"Values\": 123, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 5.0, \"Sentiment_Cat\": \"NEU\", \"Values\": 350, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 5.0, \"Sentiment_Cat\": \"POS\", \"Values\": 299, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 6.0, \"Sentiment_Cat\": \"NEG\", \"Values\": 236, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 6.0, \"Sentiment_Cat\": \"NEU\", \"Values\": 246, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 6.0, \"Sentiment_Cat\": \"POS\", \"Values\": 260, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 7.0, \"Sentiment_Cat\": \"NEG\", \"Values\": 143, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 7.0, \"Sentiment_Cat\": \"NEU\", \"Values\": 242, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 7.0, \"Sentiment_Cat\": \"POS\", \"Values\": 291, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 8.0, \"Sentiment_Cat\": \"NEG\", \"Values\": 124, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 8.0, \"Sentiment_Cat\": \"NEU\", \"Values\": 238, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 8.0, \"Sentiment_Cat\": \"POS\", \"Values\": 309, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 9.0, \"Sentiment_Cat\": \"NEG\", \"Values\": 111, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 9.0, \"Sentiment_Cat\": \"NEU\", \"Values\": 306, \"Text_Version\": \"Updated\"}, {\"Dominant_Topic\": 9.0, \"Sentiment_Cat\": \"POS\", \"Values\": 214, \"Text_Version\": \"Updated\"}]}}, {\"mode\": \"vega-lite\"});\n</script>",
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "# BLOCK ADDED AFTER SYNOPSIS HAND IN\n",
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "#import altair as alt\n",
    "\n",
    "alt.Chart(df_test_plot).mark_bar().encode(\n",
    "\n",
    "    # tell Altair which field to group columns on\n",
    "    x=alt.X('Text_Version:N', title=None),\n",
    "\n",
    "    # tell Altair which field to use as Y values and how to calculate\n",
    "    y=alt.Y('sum(Values):Q',\n",
    "        axis=alt.Axis(\n",
    "            grid=False,\n",
    "            title=None)),\n",
    "\n",
    "    # tell Altair which field to use to use as the set of columns to be  represented in each group\n",
    "    column=alt.Column('Dominant_Topic:N', title=None),\n",
    "\n",
    "    # tell Altair which field to use for color segmentation \n",
    "    color=alt.Color('Sentiment_Cat:N',\n",
    "            scale=alt.Scale(\n",
    "                # make it look pretty with an enjoyable color pallet\n",
    "                range=['#e25822', '#FCCE45', '#88B04B'],\n",
    "            ),\n",
    "        ))\\\n",
    "    .configure_view(\n",
    "        # remove grid lines around column clusters\n",
    "        strokeOpacity=0    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save df2\n",
    "df2.to_csv('df_groupedby_sentiment_dominanttopic.csv', header=True, mode='a')\n",
    "df2.to_csv('minus_amode_df_groupedby_sentiment_dominanttopic.csv', header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS BLOCK IS ADDED AFTER SYNOPSIS HAND IN\n",
    "\n",
    "# Creating a subset of the lemmas_string/text column and sentiment column to inspect/evaluate performance of the sentiment model\n",
    "#sentiment_eval_subset = df[['lemmas_string', 'sentiment', \"less_prep_text\", \"new_sentiment\"]]\n",
    "sentiment_eval_subset.to_csv('sentiment_eval_subset.csv', header=True, mode='a')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text          user  \\\n",
       "0  @Gunntwitt I saw my military neurologist yeste...  kleegrubaugh   \n",
       "1  $MRK Announces Sale of its Direct Equity Inves...     BioStocks   \n",
       "2  ‘Absolutely remarkable’: No one who got Modern...    MJosling53   \n",
       "\n",
       "             location  statuses_count  \\\n",
       "0  Texas Hill Country          172187   \n",
       "1             Toronto           45006   \n",
       "2              Canada           68026   \n",
       "\n",
       "                                    user_description  followers_count  \\\n",
       "0  First - Be kind. Reformed Christian. Conservat...             3883   \n",
       "1  Investor/trader with over a decade of experien...            40945   \n",
       "2  Former IDF bomb disposal unit member including...             1272   \n",
       "\n",
       "   friends_count  listed_count  favourites_count  \\\n",
       "0           4978            90            162046   \n",
       "1             76           912               301   \n",
       "2           1993             2             62456   \n",
       "\n",
       "                                           tidy_text  \\\n",
       "0  saw military neurologist yesterday sammc san a...   \n",
       "1  mrk announces sale its direct equity investmen...   \n",
       "2  absolutely remarkable one who got trial develo...   \n",
       "\n",
       "                                    tidy_text_tokens  \\\n",
       "0  [saw, military, neurologist, yesterday, sammc,...   \n",
       "1  [mrk, announces, sale, its, direct, equity, in...   \n",
       "2  [absolutely, remarkable, one, who, got, trial,...   \n",
       "\n",
       "                                 tokens_no_stopwords  \\\n",
       "0  [military, neurologist, yesterday, sammc, san,...   \n",
       "1  [mrk, announces, sale, direct, equity, investm...   \n",
       "2  [absolutely, remarkable, trial, developed, sev...   \n",
       "\n",
       "                                 no_stopwords_string  \\\n",
       "0  military neurologist yesterday sammc san anton...   \n",
       "1   mrk announces sale direct equity investment mrna   \n",
       "2       absolutely remarkable trial developed severe   \n",
       "\n",
       "                                            lemmas  \\\n",
       "0                                         [center]   \n",
       "1     [announce, sale, direct, equity, investment]   \n",
       "2  [absolutely_remarkable, trial, develop, severe]   \n",
       "\n",
       "                                lemmas_string  \\\n",
       "0                                      center   \n",
       "1      announce sale direct equity investment   \n",
       "2  absolutely_remarkable trial develop severe   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  [militari, neurologist, yesterday, sammc, san,...   \n",
       "1  [mrk, announc, sale, direct, equiti, invest, m...   \n",
       "2           [absolut, remark, trial, develop, sever]   \n",
       "\n",
       "                                      stemmed_string  tb_Pol  tb_Subj  \n",
       "0  militari neurologist yesterday sammc san anton...  -0.075    0.075  \n",
       "1         mrk announc sale direct equiti invest mrna   0.100    0.400  \n",
       "2                 absolut remark trial develop sever   0.425    0.525  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>user</th>\n      <th>location</th>\n      <th>statuses_count</th>\n      <th>user_description</th>\n      <th>followers_count</th>\n      <th>friends_count</th>\n      <th>listed_count</th>\n      <th>favourites_count</th>\n      <th>tidy_text</th>\n      <th>tidy_text_tokens</th>\n      <th>tokens_no_stopwords</th>\n      <th>no_stopwords_string</th>\n      <th>lemmas</th>\n      <th>lemmas_string</th>\n      <th>stemmed</th>\n      <th>stemmed_string</th>\n      <th>tb_Pol</th>\n      <th>tb_Subj</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@Gunntwitt I saw my military neurologist yeste...</td>\n      <td>kleegrubaugh</td>\n      <td>Texas Hill Country</td>\n      <td>172187</td>\n      <td>First - Be kind. Reformed Christian. Conservat...</td>\n      <td>3883</td>\n      <td>4978</td>\n      <td>90</td>\n      <td>162046</td>\n      <td>saw military neurologist yesterday sammc san a...</td>\n      <td>[saw, military, neurologist, yesterday, sammc,...</td>\n      <td>[military, neurologist, yesterday, sammc, san,...</td>\n      <td>military neurologist yesterday sammc san anton...</td>\n      <td>[center]</td>\n      <td>center</td>\n      <td>[militari, neurologist, yesterday, sammc, san,...</td>\n      <td>militari neurologist yesterday sammc san anton...</td>\n      <td>-0.075</td>\n      <td>0.075</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>$MRK Announces Sale of its Direct Equity Inves...</td>\n      <td>BioStocks</td>\n      <td>Toronto</td>\n      <td>45006</td>\n      <td>Investor/trader with over a decade of experien...</td>\n      <td>40945</td>\n      <td>76</td>\n      <td>912</td>\n      <td>301</td>\n      <td>mrk announces sale its direct equity investmen...</td>\n      <td>[mrk, announces, sale, its, direct, equity, in...</td>\n      <td>[mrk, announces, sale, direct, equity, investm...</td>\n      <td>mrk announces sale direct equity investment mrna</td>\n      <td>[announce, sale, direct, equity, investment]</td>\n      <td>announce sale direct equity investment</td>\n      <td>[mrk, announc, sale, direct, equiti, invest, m...</td>\n      <td>mrk announc sale direct equiti invest mrna</td>\n      <td>0.100</td>\n      <td>0.400</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>‘Absolutely remarkable’: No one who got Modern...</td>\n      <td>MJosling53</td>\n      <td>Canada</td>\n      <td>68026</td>\n      <td>Former IDF bomb disposal unit member including...</td>\n      <td>1272</td>\n      <td>1993</td>\n      <td>2</td>\n      <td>62456</td>\n      <td>absolutely remarkable one who got trial develo...</td>\n      <td>[absolutely, remarkable, one, who, got, trial,...</td>\n      <td>[absolutely, remarkable, trial, developed, sev...</td>\n      <td>absolutely remarkable trial developed severe</td>\n      <td>[absolutely_remarkable, trial, develop, severe]</td>\n      <td>absolutely_remarkable trial develop severe</td>\n      <td>[absolut, remark, trial, develop, sever]</td>\n      <td>absolut remark trial develop sever</td>\n      <td>0.425</td>\n      <td>0.525</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "#### Textblob analysis example as inspired by Medium article ####\n",
    "# (https://medium.com/swlh/simple-sentiment-analysis-for-nlp-beginners-and-everyone-else-using-vader-and-textblob-728da3dbe33d)\n",
    "# I am not using this part of the code\n",
    "\n",
    "# TextBlob Sentiment Analysis\n",
    "# Load the text/tweets column as a TextBlob, and add two new columns for polarity and subjectivity\n",
    "#load the text into textblob\n",
    "text_blob = [TextBlob(text) for text in tweets_df['text']]\n",
    "#add the sentiment metrics to the dataframe\n",
    "tweets_df['tb_Pol'] = [b.sentiment.polarity for b in text_blob]\n",
    "tweets_df['tb_Subj'] = [b.sentiment.subjectivity for b in text_blob]\n",
    "#show dataframe\n",
    "tweets_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentiment(polarity=0.425, subjectivity=0.5166666666666666)\n"
     ]
    }
   ],
   "source": [
    "# testing textblob\n",
    "# # Preparing an input sentence\n",
    "sentence = \"today panel will decide who gets the first umd docs thrilled with reported effec.\"\n",
    "\n",
    "# Creating a textblob object assignign sentiment\n",
    "analysis = TextBlob(sentence).sentiment\n",
    "print(analysis)\n",
    "#output is a namedtuple of the form Sentiment(polarity, subjectivity)\n",
    "# you can also do: \n",
    "#analysisPol = TextBlob(sentence).polarity\n",
    "#analysisSub = TextBlob(sentence).subjectivity\n",
    "# if you want to display polarity and subjectivity seperately...\n",
    "#print(analysisPol)\n",
    "#print(analysisSub)\n",
    "\n",
    "# you can use textblob with different algorithms\n",
    "#PatternAnalyzer - default classifier built on the pattern library\n",
    "#NaiveBayesAnalyzer - NLTK model trained on a corpus of movie reviews"
   ]
  }
 ]
}